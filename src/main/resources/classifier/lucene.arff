@relation 'sentences -C 5 '

@attribute isAlternative {0,1}
@attribute isPro {0,1}
@attribute isCon {0,1}
@attribute isDecision {0,1}
@attribute isIssue {0,1}
@attribute sentence string

@data
0,1,0,0,0,'By the way this patch looks good. Thanks Benson! I plan to commit this in a bit '
0,0,0,0,0,'Merged to 2.9 revision: 949507'
0,0,0,0,0,Patch.
0,0,0,0,0,'I agree, we should fix this. '
0,0,0,0,1,'We are storing the fps of the freq and pos files only because we need to synchronise the position of the \"inverted file pointer\" on each file. \r\n'
0,0,0,0,1,'here is a first patch allowing similarity be more flexible when encoding norms. '
0,0,0,1,0,'But we should probably think about better fixes for the future and avoid calling score() when it\'s not needed.'
0,0,0,0,0,'I am forgetting how concurrency will work in this case, ie,\r\ninsuring multi-threaded visibility due to the JMM. Actually,\r\nbecause we\'re pausing the writes/deletes when get reader is\r\ncalled on the DWPT, JMM'
0,0,0,0,0,'I\'ll change to a try/finally w/ a success boolean.\r\n\r\n'
0,0,0,0,0,'You can use IndexWriter#unlock to forcefully remove the lock, as a workaround.'
0,0,0,0,0,'This is a Lucene index _known_ to be corrupt . The Lucene used to create this index is 2.3.2, so during this UT I believe an index upgrade happens too.\r\n'
0,0,0,0,0,'Yes, I do hit [-LUCENE-2104-|https://issues.apache.org/jira/browse/LUCENE-2104] at the same time... nice.'
1,0,0,0,0,'EG maybe we should\r\nonly store skip data if dF >= 1024 (say), and then separately index\r\nskip data every N docs.\r\n\r\n'
0,0,0,0,0,'This exception shows a LockObtainFailed exception - can you post the one that resulted in NegativeArraySize – curious to know where you hit it, and what sort of corruption yields to that  !'
0,1,0,0,0,'So to clarify... no code (outside what the patch touches) needs adjustments?'
1,0,0,0,0,'So if the commit to TW succeeds and the commit to IW fails, you potentially don\'t end up in an inconsistent state. '
0,0,0,0,0,'Really I think topLevelScorer should be strongly typed: the intent is to declare whether you will call Scorer.score or whether you will call .nextDoc/.score ... they really should be different classes.\r\n\r\n'
0,0,0,1,0,'Lets reuse IW.deleteUnusedFiles() ?\r\n'
0,0,0,0,0,'Unfortunately the latest test failures on master make it hard to differentiate between failures caused by my changes and ones already there. '
0,1,0,0,0,'Robert\'s suggestion was to enable \"all\" warnings, but IMHO this is a bad idea, because if somebody compiles with a later Java version, the build may suddenly fail (because a later version of the compiler added a new warning type).\r\n\r\n'
0,0,0,1,0,'Because most of the dictionaries are L/GPL, I\'ve written my own dumb stupid dictionary for test purposes.\r\n\r\n'
0,1,0,0,0,'Yes, the build passes for me with only the two additional changes in WordDictionary and SimpleServer.\r\n\r\n'
0,0,0,1,0,'Committed revision 1328981 for the xml queryparser support. Thanks again. '
0,0,0,1,0,'I also have a sneaking suspicion that this only affects lucene and solr is somehow ignoring it, but couldn\'t find anything to confirm that.'
0,0,0,1,0,'I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint. '
0,0,0,1,0,'I am for using this patch. '
0,0,0,1,0,'committed and resolved, thanks '
1,0,0,0,0,'If we implement deletes via sequence id across all segments, then the .del file should probably remain the same (a set of bits)? '
0,0,0,0,0,'would be:\r\nBS1 completely violates the scorer interface, the only method you can call is the one taking a Collector. '
0,0,0,0,0,'In regards to the deltas, when they\'re in RAM , I\'m guessing we\'d need to place the updates into a hash map ? '
0,0,0,0,0,'\r\nTrue, however I figured it\'d be best to try our own dog food, or\r\nAPIs. '
0,0,0,0,0,'\r\n+1, i think for freq and getChildren we should throw UOE with text like this. '
1,0,0,0,0,'I think the main issue right now is the concurrency of the\r\n*BlockPools from [LUCENE-2575|https://issues.apache.org/jira/browse/LUCENE-2575]. '
0,0,0,0,0,'ProIn my case it was \"retrieving the subquery score\"...\r\nquot'
0,0,0,0,0,'For the deleted docs sequence id array, perhaps I\'m a little bit\r\nconfused, but how will we signify in the sequence id array if a\r\ndocument is deleted? '
0,0,0,0,0,'An idea  '
0,0,0,0,0,'+1\r\n\r\nBut we should word it as a \"workaround\" ... ie, it\'s sort of strange that returning false from this unrelated method means suddenly scorer.freq works: that\'s really an implementation detail. '
0,0,0,0,0,'However we\'re using readers to do the merging so this really won\'t be useful?'
0,1,0,0,0,'We had to delete it since we re-integrated it for DocumentsWriterPerThread. '
0,0,0,0,0,'Maybe this is a good first step? '
0,0,0,0,0,'Clearing 3.1 fix version... it\'s not clear how we can fix this w/o drastic API changes...'
1,0,0,0,0,'We could also [someday] move deletes to a stacked model... where we only write \"deltas\" (newly deleted docs in the current session) against the segment, and on open we coalesce these. '
1,0,0,0,0,'I believe we need a secondary sequence id\r\narray for deleted docs that is init\'d to -1. '
0,0,0,0,1,'Is there\r\nany reason we need to apply deletes in the actual updateDoc and\r\ndeleteDoc methods?'
0,0,0,0,0,'\r\nWell you are using a custom collector anyway if you are doing this, so can\'t we just add a sentence to that\r\nmethod\'s javadocs indicating that you should return false if you want to use the scorer navigation apis?'
1,0,0,0,0,'Anyway, I think there is still a need for this if we can provide it as a non-default option?'
0,0,0,0,0,'DecisionOr we could just fold both the booleans into \'BS1 is ok\' ... are they used anywhere else?  !https://i'
0,0,0,1,0,'I\'m implementing a basic doc id iterator per DWPT which will allow us to implement delete by term, and the deleted docs sequence ids. '
0,0,0,0,0,'EG someday we could make BS1 score docs in order , and then this workaround no longer works.'
0,0,0,0,0,'So returning \"false\" works around the issue currently, but it would not hurt if somebody would return false, although our new BS1 can handle in order. '
0,0,0,0,0,'If we wanted to do this in the future, we could just rename scoreDocsInOrder\r\nto needsNavigation.\r\n\r\n'
0,1,0,0,0,'+1 on my end as well'
0,0,0,0,0,'https://issues.apache.org/jira/images/icons/emoticons/smile.png|width=16,height=16!'
0,0,0,0,0,'I am forgetting how concurrency will work in this case, ie,\r\ninsuring multi-threaded visibility due to the JMM. '
1,0,0,0,0,'Actually,\r\nbecause we\'re pausing the writes/deletes when get reader is\r\ncalled on the DWPT, JMM concurren'
1,0,0,0,0,'Nothing can be deleted without the terms dictionary and the terms docs working in order to obtain the doc ids to delete.'
0,0,0,0,1,'And, for saving the new deletes in the directory (though this is not really important for the RT case).'
0,0,0,0,0,'When the deleted docs Bits is being accessed,\r\nfor a given doc, we\'ll compare the IRs seq-id-up-to with the\r\ndel-docs-seq-id, and if the IR seq-id is greater than or equal\r\nto, the Bits.get method will return true, meaning the document\r\nis deleted.\r\n\r\n'
0,0,0,1,0,'Committed revision 899955. '
0,0,0,0,0,'When a document is\r\ndeleted, the sequence id is set for that doc in the\r\ndel-docs-seq-arr. '
0,0,0,1,0,'Also, when we load up the BV on IW start, then I guess we\'ll need to init the array appropriately.'
1,0,0,0,0,'Then we should be able to\r\nimplement deleting, which doesn\'t require skip lists. '
1,0,0,0,0,'I guess if\r\nwe really wanted to, we could simply buffer terms and only apply\r\nthem in getReader. getReader would block any writes that could\r\nbe altering the *BlockPools. '
0,0,0,0,0,'\r\nI don\'t agree: the strangeness is the two booleans toplevelScorer and scoreDocsInOrder. '
0,0,0,0,0,'It does not only affect freq. '
0,0,0,0,0,'As you need a custom collector anyway to make use of Scorer.getChildren, we should maybe make BS1 throw UOE on getChildren in 4.0  and visitSubScorers in 3.6.2?'
0,0,0,1,0,'If this is so, the test should pass, so I added an exit condition, if the data dir is not available'
0,0,0,0,0,'The problem is that \"scoresDocsInOrder\" doesn\'t really capture what\'s necessary here .\r\n\r\n'
0,0,0,0,0,'And I agree Robert: the current booleans \"topLevelScorer\" and \"scoreDocsInOrder\", and then a new \"needsNavigation\", will make things rather confusing. '
0,0,0,0,0,'I agree Uwe: if we add a Collector.needsNavigation then even a \"fixed\" BS1 that sorted the docIDs before collection would not be usable since the subs will not be \"on\" the doc during collect.\r\n\r\n'
0,0,0,0,0,'ProIf we don\'t think any other future scorer would want to score docs NOT in order ... then maybe we should simple rename scoreDocsInOrder to needsNavigation? (Or scoreDocAtOnce, scoreDocAtATime, something else'
0,1,0,0,0,'What do we do with Solr? '
0,0,0,0,0,'Move issue to Lucene 4.9.'
0,1,0,0,0,'This may be the reason why Solr does not show any problems!?'
0,0,1,0,0,'We cannot declare both methods '
0,1,0,0,0,'With this patch it now behaves equal to bw checkouts:\r\n - If you have no svn.exe available, it will ignore the checkout. '
0,0,0,0,0,'or). And the navigation api  should be separated from score and freq - a simple java interface Scorer. So the current in-order scorer '
0,0,0,1,0,'OK, new patch; I changed to EOFE, and I just use in.toString() instead of special casing DI vs II.\r\n\r\n'
0,0,0,0,0,'\r\nWell let\'s remember that the \"must have doc-at-once scoring, for all subs too\" is a very rare use-case.\r\n\r\n'
1,0,0,0,0,'Otherwise I am fine with committing this. '
0,1,0,0,0,'BTW: Maybe we can enable rawtypes and unchecked errors earlier in Lucene and leave them disabled in Solr. '
0,1,0,0,0,'The data dir now exists but the build should stop in this case.'
0,1,0,0,0,'It would be really helpful to get this as soon as possible in the next Lucene version.'
0,1,0,0,0,'\r\nI agree: let\'s fix that.'
0,0,0,1,0,'But we should really work on removing unsafe and rawtypes warnings from functions module. '
0,0,0,0,0,'rface.\r\n\r\nBut yeah I agree: it should be strongly typed, and BS1 should only implement the .score interface. The Scores'
0,0,0,0,0,'DecisionThe vast majority of users just need a fast .score interface.\r\n\r\nBut yeah I'
0,0,0,0,0,'Bulk move 4.4 issues to 4.5 and 5.0'
1,0,0,0,0,'Let\'s assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. '
0,1,0,0,0,'Now those are completely undetected (no warning, no error).'
0,0,0,0,0,'But yeah, if we are going to have booleans, i would prefer something more along the lines of document-at-a-time since its less confusing than'
0,0,0,1,0,'i add the getSnowballWordSet to WordListLoader'
0,0,0,0,0,'this seems to be related to [-LUCENE-3874-|https://issues.apache.org/jira/browse/LUCENE-3874]. The patch in [-LUCENE-3874-|https://issues.apache.org/jira/browse/LUCENE-3874] isn\'t actually working all the time since on very large positions is > Int.MAX >> 1 we can easily shift a leading 1 in FreqProxTermsWriterPerField#writeProx makeing the position value negative. I think we need to do something like:\r\n'
0,0,0,0,0,'In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that\'s undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I\'m not sure how to solve it, but that seems unrelated to that issue .'
0,0,0,0,0,'Obtaining the lock for this purpose does not seem the right way to me ... '
0,0,0,0,0,'I did not implement ReaderCommit to support deletes. '
0,0,0,0,0,'IndexWriter has a deleteUnusedFiles which the application can use. '
0,1,0,0,0,'Fixed by Mike, thanks Mike!'
0,1,0,0,0,'\r\nyeah I agree. '
0,1,0,0,0,'thanks Simon, I agree '
1,0,0,0,0,'Hmm I think we need a separate check in FreqProxTermsWriterPerField?\r\n\r\n'
1,0,0,0,0,'reopening to address the issue of demo source code being in the binary checkout... we should either include it, or change the demo instructions to tell people to get the demo sources from the source release'
0,0,0,1,0,'when i changed the backwards tests i added a new param to svn exec task. '
0,0,0,0,0,'In trunk we can remove it and only provide List.'
1,0,0,0,0,'We could instantiate a new array when the map reached a certain size?'
1,0,0,0,0,'So, for each Query object class, make a Jackson mixin class, that maps the \'official inputs\' to the current collection of setters/constructor arguments. '
0,1,0,0,0,'But backwards does not fail now'
0,0,0,1,0,'\r\nThis isn\'t strictly necessary, I think? '
0,1,0,0,0,'+1 to commit'
0,0,0,0,0,'Not sure it\'s worth the efforts.\r\n\r\n'
0,0,0,0,0,'Bulk close for 3.1'
0,0,0,1,0,'I\'ll drop that to 50...\r\n\r\n'
0,1,0,0,0,'iff there is, shouldnt normsconsumer[perfield] pull the impl from the codec?'
1,0,0,0,0,'Changed NativeFSLock.release() to first obtain and then release the lock, if it\'s not held. That way, if obtain fails, LockReleaseFailedException is thrown, to indicate that.'
0,0,0,1,0,'Yes, thanks, now it passes (trunk) - with this seed as well quite a few times without specifying a seed. \r\n'
0,0,0,0,1,'The nightly build also hits too-many-open-files every so often, I suspect because our random-per-field-codec is making too many codecs... '
1,0,0,0,0,'\r\nSo maybe reuse deletePendingFiles? '
0,0,1,0,0,'Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:\r\n'
0,0,0,1,0,'Bulk closing for 3.2'
0,0,0,0,0,'focusedCommentId=13036307&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13036307]\r\n '
0,0,0,0,0,'I\'ll wait a day or two until I commit it. Your comment'
0,0,0,0,0,'Demo sources are no longer shipped in a binary release'
0,0,0,0,0,'I believe it is ready to commit. '
0,0,0,1,0,'Bulk close for 3.1'
0,0,0,1,0,'Adds revisitPolicy to IFD (package-private) and also calls it from IW.deleteUnusedFiles. '
0,0,0,1,0,'I dropped it from 100 to 50 segs. '
0,0,0,0,0,'Ie if it accumulates too many codecs, to start sharing th'
0,0,0,0,0,'Can you test if that work'
0,0,0,1,0,'Committed revision 1034140 (trunk).'
0,0,0,0,0,'So I think this can be resolved...'
0,0,0,0,0,'I fact in 3x this is not reproducible with same seed  and I was not able to reproduce it with no seed, tried with -Dtest.iter=100 as well (though I am not sure, would a new seed be created in each iteration? '
0,0,0,0,0,'I only looked at the output robert posted so far but it seems that a FrozenDelPackage gets lost somewhere here....\r\n\r\n'
0,0,0,0,0,'it shoudl check exactly at the point before shifts any bits: and the exception should be UOE'
0,0,1,0,0,'The two ops (ongoing commit\r\n– takes time since fync can be so slow – and deleting unused files)\r\nare orthogonal. '
1,0,0,0,0,'Patch with a port of the code.\r\n\r\n'
1,0,0,0,0,'When using the skip list, you will traverse the skip list until you find the skip point of interest, then decode the associated skip entry and get the doc file pointer. '
0,1,0,0,0,'On one hand, this considerably reduce the size of the skip list, since most of the information are \"exported\" and encoded into block headers. '
1,0,0,0,0,'I think our skipInterval is too low for the block codecs (and, should\r\nbe private) Separately, I think we should break out \"when skip is even\r\nstored\" vs \"how frequently we index skip data\". '
1,0,0,0,0,'The other pointers (to the freq file and position file) are in fact stored into the block header.\r\n'
0,0,1,0,0,'Also, this occurs some overhead when you only need to answer\r\n - pure boolean query: we only need to scan the doc file, but we are still reading and decoding the fp pointers of the freq and pos file;\r\n - extended boolean query: we only need to scan the doc and freq file, but we are still reading and decoding the fp pointers of the pos file;\r\n\r\nAn idea I had a few months ago (but never found the time to implement it and test it) was to change the way the skip list data structure is created. '
0,0,0,0,1,'If we are _REALLY_ sure keeping int alignment in these intblock\r\nencoded files is not important (ie, we really do get best perf by\r\nslurping in byte[] and then decoding from there), then we should also\r\nstore eg skip data into the frq/doc file (this is what Standard\r\ndoes).\r\n\r\n'
0,0,0,0,0,'But, the problem is, IFD.checkpoint will hold a new commit point when\r\nyou pass isCommit=true, which is no good. I think we need to open up\r\na new package private method in IFD, eg \"revisitPolicy\" or some such, which\r\njust does:\r\n'
0,0,0,0,0,'\r\nIe, most of what IFD.checkpoint does when isCommit=true, minus the\r\nincRef \r\nand the commits.add of a new commit point.\r\n\r\n'
0,0,0,0,0,'Invoking IDP.onCommit still isn\'t quite right \r\nbut I think it\'s OK for now? '
0,0,1,0,0,'EG we call that on every checkpoint call... so this\'d mean the IDP gets 2 onCommit calls per commit (one \"fake\" one and one \"real\")?\r\n\r\n'
0,0,0,0,0,'\r\nIn fact this is the right track, I think... rollbackSI is a clone of\r\nthe last committed segments, whereas the \"live\" segments contains all\r\nuncommitted stuff that\'s happened since. We really should not be\r\ntreating these pending changes as if they were a commit point... so\r\nusing rollbackSI makes sense.\r\n\r\n'
0,0,0,0,0,'\r\nWhat do '
0,0,0,0,0,'I.e. this method does not accept anything, and so seems that revisitPolicy won\'t . '
0,0,0,1,0,'It\'s odd because that test doesn\'t make that many fields... oh I see it makes a 100 segment index. '
0,0,0,0,0,'(Adding some kind of \"visit\" method\r\nfeels like overkill...).\r\n'
0,0,0,0,0,'\r\nUrgh.... indeed we must protect against one thread doing addIndexes\r\nand another thread calling deleteUnusedFiles.\r\n\r\n'
0,0,0,0,0,'Hmm the problem should be wider than just NRT.'
0,0,0,0,0,'I\'d love to [eventually] change addIndexes*, so that it does all its\r\nwork \"privately\" and only in the end atomically \"checks in\" the\r\n segments it produced. '
0,0,0,0,0,'It gets tricky, though, since\r\n\"normal\" segment merging, and flushing, is still ongoing, and we\'d not\r\nwant to do redundant merging work.\r\n\r\nThis also messes up NRT, ie, if you open an NRT reader during an\r\naddIndexes*, you can see some segments already added and some now –\r\nie NRT violates the advertised atomicity of addIndexes* (the javadocs\r\nnote this).\r\n\r\n'
0,0,0,0,0,'It gets tricky, though, since\r\n\"normal\" segment merging, and flushing, is still ongoing, and we\'d not\r\nwant to do redundant merging work.\r\n\r\n'
0,0,0,0,0,'I think we really need to factor IW apart:\r\n # Indexer , also flushes new segments\r\n\r\n # Keeper of the segments file (exposes API to make atomic changes to\r\nsegments file, does commits, interacts w/ IDP/IFD)\r\n\r\n # Merger (normal merging, optimize, expungeDeletes, addIndexe'
0,0,0,0,0,'If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. '
0,0,0,0,0,'\", but, I think there are'
0,0,0,0,0,'\r\nI agree.\r\n\r\n'
0,0,0,0,0,'.\r\nquote\r\nBut with in-order scoring we are in all cases use correctly positioned scorers, otherwise it is a bug . So returning \"f'
0,0,0,1,0,'I tried to keep the actual DocValues type out of similarity to reduce complexity. '
0,0,0,0,0,'New patch attached; I think it\'s ready to commit.\r\n\r\n'
0,0,0,0,0,'That patch also requires you first apply [-LUCENE-3112-|https://issues.apache.org/jira/browse/LUCENE-3112].'
0,0,0,0,0,'Not as much as I expecte'
0,0,0,0,0,'I ran quick perf test – single pass was ~18\% faster than two-pass . '
0,0,0,1,0,'From my perspective this is a fair game here not exposing the entire power of DocValues to similarity to reduce complexity in the API.\r\n\r\n'
0,0,0,0,1,'I also understand the preflex problem and I totally agree with the solution here.'
0,1,0,0,0,'there is certainly room for improvement but its a start. comments are welcome'
0,0,0,1,0,'I also fixed some JavaDoc issues since computeNorm signature has changed. '
1,0,0,0,0,'I forced the IW to use OpenMode.CREATE and suddenly the tests are not failing anymore. '
0,0,0,0,1,'I cleaned up the Lucene4FieldInfoFormat and added a preflex and 3x version. '
0,0,0,0,1,'if I run the test by itself, it seems to pass.\r\n\r\n'
0,0,0,1,0,'Even that this is a 3x mock codec for RW support I need to write fieldnumbers since the actual FI ord might be different from the field number assigned by the trunk IW. so I have a special format for this.\r\n\r\n'
0,0,0,1,0,'I also remove setByte in favor of setInt(byte)\r\nquote]\r\n\r\nWhy? '
0,0,0,0,0,'this '
0,1,0,0,0,'anyway I think we should name the methos setXXX instead.'
1,0,0,0,0,'I think we must change it to limit itself by number of docs instead. '
0,1,0,0,0,'new patch with setXXX methods on Norm class. '
0,0,0,1,0,'Committing tomorrow.'
0,0,0,1,0,'I now throw a hard IArgExc if sim emits different types for the same field and added tests to check if that is happening.\r\n\r\n'
0,1,0,0,0,'I think this is much safer.'
0,0,0,1,0,'patch created with svn diff --show-copies-as-adds for readability.'
1,0,0,0,0,'The preflex version is slightly different. '
1,0,0,0,0,'For instance, when you commit such pair, you need to first commit IW, and only then TW. '
1,0,0,0,0,'the reason this test is not reproducible is that this test uses \'n seconds\' as a limit.\r\n'
0,0,0,0,0,'There was a specific issue to fix this problem [-LUCENE-2956-|https://issues.apache.org/jira/browse/LUCENE-2956].'
0,0,1,0,0,'\r\nHmm I think this is too much? '
0,0,0,1,0,'All tests pass but still need to add JavaDoc to the norms class.\r\n\r\n'
0,1,0,0,0,'what a typo... s/omit/emit in the patch! '
0,0,0,0,0,'taking a look at this, I don\'t like the way _TestUtil.getTempDir was working before... it was basically desc + LTC.random.nextInt, so if you wired the seed like I did, and somehow stuff doesnt totally clean up, then its easy to see how it could return an already-created dir.\r\n\r\n'
0,0,0,1,0,'I forgot to make the methods public, which I\'ll fix when it gets committed.'
0,0,0,0,0,'It\'s probably the new DWPT code. '
0,0,0,1,0,'new patch. added some cleanups + javadoc.\r\n'
0,0,0,0,0,'because it also deletes the pending files . Unless IW.deleteUnusedFiles will invoke both deletePendingFiles and revisitPolicy ... the latter will just do\r\n'
0,0,0,0,0,'I will '
1,0,0,0,0,'I can reproduce this easily and even if I set search threads to 0 and index threads to 1. '
0,0,0,0,0,'It seems that the tempdir is not cleaned up since always the second test fails for me but never the first run.\r\n\r\n'
0,0,0,0,1,'however, if my machine is \"busy\" , then it fails!'
0,0,0,0,0,'This seems like a delete issue. '
0,0,0,0,0,'And then IW does not n'
0,1,0,0,0,'Patch looks good Shai!\r\n\r\n'
0,0,0,0,0,'Will commit this later - giving chance for more people to review.'
0,0,0,0,1,'As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls.'
1,0,0,0,0,'\r\nI\'ve tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I\'m not sure if that\'s the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...\r\n\r\n'
0,0,0,0,0,'BTW, the test fails on DirReader.doClose, where it checks if writer !'
0,0,0,0,0,'= null and then calls deleteUnusedFiles. So I guess it\'s a NRT problem only.\r\n\r\n'
0,0,0,1,0,'ok I\'ll remove them before commit. '
0,0,0,1,0,'I changed the approach, poaching an improvement from nested docs\r\n([-LUCENE-2454-|https://issues.apache.org/jira/browse/LUCENE-2454]): instead of pulling a DocTermsIndex from the field\r\ncache, and detecting new group by changing ord, I require the app\r\nprovides a Filter to denote the transition between groups.\r\n\r\n'
1,0,0,0,0,'here\'s a patch for the regular FixedIntBlock and VariableIntBlock cases: they write upto first, if a bit is set then its within block and upto is the upto delta. otherwise they then read the vlong for the fp delta.\r\n\r\n'
0,0,0,0,0,'I\'ll give it a try. deletePendingFiles would just do what it does, then in the end call policy.onCommit and deleteCommits ... what do you think? '
0,1,0,0,0,'Both patches look great! '
0,1,0,0,0,'It fixes the problem when usePhraseHighlighter=true.\r\n\r\n'
0,1,0,0,0,'this saves about 5\% total bulkvint index size.'
0,0,0,0,0,'+1, I think that\'s a good approach.'
0,0,0,0,0,'Ok I understand. About the name, revisitPolicy is not exactly accurate  '
0,0,0,0,0,'addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it \'fixes\' their Directory reference. I wondered how is regular commit works when addIndexesNoOptimize is called, but couldn\'t find any synchronization block where one blocks the other. Eventually, I\'ve changed deleteUnusedFiles to this:\r\n'
0,0,0,1,0,'Committed revision 941417.'
1,0,0,0,0,'However, if you think of it, the main goal of the skip list is to skip doc id, not freq or pos. '
0,0,0,1,0,'Committed revision 936605.'
0,0,0,0,0,'So, I think we should apply this little patch as'
1,0,0,0,0,'we could instead have a SnowballWordListLoader in our snowball package or something, doesn\'t matter to me.'
0,0,1,0,0,'But you can remove the \" ...\" in the message output – I had put into my code thinking there may be details we put instead of that \"'
0,0,0,1,0,'I think we can just change incRef to call tryIncRef, and then throw ACE if tryIncRef returns false?'
0,0,0,1,0,'trunk: Committed revision 1170908.\r\n3x: Committed revision 1170913.'
0,1,0,0,0,'When the flag is false and FVH works on N-gram field, not a few terms may be created in tree, then it causes uncontrollable.\r\n\r\n'
0,0,1,0,0,'Uh, that is not good news. '
0,0,0,0,1,'If you have a svn.exe available, but the checkout fails, there is an network error or something else.'
0,1,0,0,0,'All tests pass'
1,0,0,0,0,'Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.\r\n\r\n'
0,0,0,0,0,'here\'s a patch solving a lot of the issue for the skiplists and doc/freq/prox etc pointers for Simple64.\r\n\r\n'
0,0,0,1,0,'I\'ll commit soon.'
1,0,0,0,0,'The skip list provides only the first pointer to the block doc file, then pointers to subsequent blocks are included into the block headers.\r\n'
0,0,0,1,0,'here\'s a patch with what Mike suggested, moving the skipping stuff private to the codecs, and separating the interval from the minimum df necessary to index skip data.\r\n\r\n'
0,1,0,0,0,+1
0,0,0,0,0,'But I think the case of using usePhraseHighlighter=false with N-gram field is rare, the attached p'
0,0,0,0,0,'As a side benefit I also expect that when the Ooo dictionaries get more use in Lucene, users will over time be able to extend and improve the dictionaries, and contribute their changes back, benefiting also Ooo users.'
0,0,0,0,1,'Our queries sometime contain tens of thousands of terms. '
0,0,0,0,0,'Commit 1691282 from [~mikemccand] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1691282] ]\r\n\r\n[-LUCENE-6680-|https://issues.apache.org/jira/browse/LUCENE-6680]: don\'t lose a suggestion that differs only in payload from another suggestion'
0,0,0,1,0,'pointers in the term dictionary and skipdata are reduced, because they only point to the block/offset in the single .doc file, and the freq is implied and parallel following it.\r\n\r\n'
1,0,0,0,0,'\r\nTermQuery currently has a bogus constructor today:\r\n'
1,0,0,0,0,'As a quick experiment, i compared simple64-varint with the default skipInterval (16) against one with a higher interval (32).\r\n\r\n'
0,1,0,0,0,'This way you ll have the better comprise betwenn fetching the less number of boxes and not fetching too big boxes with too many documents in them.'
1,0,0,0,0,'Hi Nicolas, I like the idea of reducing the number of tiles that need to be queried, but it does look like the current reduction might be a little aggressive for the default. '
0,0,0,0,0,'I guess thats something they will have to sort out during incubation.\r\n\r\n'
0,0,0,1,0,'During testing I discovered a long standing bug to do with recursive application of rules This has now been fixed.\r\n\r\n'
0,1,0,0,0,+1
0,0,0,1,0,'all tests pass, I think this is ready though!\r\n'
0,0,0,1,0,'Patch now includes a package.html linking to a PDF about hunspell and suggesting dictionaries are sourced from the OpenOffice wiki.\r\n\r\n'
0,0,0,0,0,'All\r\nthat\'s necessary is the app has some way to create the Filter noting\r\nthe last doc in each group. '
0,0,0,1,0,'The patch. '
0,0,0,1,0,'\r\nI don\'t think we should read too much into that text file: its not even obvious which of the many dictionaries in that folder it applies to!\r\n\r\n'
0,0,0,1,0,'the docs and freq blocks are interleaved (doc block, freq block) in such a way that its generally transparent to bulk enum consumers. '
0,0,0,0,0,'I\'ll commit shortly...'
0,0,0,1,0,'Code now is also version aware, as required by the CharArray* data structures.'
0,0,0,0,0,'Okay good spotting. so how do we want to proceed? '
0,0,0,0,0,'\r\nhow is OpenOffice dealing with those dictionaries since they are now an ASF incubation project? '
1,0,0,0,0,'We did lots of work on Google Code, so it should really be in Lucene, except the dictionaries. '
0,0,0,0,0,'It need not be a \"single valued indexed\r\nfield\"...\r\n\r\n'
0,1,0,0,0,'Interleaving frq/doc makes tons of sense, not only for better\r\ncompression, but because for large (can\'t-be-hot) indexes the need to\r\nseek to 2 files during search is costly.\r\n\r\n'
0,1,0,0,0,'this is really confusing! '
0,0,0,1,0,'Created [-LUCENE-3719-|https://issues.apache.org/jira/browse/LUCENE-3719] with a patch.'
0,0,0,1,0,'patch with mod to wordlistloader, test, and snowball stoplists for danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, russian, spanish, and swedish '
0,0,1,0,0,'They both invoke IDP/IFD, but this is still protected\r\n(sync\'d on IW)...\r\n'
0,0,0,0,0,'Should we address that in a later issue once its become clearer in OO'
0,0,0,0,0,'We should only add links to web pages where to get them.'
0,0,0,1,0,'Bulk closing for 3.2'
0,0,0,0,0,'Do we want to bring some of the dictionaries in? '
0,0,0,0,0,'3x back port:\r\n\r\nCommitted revision 1167505.'
0,0,0,0,0,'Reopening for 3x backport.'
0,0,0,0,0,'Is there a JIRA for adding HunspellStemFilterFactory to Solr?'
0,0,0,0,0,'I don\'t think we should do anything with the dictionaries ever, its much better to make small \"test\" dictionaries that are actually more like unit tests and test certain things, like what you did in the patch.'
0,0,0,0,0,'Bulk close after release of 3.5'
0,0,0,0,0,'Our sign bit is free here to steal for payloads. '
0,0,0,0,0,'So we don\'t need to limit positions to'
0,0,0,0,0,'I want to commit soon.\r\n\r\n'
0,0,0,0,0,'See my comment and test \r\n\r\nThe fact this test doesnt fail on 3.x is a *bad thing* :)'
0,1,0,0,0,+1
0,0,0,1,0,'* In refreshIfNeeded, I understand this code  is equivalent to closing  (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don\'t call close(), perhaps we should, for clarity? '
1,0,0,0,0,'I\'ll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.'
0,0,0,1,0,'Bulk close for 5.3.0 release'
0,0,0,1,0,'\r\nGood, I\'ll fix that.\r\n'
0,1,0,0,0,'But this is valid, the documents might just not have any.'
0,0,0,1,0,' * I don\'t know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it\'s the best we can do? '
1,0,0,0,0,'\r\nActually I think all lookups for a del doc should still be against the BV.\r\n\r\n'
0,1,0,0,0,'+1 too bad we can\'t expect ArrayList to always perform like a plain array  !'
0,0,0,1,0,'SegemntCoreReaders atomic increment is also correct - @BrianGoetzSays  !'
0,1,0,0,0,'+1, looks correct. '
0,0,0,1,0,'* It\'s good that you identify replaceTaxonomy, makes the code safer'
0,0,0,1,0,'* You wrote previously that the test uses LineFileDocs, but I don\'t see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?  '
0,1,0,0,0,'\r\nOtherwise this looks great! When I worked on it in the past, DTR wasn\'t NRT and the sync was a nightmare. Making it NRT really simplified this manager!'
0,0,0,1,0,'Bulk close after release of 3.5'
0,0,0,0,0,'Task QPS trunk StdDev QPS patch StdDev Pct diff\r\n OrHighNotLow 99.21  110.09  11.0\% \r\n OrHighNotMed 78.46  90.47  15.3\% \r\n OrHighNotHigh 24.80  29.90  20.5\% \r\n OrNotHighHigh 33.71  50.06  48.5\% \r\n OrNotHighMed 57.14  183.47  221.1\% \r\n OrNotHighLow 62.74  922.24  1369.9\% '
0,0,0,1,0,'\r\n * IndexCommit implements Comparable. '
0,0,0,1,0,'I\'ll fix to just use DTW...'
0,0,0,0,1,'Basically what this means is that nothing seems to be inside anything else – and yet there are no edge intersections anywhere either. I\'m wondering if this is a result of granularity? Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid\'s boundaries.\r\n\r\nIt does seem odd that all failures we\'ve seen have been for very complex polygons, though. '
1,0,0,0,0,'It looks how many times you can fit the search diameter (2.0d * range) into the distance that will be split into longitudes range (I-e distanceUnit.earthCircumference()).\r\n\r\nAnd then it takes the first biggest level of Tier that will have a range just above the search diameter (int bestFit = (int) Math.ceil(log2(times)) '
0,0,0,0,1,'Also, the main problem of the current skip list implementation when it is applied on Sep codec is in my opinion the fact that we have to store for each skip list entry the fp of each of the sep file. '
0,1,0,0,0,'\r\nagreed - uploaded a new patch'
0,0,0,0,1,'Another non-reproducing failure, from my Jenkins:'
0,0,0,0,0,'Commit 1549012 from [~simonw] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1549012] ]\r\n\r\n[-LUCENE-5362-|https://issues.apache.org/jira/browse/LUCENE-5362]: IndexReader and SegmentCoreReaders now throw AlreadyClosedException if the refCount in incremented but is less that 1.'
0,0,0,0,0,'https://issues.apache.org/jira/images/icons/emoticons/sad.png|width=16,'
0,1,0,0,0,'+1\r\n\r\nWe now use Lucene Hunspell for a few customer deployments, and it would be great to have it the analysis module, since it supports some 70-80 languages out of the box, and gives great flexibility since you can edit - or augment - the dictionaries to change behaviour and fix stemming bugs.\r\n\r\n'
1,0,0,0,0,'I am interested in bring this branch up to date with the current trunk and contributing it to Lucene.'
1,0,0,0,0,'The idea was to store the pointer to the doc file in the skip entry, and nothing else. '
0,0,0,1,0,'\r\n * This assert in the test  is kinda moot because we always return a FacetResult, even if empty. '
0,0,0,0,1,'Sudarshan,\r\n\r\nHow would you boost by geodistance instead of limiting by a location? '
0,0,0,1,0,'Committed revision 1167467.'
1,0,0,0,0,'Use case:\r\n\r\nSearch a business name for the whole USA, but show closest to a lat,long first.'
0,0,0,0,0,'Also the UOE\'s in the NearSpansOrdered payload methods have gone in this patch, I had put these in to check the tests.'
0,0,0,1,0,'I removed the UOE because now the no-payload impl is used if a segment doesn\'t happen to have any payloads. '
0,0,1,0,0,'The first ensureOpen() in incRef() now seems redundant after this patch?'
0,0,0,0,0,'https://issues.apache.org/jira/images/icons/emoticons/smile.png|width=16,'
0,0,0,1,0,'For now the check is implemented via Terms.getPayloads() until [LUCENE-6390|https://issues.apache.org/jira/browse/LUCENE-6390] is fixed.'
0,0,0,1,0,'* Maybe change the end of the test to a single-line IOUtils.close()? '
1,0,0,0,0,'here is a patch'
0,0,0,1,0,'Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1? '
0,0,0,0,0,'Commit 1549013 from [~simonw] in branch \'dev/branches/branch_4x\'\r\n[ [https://svn.apache.org/r1549013] ]\r\n\r\n[-LUCENE-5362-|https://issues.apache.org/jira/browse/LUCENE-5362]: IndexReader and SegmentCoreReaders now throw AlreadyClosedException if the refCount in incremented but is less that 1.'
0,0,0,0,0,'It\'s somewhat crazy for an app to be closing AND trying to reopen at the same time , but I agree we should make a best effort here if the code changes are minor.\r\n\r\n'
1,0,0,0,0,'Can WFST enable that? '
0,1,0,0,0,'\r\n+1'
0,0,0,0,0,'Commit 1691283 from [~mikemccand] in branch \'dev/branches/branch_5x\'\r\n[ [https://svn.apache.org/r1691283] ]\r\n\r\n[-LUCENE-6680-|https://issues.apache.org/jira/browse/LUCENE-6680]: don\'t lose a suggestion that differs only in payload from another suggestion'
0,0,0,1,0,'New patch w/ last round of changes ... thanks Shai!'
0,0,0,1,0,'this was a temp file issue - fixed'
0,0,0,1,0,'it returns a hashset but should really return a Set<String>. We plan to change all return types to the interface instead of the implementation. '
0,0,1,0,0,'Passing the whole directory name is in my opinion useless. '
0,0,0,0,0,'you might be able to work me down to a partial path rather than a full path...\r\n\r\n'
0,0,0,1,0,'Bulk close for 5.3.0 release'
0,0,0,1,0,'When our II impls originate an exception (eg from EOF), I also include\r\nII.toString(); if a method they call throws IOE (eg file.read(...)\r\n'
0,0,0,0,0,'\r\nfor a DocIdSetIterator, and because of the symmetry it can also do that in the reverse direction as needed here.\r\n'
0,0,0,0,0,'Though, I suspect this  is a minor cost overall, if indeed the app has so many child docs per parent that a sparse bit set would be warranted? '
0,0,0,0,1,'Maybe the IWConfig getter/setter here\r\nshould also be pkg-private?\r\n\r\n'
1,0,0,0,0,'I would close this as won\'t fix and maybe only fix the remaining places that misses the file name (e.g. '
0,0,0,0,0,'can this be done in a cleaner way?'
0,0,0,0,0,'I need to implement an index with nested docs and an example scheme and query would be awesome. '
0,0,0,1,0,'inside SimpleFSII), then I catch & rethrow w/ II.toString() included.\r\n\r\n'
0,0,0,1,0,'OK I committed this for now to the branch (r1070580), we can always revert it.\r\n\r\n'
1,0,0,0,0,'I think, only adding the file name as done before should be fine. '
0,0,0,0,1,'It really seems like an implementation detail that we check the stored fields to determine if an indexformat is too old. who cares what the file name is?'
0,0,0,0,0,'Or, where is this possible inefficiency ?\r\n'
0,1,0,0,0,+1
0,0,0,0,0,'\r\nI wonder how often apps \"typically\" need just the parent docs vs the groups ...\r\n\r\n'
1,0,0,0,0,'That means that I have no preference for 2454 or this one.'
0,0,0,0,0,'then we wouldnt have to track additional variables in each indexinput impl, only change openinput and the ctors to pass this information.\r\n\r\n'
0,0,0,0,0,'This thing can implement\r\n'
0,0,0,0,0,'Patch. '
0,0,0,0,0,'\r\neasily, and an iterator over this can implement\r\n'
0,0,0,0,0,'Some more thoughts:\r\n * maybe IndexInput should have a ctor like IndexInput\r\n * then, in *all* error messages  we could include this, for better error descriptions\r\n * when directories call openInput they could pass whatever they want , and CFS could include the \"real\" file within cfs\r\n * we could declare this as just an opaque string describing the indexinput  as well)\r\n * if we do this, i think it should be non-null \r\n\r\nIf we did it consistently like this, i think it could improve error messages and debugging and be a pretty clean change.'
1,0,0,0,0,'Patch w/ test.'
0,0,0,0,0,'Occasionaly the need for a more space efficient filter shows up on the mailing lists, so if anyone wants to give'
0,0,0,0,0,'Ie, the Query/Collector would still be visiting these many child docs per parent, I guess? '
0,0,0,0,0,'But, still this patch only calls .nextSetBit once per group so that ought to be faster than [-LUCENE-2454-|https://issues.apache.org/jira/browse/LUCENE-2454], I think... hmm, unless you typically only have 1 child match per parent.'
0,0,0,0,0,'The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.\r\n\r\n'
0,0,0,0,0,'Another implementation  could be a set of increasing integers, based on a balanced tree structure with a moderate fanout , and all integer values relative to the minimum determined by the data for the pointer from the parent. The whole thing could be stored in one int[], the pointers would be  indexes into this one array, and each internal node would consist of two rows of integers , and each row would be compressed as a frame of reference into the array.\r\n\r\n'
0,0,0,0,0,'Its up to the implementation using lucene to keep track of its directory name .\r\n\r\n'
0,0,0,0,0,'Compression at higher levels might not be necessary.\r\n\r\n'
0,0,0,0,0,'I\'ll commit this soon and resolve [-LUCENE-2454-|https://issues.apache.org/jira/browse/LUCENE-2454] as duplicate!'
0,0,0,0,0,'I agree with Robert, adding this to IndexInput is stupid.\r\n\r\n'
0,0,0,0,0,'like if IndexInput takes String name in its ctor , and implements toString itself.\r\n\r\n'
0,0,0,0,0,'I\'ll rework the patch...'
0,0,0,1,0,'I now pass DataInput down to IndexFormatTooNew/OldExc, and .toString() it, and impl\'d .toString in the all the IndexInput impls I could find.'
0,0,0,0,0,SegmentTermsEnumReader).
0,0,0,0,0,'There are only few places where we pass null as file name to the exception . '
0,0,0,0,0,'But i\'m still not sure how useful this is.\r\n\r\n'
0,0,0,1,0,'But this is a real serious trap. '
0,1,0,0,0,'Woohoo! '
0,1,0,0,0,'Its better than coping into a StringBuilder before and after. '
0,0,0,1,0,'attached is an updated patch, with examples in the overview etc.\r\n\r\n'
0,0,0,1,0,'I would like to commit at the end of the day if no one objects.'
1,0,0,0,0,'The code to do this is available here\r\n\r\n[https://github.com/sudarshang/lucene-solr]\r\n\r\nPlease see \r\n[https://github.com/sudarshang/lucene-solr/blob/master/lucene/spatial-suggest/src/java/org/apache/lucene/search/spatial_suggest/WFSTGeoSpatialLookup.java].\r\n\r\n'
0,1,0,0,0,+1
1,0,0,0,0,'The modifications I made to the WFST suggester are not for boosting the suggestions by geodistance but to restrict the suggestions to a particular geographical location. '
0,0,0,1,0,'Thanks Uwe, i will remove the \"crude benchmark\" (as you can bench tokenfilters with benchmark), and add some examples and stuff to overview.html'
1,0,0,0,0,'Initial patch fixing the comparator to take into account the payload when everything else is same.'
0,0,1,0,0,'More non-reproducing master failures from my Jenkins, commit shas are all after Mike\'s commit on this issue:'
0,0,0,0,0,'This remark the in javadocs would allow us to wait for someone to come along with bigger group sizes and a real performance problem here.\r\n\r\n'
0,0,0,0,0,'New patch, I think it\'s ready to commit!'
0,0,0,1,0,'Committed revision 937039.'
0,0,0,1,0,'Bulk close for 3.1'
0,0,0,1,0,'Bulk close for 5.3.0 release'
1,0,0,0,0,'Looking at the first failure, I have a simple unit test that demonstrates it'
0,1,0,0,0,'Looks good, +1. '
0,0,0,0,0,'It\'s unrelated to my fixes here? '
0,0,0,0,0,'\r\nHmm I don\'t think so. '
0,1,0,0,0,'\r\n+1'
0,0,0,0,0,'Ie, that class is \"private\" to the indexing chain; it\'s a like a codec, that\'s used to buffer postings in RAM until we write them to the \"real\" codec, and in theory an app could swap in a different indexing chain that didn\'t steal a bit from the posDelta...'
0,0,0,1,0,'backported to 3x, revision 941698'
0,0,0,1,0,'attached is a patch, its a little ugly since CharTermAttribute doesn\'t implement Replaceable '
0,0,0,0,0,'attached is a new test demonstrating this: for some codecs it triggers an assert, for others it makes a corrumpt index. '
1,0,0,0,0,patch
0,0,0,0,0,'I think it\'s ready!'
0,0,0,0,0,'Here I just \"improved\" the error message you see when you get an IOE, CorruptIndexE, IndexFormatTooXE '
0,0,0,0,1,'Another non-reproducing failure,'
0,0,0,0,1,'The problem is this guy is configurable via IndexWriterConfig too. '
0,0,0,0,0,'????, I can reproduce, by clearing  and running  then . Thanks for reporting!\r\n\r\nAlternativeI have no '
1,0,0,0,0,'The 3x version is read-only and only deals with what was supported in 3x. 4x FieldInfo format has no backwards logic in there anymore too.\r\n\r\n'
0,1,0,0,0,'Go for it, its a private impl class, what should we do else. '
0,0,1,0,0,'On the other hand, I am not sure if it reduces the size of the index, as it just move the data from the skip list to the inverted file. '
0,0,0,1,0,'Bulk close after release of 3.5'
1,0,0,0,0,'eproduce, by clearing  and running  then . Thanks for reporting!\r\n\r\nAlternativeI have no idea why a javadoc jar would be depended on - it\'s definitely not intentional - my first guess is that there is some form of transitive dependency resolution happening - again, unintentionally.\r\n\r\nAlternativeI\'m i'
0,1,0,0,0,'Thanks Dawid!'
1,0,0,0,0,'We could then also make ThreadAffinityDWPTpool pkg-private too.'
0,0,0,1,0,'Bulk close after 5.0 release.'
0,0,0,0,0,'To me, based on these values, the answer is to revert and then refactor.\r\n\r\n'
0,0,0,0,0,'Also, is the 9 in the last column of the second table  an outlier or a c & p error? '
0,0,0,0,0,'That see'
0,0,0,0,0,'we can try to improve the language etc later but its a start.'
0,0,0,1,0,'I\'ll fix.\r\n'
0,0,0,1,0,'Attached is a \"boilerplate\" patch for the analysis module.\r\n\r\n'
0,0,0,1,0,'It\'s much less important that we perfectly *enforce* it 100\% of the time :)\r\n\r\nIt\'s just like in the U.S., how we have laws created over time by our legislative branch, but then separately \"enforced\" by the judicial branch and sometimes scary police forces. '
0,0,0,0,0,'I think a module having its own would make sense\r\n * ... other things?'
1,0,0,0,0,'How about simply removing this from IWC?\r\n\r\n'
1,0,0,0,0,'New patch folding in Robert\'s idea....\r\n\r\n'
0,1,0,0,0,'This looks great Robert!'
0,0,0,1,0,'Committed [-LUCENE-2444-|https://issues.apache.org/jira/browse/LUCENE-2444]_boilerplate.patch revision 941369.'
0,0,0,1,0,'ok if no one objects i\'ll commit this boilerplate stuff soon.\r\n\r\n'
0,0,0,1,0,'there was a problem, the contrib-uptodate macro assumes contrib/*\r\n\r\nSo this patch fixes the problem by adding a module-uptodate macro, you can test it\r\nby following the same instructions, but additionally doing \'rm -rf contrib/analyzers\'.'
0,0,0,0,0,'i applied this patch to a checkout, then removed contrib/analyzers completely.\r\n\r\n'
0,1,0,0,0,'I think setByte should take byte, setInt int, setLong, long etc.'
0,0,1,0,0,'This seems pretty expert to expose publicly? '
0,0,0,1,0,'I love how you pass Norm to the sim and it sets\r\nwhat it wants...\r\n\r\n'
0,0,0,1,0,'have awesomely huge disagreements over where a space or a  goes ([https://en.wikipedia.org/wiki/Parkinson\'s_law_of_triviality]) and never actually make progress building search engines.\r\n\r\n'
0,1,0,0,0,'+1, I like that solution, Robert. '
0,1,0,0,0,Thanks [~arcadius]
0,0,0,0,1,'So, yes, much of our code currently \"violates\" the law, but that\'s relatively minor problem'
0,0,0,1,0,'I also remove setByte in favor of setInt(byte)\r\n\r\nI think its ready!'
0,0,0,1,0,'I found the Eclipse CodeStyle.\r\nApparently spacing related it wants :\r\n - private int test(int a,int b) -> no spaces in the declaration\r\n - \"expanded\".equals(n) -> no spaces in invocation\r\n\r\nAnd I find a lot of cases that satisfy this scenario.\r\nActually that seems the standard.\r\nIf we agree that, probably the reason it was not in the codeStyle.xml is that Intellij Idea put it by default !\r\n\r\nMy assumption that something was wrong is because the class in SolrJ I have been working with is not following the standard, hence my confusion :)'
0,0,0,0,0,'This is just a step. '
0,0,0,0,0,'Then we can keep the issue open and discuss what/if-any additional\r\nthings should be moved to the module:\r\n * LICENSE/NOTICE? '
0,1,0,0,0,'Currently by default the Codestyle is consistent with spaces.\r\n'
0,1,0,0,0,'\r\nI agree. '
0,0,0,1,0,'The problem was actually with un-consistent classes already committed that were causing the confusion.\r\n'
0,0,0,0,0,'I just made this patch up to show the problem though!'
0,0,0,0,1,'Almost got this working, two bugs to resolve:\r\n # a bug in eclipse compiler (imo), i tell it to create no class files, but its creating some for spatial (package-info.class processing) because it uses package-info.java instead of package.html. '
0,1,0,0,0,'Awesome! '
0,0,0,1,0,'Committed revision 941308.'
0,1,0,0,0,'We can close this now.'
0,0,0,1,0,'noformat\r\nmkdir -p modules/analysis\r\nsvn add modules\r\nsvn move lucene/contrib/analyzers/* modules/analysis\r\npatch -p0 < ../LUCENE-2444.patch\r\nnoformat'
1,0,0,0,0,'I just committed your patch with a small tweak to the if statement logic in the comparator...'
0,0,0,1,0,'i also added the \'dont skip when close\' opto to Sep and Preflex codecs (since i neglected to do this and only did Standard).\r\n\r\n'
0,0,0,1,0,'If we were law-less, we would (trust me, I\'ve seen it!) '
0,1,0,0,0,'I agree, I will change this accordingly for all methods in Norm. '
0,0,0,1,0,'I added to the demo-part-2 instructions that you should get a source checkout.'
0,0,0,1,0,'\r\nLucene uses Sun\'s java coding conventions, apparently moved here: [http://www.oracle.com/technetwork/java/codeconvtoc-136057.html]\r\n\r\nWith one exception: 2 space indent, not 4.'
0,0,0,1,0,'Still, this way, if ever there is a disagreement on code styling, we can immediately fall back on our \"law\" to quickly resolve it. '
0,0,0,1,0,'Here are the instructions:\r\n'
0,0,0,1,0,'I made getAndLock pkg-private (since its effectively is anyway, does no harm),\r\nin r1329024 to fix the broken javadocs links... but we should figure out what we want to do here\r\nfor a real fix.'
0,1,0,0,0,+1
0,1,0,0,0,'+1 That\'s pretty damn cool'
0,0,0,1,0,'Ill run precommit and get this thing in (trunk/4x only): i spent a lot of time cleaning up docs and want to keep the bar high.\r\n\r\n'
0,0,0,0,1,'* Manager.decRef()-- I think you should searcher.reader.incRef() if taxoReader.decRef() failed?\r\n '
0,0,0,0,1,'The test has a whitespace in pathname problem. '
0,0,0,0,1,'About the patch:\r\n * Why does the test uses newFSDirectory? '
0,0,0,0,0,'view=revision&revision=1389492]\r\n\r\nMerged revision 1389491 from lucene/dev/trunk:\r\n[-LUCENE-4409-|https://issues.apache.org/jira/browse/LUCENE-4409]: Improve ECJ-Linter  + fix whitespace bug'
0,0,0,1,0,'\r\n - It also adds taskname=\"ecj-lint\" als param to javac, so the log output is nice'
0,0,0,1,0,'Committed trunk revision: 1389491\r\nCommitted 4.x revision: 1389492'
0,1,0,0,0,+1
0,0,1,0,0,'I.e. in your patch, opening a pair on a Directory assumes that the pair is \"valid\", which may not be the case at all. '
1,0,0,0,0,'I think I had issues recording that in the IW commitData as well, because you need to first commit IW, but the epoch on TW commitData is unknown until it is committed... and pulling it from DirTaxoWriter\'s member is dangerous because in between you might get a replaceTaxo call which increments the epoch.\r\n\r\n'
0,0,0,0,0,'we can adjust the properties as needed later as more cleanup happens, but i dont want to let them get any worse.'
0,0,0,1,0,'I\'ll fix it to use docRoot.'
0,1,0,0,0,'+1, thanks for cleaning up. taskname is cool, i didn\'t like the log output but didnt know about this.'
0,0,0,1,0,'I\'ll make the macro use a throwaway directory and delete it'
0,0,0,1,0,''
0,0,0,1,0,'If no one objects, i\'d like to commit this first patch today to move the code in SVN.\r\n\r\n'
0,1,0,0,0,+1!
0,0,0,0,1,'* It\'s odd that acquire() throws IOE ... '
0,0,0,1,0,'New patch, added a test case, and fixed PSDP to detect if you try to snapshot/release when it\'s not being used by an IW ... '
1,0,0,0,0,'That way, TW always contains ordinals that are >= than what IW knows about, in which case they are \"valid\". '
0,0,0,1,0,'I have a patch fixing those 2 problems. '
0,0,0,0,1,'Maybe the dictionaries are under ASL eventually?'
0,0,0,0,1,'\r\n # a bug in solrj javadocs: it links to the lucene queryparser syntax incorrectly. '
0,0,0,0,1,'I don\'t know why this is working with \'ant javadocs\', but it really shouldnt, since lucene queryparser should not be in its compile classpath. '
0,0,0,1,0,'For that reason, I created (in the un-posted patch) a taxonomy timestamp class (which we can treat as version or something similar) which is written to both TW\'s and IW\'s commitData, and the manager checks for that at initialization to ensure the two actually match.\r\n\r\n'
0,0,0,1,0,'Closed after release.'
1,0,0,0,0,'updated patch: everything is passing.\r\n\r\n'
0,1,0,0,0,'\r\nAll tests pass.'
0,1,0,0,0,'I think it\'s ready.'
0,0,0,0,0,'I will commit that soon, just'
0,0,0,0,1,'I didn\'t read through, but can\'t it work with RAMDirectory too?\r\n '
0,0,0,1,0,'\r\n - The <compilearg/> has to use value=\"...\" instead of line, otherwise it breaks with whitespace in the properties file path.\r\n\r\n'
0,0,0,0,0,'I will add a CHANGES entry in both Lucene and Solr, because this affects both projects. '
0,0,0,0,1,'But, if the app\'s commit logic is fine, yet it opened IW and TW with OpenMode.CREATE, then the taxonomy will include ordinals that may be completely unrelated to what IW stores, right? '
0,0,0,0,1,'The change from HashSet to ArrayList for flatQueries resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. '
0,0,1,0,0,'This is incomplete, there no tests.\r\n'
0,1,0,0,0,+1
0,0,0,1,0,'Bulk close resolved 4.4 issues'
1,0,0,0,0,'Here is a patch.'
0,0,0,1,0,'Bulk close for 5.3.0 release'
1,0,0,0,0,'Patch of 15 Nov 2015.\r\n'
1,0,0,0,0,'Most of the changes are to pass numDocs down to where it is actually used:\r\nConjunctionDISI, DisjunctionDISIApproximation, DisjunctionScorer, ConjunctionSpans, SpanOrQuery.\r\n\r\n'
0,1,0,0,0,'Patch looks good, only one minor idea for improvement:\r\n\r\nWhen we throw \"read past EOF\" I think we should throw java.io.EOFException (extends IOException) rather than a generic one?'
0,0,0,0,0,'Hopefully we add more test methods to this base class!'
0,1,0,0,0,'MinShouldMatchSumScorer only has the disjunctions done.\r\n'
0,1,0,0,0,'+1, nice that all codecs support offsets now!'
0,0,0,0,0,'I can backport if we want to, but I\'m not sure its worth the trouble here. '
0,0,0,1,0,'Bulk close after 5.0 release.'
1,0,0,0,0,'Patch. '
0,0,0,1,0,'Closed after release.'
0,0,0,1,0,'What happens if a sim doesnt ever set anything in computeNorm at all for a field? \r\n'
0,0,0,0,0,'Commit 1683744 from [~jpountz] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1683744] ]\r\n\r\n[-LUCENE-6526-|https://issues.apache.org/jira/browse/LUCENE-6526]: Revert some changes that were committed by mistake.'
0,0,0,1,0,'OK will do; just add @deprecated / @Deprecated to JaspellLookupFactory right?'
1,0,0,0,0,'Here\'s the patch I ended up with when working on this on top of 3.x (don\'t remember if it was 3.5 or 3.6). '
0,0,0,0,0,'We still have 3.x codec in 4.x, as well as the fact the blockterms readers/indexes have changed in trunk and have better testing: so backporting poses some risks.'
0,0,0,0,0,'Ugh, I thought my commit went in before 7.x/7.0 branched; I\'ll back port tomorrow, and look into the new test failures!'
0,0,0,0,0,'SpanContaining and SpanWithin use the conjunction estimation, the'
0,0,0,0,0,'There are very likely situations where you can still decrease query runtime even further with a different order of clauses than the one based on current worst-case estimates, and I agree that the naming \'cost\' doesn\'t really reflect the conservative estimates. '
0,0,0,0,0,'Which phrase should lead the top level conjunction?'
0,0,0,0,0,'That one is actually solved nowadays by the two phase approach.\r\n'
0,0,0,0,0,'\r\nBut so would the current worst-case approach?'
0,0,0,0,0,'It comes down to trading in worst-case behavior to gain average/throughput, but usually people care more about the slowest/hardest queries. '
0,0,0,0,0,'However, any other non-worst-case estimate might err very badly and make queries that are currently reasonably fast extremely slow.\r\n\r\n'
0,0,0,0,0,'Suppose the query is a conjunction of 2 phrases \"a b\" and \"c d\". a and b each occur in 10\% of the docs, c in 9.7\% and d in 50\%.\r\n'
0,0,0,0,0,'For un/ordered NearSpans there is a division by 4  and by 8  for zero allowed slop, something like this should also be done for the PhraseQueries.\r\n'
0,0,0,1,0,'OK I committed the deprecation to trunk; I\'d really like to just remove it, but we can\'t do that until we address [SOLR-6178|https://issues.apache.org/jira/browse/SOLR-6178] ... so I\'ll leave this issue open to remove Jaspell in trunk.'
0,0,0,1,0,'But second\r\noff, even if it does that, there is at least this multi-threaded case\r\nwhere .maybeRefresh can screw up:\r\n * Thread 1 (indexer) commits IW1\r\n * Thread 1 (indexer) commits TW1\r\n * Thread 2 (indexer) commits IW2\r\n * Thread 3 (searcher) maybeRefresh opens IW2\r\n * Thread 3 (searcher) maybeRefresh opens TW1\r\n * Thread 1 (indexer) commits TW2\r\n\r\nThat will then lead to confusing AIOOBEs during facet counting...\r\n\r\n'
0,0,0,1,0,'Backport to 3.1'
0,0,0,0,0,'Because then Solr prints a warning on startup if they are used.'
0,0,0,0,0,'Commit 1603521 from [~mikemccand] in branch \'dev/branches/lucene_solr_4_9\'\r\n[ [https://svn.apache.org/r1603521] ]\r\n\r\n[LUCENE-5775|https://issues.apache.org/jira/browse/LUCENE-5775]: Deprecate JaspellLookup; fix its ramBytesUsed to not StackOverflow'
1,0,0,0,0,'\r\nBasically just sum up \"downwards\" in the tree ... I\'ll commit this & deprecate in 4.x, and remove in trunk.'
0,0,0,1,0,'I\r\nsuppose I could do a \"best effort\" tryIncRef so that if the app\r\nsomehow catches the exception and retries the decRef we don\'t\r\nprematurely close the reader ...\r\n'
0,0,0,0,1,'Hi,\r\nshould we maybe also deprecate the Solr factories? '
0,0,0,1,0,'Type give: \" + type);\r\nI think this should be UOE?\r\n\r\n'
0,0,0,1,0,'Just because it\'s using the LineFileDocs, which have biggish docs in\r\nthem. '
0,0,0,0,0,'Removing 3.6 Fix version. '
0,0,0,1,0,'It also prints a nice taskname instead of [javac] on ANT output.'
1,0,0,0,0,'Simple patch fixes it:\r\n'
0,0,0,1,0,'With Eclipse it looks like this should be enough to deprecate the factory you mentioned. But the default lookup for file-based stuff in Solr is still Jaspell:\r\n'
0,0,0,1,0,'Hmm this isn\'t so simple: that decRef could have closed the reader. '
0,0,0,0,0,'Commit 1603523 from [~mikemccand] in branch \'dev/branches/branch_4x\'\r\n[ [https://svn.apache.org/r1603523] ]\r\n\r\n[LUCENE-5775|https://issues.apache.org/jira/browse/LUCENE-5775]: Deprecate JaspellLookup; fix its ramBytesUsed to not StackOverflow'
0,0,0,0,0,'If I\'ll make it by the release, I\'ll put it back.'
0,0,1,0,0,'One thing: estimatedDocCount is long, but docIds in Lucene are still int - this makes no sense, because the current scorer/disi interface can never return anything > Integer.MAX_VALUE, so the estimatedDocCount can never be 64 bits.'
0,1,0,0,0,'This showed no significant differences to current trunk, the differences where never bigger than 2.1\% either way, and well within the standard deviations.\r\n'
0,1,0,0,0,'Another reason why I started this is that the result of cost() is also used as weights for matchCost() at [-LUCENE-6276-|https://issues.apache.org/jira/browse/LUCENE-6276], and I\'d prefer those weights to be as accurate as reasonably possible.\r\n\r\n'
0,0,0,0,0,'For the proximity queries  this reduces the conjunction cost using the allowed slop.\r\n'
0,0,0,0,0,'I might try to add more complex queries to the benchmark later.'
0,0,0,0,0,'Would it be worthwhile to open a separate issue'
0,0,0,0,0,'I think we can keep this  as a possible alternative until the current implementation gives a bad result.\r\n\r\n'
0,0,0,0,0,'Here is the benchmark output, it might be good for future reference:\r\n'
0,0,0,0,0,'I tried this patch with the wikimedium5m benchmark. '
0,0,0,0,0,'However, maybe we can have worst-case and other estimates too and choose to use the latter only in cases where even making the wrong decision won\'t be too bad, so that you\'re speculative on the fast queries to gain throughput, but conservative on'
0,0,0,1,0,'I think it\'s OK to add IOE to the signature?'
0,0,0,1,0,'First off, the app must always commit IW first then TW. '
1,0,0,0,0,'I wonder if we should throttle it? '
1,0,0,0,0,'OK I discussed these tricky issues with Shai ... with the non-NRT case\r\n(app commits and then calls .maybeRefresh) there are some big\r\nchallenges.\r\n\r\n'
0,0,0,1,0,'We must still document that you cannot do IW.deleteAll /\r\nTW.replaceTaxonomy (I\'ll add it).\r\n'
0,1,0,0,0,'I did some live test with the standalone techproducts example. proproI have seen no issues, so I think this should'
0,0,0,0,0,'Hmm seems like something is wrong w/ BQ\'s bulk scorer, because if I take it out and just use the default:\r\n'
0,1,0,0,0,'looks good!'
0,0,0,1,0,'shouldn\'t we combine Norm and DocValueNorm into one final class (Norm) and pull it out of Sim?\r\n'
0,1,0,0,0,'I see: this makes sense. '
1,0,0,0,0,'I think it\'s better if the \"fake\" onCommit only arrives when IW.deleteUnusedFiles is invoked'
0,1,0,0,0,'\r\nAhh ok. '
0,0,0,1,0,'\r\nI don\'t like that cutting over to DTW would open up the thread hazard\r\nthat we fail to catch the replace ... admittedly it\'d be rare but why\r\nopen it up? '
0,1,0,0,0,'\r\n\r\n+1 to your suggestion about ShingleFilterTest.TestTokenStream:\r\n'
0,1,0,0,0,'The added Expert/@lucene.internal method seems minor ...\r\n'
0,0,0,1,0,'I added final String resourceDescription to II, returned from\r\ntoString, made it required arg to the ctor, and fix all II subclasses\r\nto pass something reasonable.\r\n\r\n'
0,0,0,0,0,'Commit 1556485 from [~jpountz] in branch \'dev/branches/lucene_solr_4_6\'\r\n[ [https://svn.apache.org/r1556485] ]\r\n\r\n[-LUCENE-5361-|https://issues.apache.org/jira/browse/LUCENE-5361]: Fixed handling of query boosts in FastVectorHighlighter.'
0,1,0,0,0,'\r\nOk.\r\n'
1,0,0,0,0,'Patch; it turned out to be easier than I expected: I just tapped into the existing logic that ShingleFilter has for handling holes between tokens.'
1,0,0,0,0,'+1 for backport to 4.10.x'
0,0,0,1,0,'fixing a race ... so I\'m hoping there are no more failures in this challenging test :)'
0,0,0,0,1,'For the same reason, i think the boostingscorer (since its just an implementation detail of how the current BS2 stuff solves this case) should be transparent.'
0,1,0,0,0,'Reopening to backport to the 4.10 branch'
0,1,0,0,0,'If this is the case, aren\'t we now able to represent (in 4.x codec) that norms are not omitted for the field,\r\nthey just don\'t exist at all (e.g. all documents without norms are deleted).\r\n'
0,0,0,1,0,'I think it\'s ready.'
0,0,0,1,0,'In other words, the current wiring does make the BoostedScorer transparent but with the disadvantage of hiding the actual scorer that performs the work.\r\n\r\n'
0,0,0,0,0,'In this scenario, a BooleanQuery containing two TermQueries  returns the following from BooleanWeight.scorer:\r\n * BoostedScorer\r\n ** TermScorer \r\n\r\nCalling getChildren on this returns an empty list because the BoostedScorer just returns in.getChildren and thus you are unable to navigate to the actual TermScorer in play. '
1,0,0,0,0,'Here\'s a new patch w/ that TODO done ... '
0,1,0,0,0,'+1, patch looks good'
0,0,0,0,0,'If you have a custom scorer you may need access to the raw one, so this makes sense to remove the transparency... '
0,0,0,0,0,'This has been working great across quite a few Lucene releases but failed when I upgraded to 4.9 due to the two regressions in behavior for Scorer.getChildren as described in this ticket.\r\n\r\n'
0,0,0,0,1,'I don\'t think we should remove the default implementation for FilterScorer, as the scorer is not really changed when using this abstract class, its just wrapped? '
0,1,0,0,0,'Thanks Terry!'
0,0,0,0,0,'Commit 1608454 from [~rcmuir] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1608454] ]\r\n\r\n[-LUCENE-5796-|https://issues.apache.org/jira/browse/LUCENE-5796]: Fix Scorer getChildren for two combinations of BooleanQuery'
0,0,0,0,0,'I\'ll look at the patch again and reply back if I have more '
0,0,0,0,0,'If this is an unsupported workflow, I\'m happy to move the discussion over to th'
0,0,0,0,0,'Commit 1556484 from [~jpountz] in branch \'dev/branches/branch_4x\'\r\n[ [https://svn.apache.org/r1556484] ]\r\n\r\n[-LUCENE-5361-|https://issues.apache.org/jira/browse/LUCENE-5361]: Fixed handling of query boosts in FastVectorHighlighter.'
0,0,0,0,0,'This would impact any classes that extend FilterScorer and don\'t override getChildren. '
0,0,0,1,0,'This patch also adds context class loaders on the forbidden api list.'
0,0,0,0,0,'Commit 1608457 from [~rcmuir] in branch \'dev/branches/branch_4x\'\r\n[ [https://svn.apache.org/r1608457] ]\r\n\r\n[-LUCENE-5796-|https://issues.apache.org/jira/browse/LUCENE-5796]: Fix Scorer getChildren for two combinations of BooleanQuery'
0,0,0,1,0,'The reason that I advocated changing FilterScorer and BoostedScorer is to allow some of my custom Query implementations to use a regular BooleanQuery for recall and optionally scoring while taking advantage of the actual Scorers used on a per document, per clause basis.\r\n\r\n'
0,0,0,0,1,'Hi,\r\nwould it be possible to remove the description from the top-level modules?\r\n\r\nAs the ant beast target does not work, it should not appear in \"ant -projecthelp\".'
0,0,0,1,0,'It is very much like the old instantiated (though I think its terms dict is faster than instantiated\'s)... but I didn\'t really like the name \"Instanstiated\"... I had picked Direct because it \"directly\" represents the postings ... but maybe we can find a better name.'
0,1,0,0,0,'Thats a good point and started bugging me last night when I was thinking about it.'
0,0,0,0,1,'Maybe CharTermAttributeImpl should extend this class or wrap it somehow - and we move all stuff out of CharTermAttributeImpl (and just add Attribute-specific stuff)?\r\n\r\n'
0,0,1,0,0,'Also the current patch misses to append the String \"null\", if the CSQ is null.'
0,1,0,0,0,+1!
1,0,0,0,0,'Patch .'
1,0,0,0,0,'Here is a simple test. '
1,0,0,0,0,'This patch adds more param parsing methods to AbstractAnalysisFactory, including get(), require(), getFloat(), getChar(), and getSet(), and changed all analysis factories to use them where appropriate.'
0,0,1,0,0,'There\'s a lot of things i\'m not happy with in the patch, i think it was more of an exploration of ideas.\r\n'
1,0,0,0,0,'Previously this was \'required\' because the method required that you return a byte.\r\n'
0,0,0,0,1,'There are also other things: remove IOException (you did that already, it is there for stuff like BufferedWriter that implement Appendable). Important is the following: Let the append methods use covariant return:\r\n'
0,0,0,1,0,'There are potential things to share with CharTermAttributeImpl but I\'d rather leave it to another issue as it has some pitfalls as noted.'
1,0,0,0,0,'Here is a simple patch.'
0,0,1,0,0,'The previous patch had a bug: coord was computed for the conjunction as scorers.length/scorers.length but it should be scorers.length/maxCoord (in the case of optional clauses that happened to not exist in the segment).\r\n'
0,0,0,0,0,'\r\nWithout the covariant return, the typical \"chaining\" is hard, because suddenly the builder changes its type : [http://en.wikipedia.org/wiki/Covarian'
0,0,0,1,0,'I implemented these as require(), requireXXX(), etc.'
0,0,0,1,0,'I changed this to estimateDocCount since its now on DISI'
0,0,0,1,0,'Overloading setint as setInt(short) setInt(int) setInt(byte) setInt(long),\r\nbesides being confusing (and I see no advantage to doing this), has the nice property\r\nof easily quadrupling peoples norms with no type safety unless they are like, peeking at \r\ntheir generated bytecode to verify java didn\'t promote them up to setInt(int)\r\n\r\nby making it setByte(byte) instead, if they don\'t pass an actual byte they will get\r\na compile error.'
0,1,0,0,0,'Tests all pass, and precommit\'s happy.'
0,0,0,1,0,'\r\nWoops, I missed that one ... '
0,0,0,0,0,'Wonderful! '
0,0,0,1,0,'I will commit in a moment.'
1,0,0,0,0,'New patch, just handling the NRT case.'
0,0,0,0,1,'Mike, any reason not to backport to branch_7x and branch_7_0? There was a recent failure on a Jenkins branch_7x job'
0,0,0,1,0,'That\'s also a problem in other DocValues use cases like sorting. '
0,0,0,1,0,'Patch, just making the IWC set/get package private, and fixing LTC to use reflection to gain access...'
0,1,0,0,0,' \r\n * Thanks!\r\n\r\n'
0,0,0,1,0,'Closed after release.'
1,0,0,0,0,'Patch removing context classloader usage. '
0,0,0,0,0,'If the tests make it this far  then does the query again, this time checking the \"Explanation\" for every matching document against the score.\r\n----\r\nWith this seed, both tests demonstrate an identical inconsistency between the scores of a document containing body=\"Microsoft Office 365\" between the pre-merge and single-segment indexes....\r\n'
0,1,0,0,0,'I can look into the clustering plugin\'s use of it. '
0,0,0,0,0,'\r\nshould be:\r\n'
0,0,0,1,0,'Two more questions:\r\n\r\nIn the patch omitNorms is still a separate boolean from normValueType == null.\r\n'
1,0,0,0,0,'ASM static test version that dislikes System.out / System.err'
0,0,0,0,1,'Also test() also has these 5 close() statements which can be folded into one IOUtils. '
0,1,0,0,0,'If I were to choose though, I\'d use aspectj rather than asm-based code. It just seems cleaner to me.'
0,0,0,1,0,'We design it not to throw the exception. If the exception is thrown, then it is a bug.'
0,0,0,1,0,'This is not intended for commit, but you many want to look at the manager and its validation logic. '
0,1,0,0,0,'It is indeed safer to put it on TR and let TR provide evidence \"for itself\". '
0,0,0,0,1,'Maybe we should make it .estimateHitCount instead of estimateCost, so it\'s more explicit?'
0,0,0,1,0,'Makes sense. '
0,0,0,1,0,'I think, if we do the above and are able to separately represent \'doesnt hvae any norms for this segment\' \r\nfrom \'omitNorms for this field always\', that we would just have normValueType as null, and it would all just work?'
1,0,0,0,0,'If we totally change a Query\'s API, does that push all the responsibility of the\r\nAPI designer to deal with serialization backwards compat? '
0,0,0,0,1,'The problem\r\nis if people start using such a structure (\'remote api\') that depends upon\r\nthe structure of our java source code, then they will be upset if we break it.\r\n\r\n'
0,1,0,0,0,'Patch looks good!'
0,0,0,1,0,'Here\'s the patch'
0,0,0,0,1,'The SortField name uses INT but sorts by long and so on. '
0,0,0,1,0,'I\'ll add some scary warnings  '
0,0,0,0,1,'I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?\r\n\r\n'
1,0,0,0,0,'Don\'t know if there\'s anything out of the box in findbugs/ pmd, but even if not then this can be done as a 10-liner by applying an aspect to classes via aspectj and parsing the output logs detecting if an aspect has been applied (it shouldn\'t match anywhere).'
0,0,0,0,1,'Can\'t the serialization be totally independent?'
0,0,0,1,0,Done.
0,0,0,0,1,'Back then I was thinking of how can the search and taxonomy indexes be synced. '
0,0,0,0,1,'* TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It\'s odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there\'s no reason why it shouldn\'t always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that\'s ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.  '
0,0,0,1,0,'Bulk close after 5.0 release.'
1,0,0,0,0,'I committed this as a start. '
0,0,0,1,0,'I think the case was that clustering was under shared libraries and resources loaded per-core couldn\'t figure where to load classes from.'
0,0,0,0,1,'\r\n\r\nI\'ll check why the context classloader is required in clustering later on. \r\n'
0,0,0,1,0,'Closed after release.'
0,0,1,0,0,'I recall it was unfortunately required, but will have to go into this again to remind myself why.\r\n\r\n'
0,0,0,0,0,'\r\n... which leads me to wonder if Java 9 was fixed and we should instead be using Math.toDegrees/toRadians everywhere? '
0,0,0,1,0,'\r\nThis isn\'t a huge deal though, its mostly just curiousity. '
0,0,0,1,0,'(FieldInfo.setNormValueType silently does nothing if it\'s already\r\nset). Can we throw an exception if it\'s already set to a conflicting\r\nvalue?\r\n\r\n'
0,0,0,1,0,'This looks great! '
0,0,0,0,1,'\r\nI agree this should be UOE\r\n'
0,0,0,1,0,'\r\nmaybe we should. if a user wants to do crazy stuff with norms ie. use packed ints for norms or var length bytes they should be able the do this ie return a different type then we do right now?'
0,0,0,1,0,'\r\nyeah we can do that I will look into it but I am not sure if we should rather let that patch bake in for a bit and then do that change in a second issue. '
0,0,0,1,0,'If its tricky or messy, in my opinion we could even just add an assertion for now and document \"you must set something\" in Similarity, because its a lower level API than it was in previous release (most people would generally extend higher level stuff like BM25Similarity, TFIDFSimilarity, or even DefaultSimilarity that do not expose this stuff).'
0,1,0,0,0,'here is a new patch with changes.txt and more tests for cases where norms are not written but also not omitted.\r\n\r\n'
0,0,0,1,0,'New patch, adding scary warning & MIGRATE.txt entry, fixing javadoc errors, and adding lucene.experimental'
0,1,0,0,0,'\r\nI kind of like the separation here. '
0,0,0,1,0,'Keep in mind that the only thing you can put in a norm is stuff from FieldInvertState.'
0,1,0,0,0,'Maybe instead of set and reset methods on FieldInfo just have a single\r\nset method with force boolean?\r\n\r\n'
0,1,0,0,0,'This fails for me:\r\n'
0,1,0,0,0,'Would make debugging simpler if we run into problems.\r\n'
0,0,0,0,0,'So, you\'re saying then that your approach only ever has to retrieve 4 boxes no matter the radius? '
0,0,0,1,0,'Sims that don\'t set a value, currently I have tons of asserts that enforce a value.'
0,0,0,1,0,'Lucene4 can now check if norms are present without looking at the directory.\r\n\r\n'
0,0,0,1,0,'Very nice to have a private 3x FieldInfos reader/writer!\r\n\r\n'
0,0,0,1,0,'new patch\r\n * renamed setXXX to setDouble, setFloat etc'
0,0,0,1,0,'\r\nI agree, maybe its better to get this right in this patch already I can still move the stupid file checks removed in a second issue. '
0,1,0,0,0,'Do we have any requirement that this needs to happen even if there is not a single doc with norms in that field? '
0,0,0,1,0,'\r\nthere is a test for this in TestCustomNorms (testExceptionOnRandomType) it throws a IAE in NormsConsumerPerField.\r\n'
0,0,0,0,0,'Some questions:\r\n - which package should this stuff be in? '
0,1,0,0,0,'It was a c&p error ! '
0,0,0,1,0,'The way you had to do that before was to return a bogus byte in computeNorm and ensure you always did omitNorms for the field.\r\n\r\n'
0,1,0,0,0,'\r\nBut what is the use case of having a separate Abstract class (Norm) from an implementation class (DocValueNorm) that the codec doesn\'t even provide (its instantiated as DocValues by normsconsumer[perfield] directly.\r\n\r\n'
0,0,0,1,0,'Hmm... Norm impls DocValue which I just removed... maybe we should add\r\nit back?\r\n\r\n'
0,0,0,1,0,'If we are going to have separate classes, then norm should be abstract and the codec should provide the implementation. '
0,0,0,1,0,'What happens if an app (incorrectly) tries to change up the normType\r\nsuddenly...? '
0,0,0,1,0,'Previously you always had to return something, we didnt even have the option for a sim (like basic tf * idf) to not encode any length normalization information. '
0,0,0,0,1,'I don\'t think we need this interface at this point.\r\n'
0,0,0,1,0,'Bulk close 4.7.1 issues'
1,0,0,0,0,'\r\nI agree, that case is crazy today and it shouldn\'t block nor confuse the issue. '
0,1,0,0,0,'this patch actually doesn\'t even write norms if not absolutely needed. '
0,0,0,1,0,'If you change in the middle of an IW session... it\'s\r\nquietly ignored and you get 0/empty bytes indexed instead?\r\n'
0,1,0,0,0,'+1, looks good!'
0,0,0,0,1,'If there is a case where these bogus norms are required we need to have a test for it.'
0,0,0,1,0,'But we need a real use case as to why a codec would need to customize the implementation of Norm to justify this.'
0,0,0,0,0,'DecisionFieldCache is in o.a.l.search, and the reader is in o.a.l.index.\r\n - ther'
0,0,0,1,0,'Otherwise there is no point in writing OMIT_NORMS bit in the fieldinfoswriter because it could be represented by normValueType of 0.\r\n'
0,0,0,0,0,' o.a.l.index.\r\n - there are a bunch of FieldCache-specific queries and filters. Can these just be reworked to '
0,1,0,0,0,'Looks good to mee. '
0,0,0,1,0,'Because of that I used the  withContextClassLoader(ClassLoader, () -> ...)  lambda method.'
0,0,1,0,0,'Tests seem to pass, unfortunately Solr trunk is very unstable. Some unrelated tests also fail on Jenkins, so I cannot be sure all is fine.'
0,0,0,1,0,'\r\n * moved Norm our of similarity and merged it with DocValueNorm\r\n * use UOE in simpletext and 3x norms\r\n * utilize normValueType to actually not write norms if only bogus norms would be written. '
0,1,0,0,0,'I did some live test with the standalone techproducts example. '
0,1,0,0,0,'I have seen no issues, so I think this should be fine to commit. \r\n\r\n'
0,0,0,1,0,'I added changes and migrate entries and committed to master (7.0)'
0,0,0,0,0,'AFAICT, that is covered by the current code.'
0,0,1,0,0,'\r\nright!\r\n'
0,0,0,1,0,'Committed Nicolas\' updates to trunk'
0,0,0,1,0,'Couldn\'t we take advantage of this to provide a true fix for [-LUCENE-3678-|https://issues.apache.org/jira/browse/LUCENE-3678], removing the bogus fileExists() stuff in NormsFormat\'s files()?\r\n\r\n'
0,1,0,0,0,'\r\nI will fix that.\r\n'
1,0,0,0,0,'Maybe if they are really useful we should see if we can pull them out into static methods.'
1,0,0,0,0,'= null.\r\n'
0,0,1,0,0,'FWIW CharTermAttribute.java has some optimizations for this method. '
0,0,0,0,0,'Committed to lucene_solr_4_10.'
0,0,0,1,0,'Patch looks good! '
0,0,0,0,0,'Re checked and you are right.\r\n\r\n'
1,0,0,0,0,'Several points:\r\n\r\nFirst, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. '
0,0,1,0,0,'In my opinion its not worth it!\r\n\r\n'
0,0,0,0,1,'Nicolas,\r\n\r\nWhy the change in the best fit algorithm? '
0,1,0,0,0,'I don\'t like the inconsistency of Field.setValue(T value) but Norm.setT(T value)... we should somehow reconcile how we name the native type setters on these \"value holder\" classes... but we can sort this later.\r\n\r\n'
0,0,0,1,0,'I plan to commit this soon if nobody objects.'
0,0,0,0,0,'I do agree, it is odd.\r\n\r\n'
0,0,0,0,0,'updated description with work around for older sources'
0,0,0,0,1,'I think we should also use a buffered reader in FileDocument?\n'
0,0,0,0,0,'Do you have a reference to calcul'
0,0,0,0,0,'With your fix the code is doing the right thing concerning the prime meridian.\r\n\r\n'
0,0,0,1,0,'So, mine should only be considered as a cosmetic one.\r\n'
0,1,0,0,0,'If you do no find that the logic is clearer with it, you can ignore it.'
0,0,0,0,0,'I read too fast your patch without the global code context.\r\n'
0,0,0,1,0,'Committed to trunk and 4x.'
0,0,0,0,1,'Also its not nice to permgen. '
0,0,0,1,0,'\r\nIf the search area cross the prime meridian we spilt the addBoxes call in two : one half East of the meridian, one half West to handle the jump from -180 degree to 180\r\nIf not we do a normal addBoxes call'
0,0,0,0,1,'\r\nHow so?'
0,0,0,0,1,'I also added some sugar to FieldInfo to check if the norms are present.\r\n'
0,1,0,0,0,'Looks good. '
0,1,0,0,0,'\r\nfixed my fault'
0,1,0,0,0,'Merging would also periodically coalesce and write a new full vector...'
0,0,0,0,0,'\r\nGood point, I shoul'
0,0,0,0,0,'Given that there are very large areas that only have small towns, it could happen that the population dimension does not get indexed at all in these areas?\r\n'
0,0,0,1,0,'\r\nI added some logic in there to throw an exc if it is not forced and the value !'
0,0,0,0,1,'\r\nHmm this got me curious, why is it an adversarial case if all points are equidistant from an origin?\r\n'
0,0,0,1,0,'\r\nDone. (IDE was trying to be smart... I\'ll change my settings...)'
1,0,0,0,0,'Deletes are special because 1) they are allowed to change after a\r\nsegment is written, and 2) IndexWriter carries them in RAM, rather\r\nthan writing to / reading from the filesystem, so the only\r\n\"point-in-time-ness\" is maintained by IW being careful to not change a\r\nliveDocs already paired up with a SegmentReader.'
0,1,0,0,0,'Thanks Rob, patch looks great, except: I think we can keep the illegalDocCountChange safety? '
1,0,0,0,0,'I think that we also need an object that manages IW and TW pair, so that a faceted search app calls commit() on it, and it handles the delicate commit order + whatever metadata we need to commit to make these two in sync. '
1,0,0,0,0,'Patch: I think the use-case is cool and it should be supported: its just adding an \'if\' and removing the current exception (which is geared at protecting some user who manually rm -rf\'s files from their index.)\r\n\r\n'
0,0,0,0,1,'Can you make a test change to trip it without manually removing files from your index?'
0,0,0,0,0,'This is general and would give you notification when such things happen rather than having a hack for one particular user\'s mistake.'
0,0,0,1,0,'This patch allows similarity to write all numeric values plus fixed bytes. '
0,0,0,1,0,'I improved the test a bit to ensure that cores are shared and also tested the dv updates case.'
0,1,0,0,0,'\r\nIt is index corruption though. '
0,0,0,0,0,'I think I could make a test case to trip that ...'
0,0,0,0,0,'Why hide that Mike?'
0,1,0,0,0,' This helps save conditionals since the add method never needs to care about whether the buffer is large enough or whether to upgrade to a bitset since everything is done up-front in the grow() call.'
0,0,0,1,0,'If the old reader passed in to DirectoryReader.openIfChanged(DirectoryReader, IndexCommit) is actually an NRT reader, then it seems that if there is unflushed/uncommitted data in the associated writer\'s buffers, in particular deletes, the returned reader will see those changes - thus violating the intent of opening the index at just the commit point we wanted, frozen in time. Here\'s my original test case modified to show the problem:\r\n'
0,1,0,0,0,'If you want safety against deleting files, maybe look at NIO.2 WatchService. '
1,0,0,0,0,'I think we can do both (keep our best effort check, and allow reopening to \"older\" commit point) ... here\'s a patch.'
0,0,0,0,0,'This is because liveDocs won\'t capture unflushe'
0,0,0,1,0,'Yes, I really want to. '
0,0,0,1,0,'I modernized the patch, and was able to improve how effective its check is, by switching to comparing segment IDs (a very good check that the segments changed on disk) vs what the patch used to do, comparing maxDoc.'
0,1,0,0,0,'Here is a patch to import the CartesianPolyFilterBuilder:getBoxShape part of the code from the worked we have done with Chris @ GoogleCode\r\nI tried to keep it as minimal as possible\r\n\r\nNext step is to patch LatLngRectangle in the same way'
0,0,0,1,0,'Also I moved the \"best-effort detection of rm -rf index\" up\r\nhigher... it\'s currently too low now, i.e. will fail to detect some\r\ncases that it should.'
1,0,0,0,0,'This patch changes the DocIdSetBuilder API. add() is gone. Instead, grow() returns a new BulkAdder object that can be used to add up to the number of documents that have been passed to the grow() method.'
0,1,0,0,0,'Hi [~vfunstein], I\'m glad the patched fixed your issue ... thanks for\r\ntesting.\r\n\r\n'
0,0,0,0,0,'But I just wanted to understand why deletes are so special, in that - if I don\'t have any buffered deletes for the segment, but new documents only, the reused reader instance won\'t pick them up, even without the fix in place. '
0,0,0,1,0,'\r\nI don\' t think so ... the only way I know of this happening is if an app has a reader open, then removes the index, rebuilds it, then tries to openIfChanged the reader.\r\n\r\n'
0,0,0,0,0,'If you want to check / compare code while waiting for the commit from Chris :\r\n\r\n[http://code.google.com/p/spatial-search-lucene/source/browse/trunk/src/main/java/org/apache/lucene/spatial/tier/CartesianPolyFilterBuilder.java]\r\n\r\nRegards,\r\n\r\nNicolas'
0,0,0,0,0,'I folded in your example as another\r\ntest case .\r\n\r\n'
0,0,0,0,0,'Thanks for reviewing Mike! \r\n\r\n'
0,0,0,0,0,'I\'m only looking at the license here.'
0,0,0,1,0,'Cut and paste error in complex polygon building. '
1,0,0,0,0,'OK new patch, fixing a number of things:\r\n * I close the Reader (thanks Mark).\r\n\r\n * I cutover to NumericField (and stopped using DateTools) for the\r\n\"modified\" field.\r\n\r\n * I added a -create option to IndexFiles, so you can see how to\r\nCREATE vs CREATE_OR_APPEND\r\n\r\n * I left commented-out optional things – calling optimize,\r\nincreasing IW\'s RAM buffer.\r\n\r\n * Don\'t use Version.LUCENE_CURRENT.\r\n\r\n * I sucked in test files from Lucene in Action 2E\'s tests (open\r\nsource licenses).\r\n\r\n * I use addDocument or updateDocument depending on -create.\r\n\r\n * I made the \"demo html parser\" private to modules/benchmark, which\r\nhad a dependency on it. Can someone lookover my changes to the\r\nbuild xml files? (Especially the Maven part, where I completely\r\nguessed!).\r\n\r\n * IndexHTML is gone, and the webapp (src/jsp/*) is gone too.\r\n'
0,0,0,0,0,'There is also this interesting tool: [http://babelfish.arc.nasa.gov/trac/jpf]\r\n\r\nI haven\'t used it and I don\'t know if it can handle Lucene size codebase  but if somebody has some time to play with it, it\'d be interesting to hear what it can do.'
0,0,0,0,0,'\r\nI was just saying to fetch them via ivy and then spawn a separate jvm to run them, much like you\'d do anyway if they are separate installations.\r\n\r\n'
0,0,0,1,0,'We can deal with fixing existing rawtypes or a few other classes of warning in the future.\r\n\r\n'
0,0,0,1,0,'Yes on Solr your change is not enabled: [https://github.com/apache/lucene-solr/blob/master/solr/common-build.xml#L30]\r\n\r\nWe should also review Solr (maybe in a separate issue).'
0,0,0,1,0,'Patch that ensures we won\'t add any new compiler warnings in several categories (that we\'re pretty good on already) in the future. '
0,0,0,0,1,'\r\nThe point is within TWO parts of the composite polygon. This is, of course, nonsense unless the point happens to be on a shared edge between the two. But these two don\'t even share an edge.\r\n\r\n'
0,0,0,0,0,'\r\nUnless you run into the same taskdef/classloader/sub-build/permgen-OOM problem we had with clover, and the maven-ant-tasks, and ivy that have prevented us from doing the same thing with them.'
0,0,0,1,0,'In that case, with our current splitting, running a range filter for \"small population\" will be costly. '
0,1,0,0,0,'Resolves the first problem, confirming the second.'
0,1,0,0,0,'\r\nNote, that if I comment out the updateDocument()call, the test passes. Also, if you only have one entry in the index, then it appears that while refreshing the NRT reader, the segment containing just the single delete will be removed, making it look like the test passes:\r\n'
0,0,0,0,1,'I\'m not pressing on this, this is a no-issue.'
1,0,0,0,0,'New patch.\r\n\r\nI inverted IndexFiles -create to IndexFiles -update (ie default is now create, like before).\r\n\r\nAdded reference to web-site docs in the jdocs, and added comment on the optional \"increase RAM buffer size\" that you should also -Xmx your JVM.'
0,0,0,0,0,'Besides – we already have an \'ivy warning with instructions\', the same can be done with permgen/OOM problems – detect the current  VM\'s settings  and warn/ fail the build if the defaults are too low, instructing the user to set up ANT_OPTS properly...\r\n\r\n'
0,0,0,1,0,'Keep it as it is (it overrides to do no Xlint warnings at all and don\'t fail on warning). '
0,0,0,1,0,'I will create a patch based on your work so we can get this fixed up in the next day or two.'
0,0,0,0,0,'\r\nOnce upon a time i actually implemented this for PMD in solr\'s  build.xml, but it got bogged down by some more serious problems in our automated builds and forgotten about \r\n\r\nanyway ... the patch in [SOLR-143|https://issues.apache.org/jira/browse/SOLR-143] might be helpful for the \"run lots of checks, but only fail the build for serious ones\" idea.'
0,0,0,1,0,'Committed revision 929956.'
0,0,0,1,0,'Though, without other filters (by lat/lon) it will likely be costly anyway since town population is probably Zipf\'s law like? '
0,1,0,0,0,'OK, here\'s another iteration on the patch ...\r\n'
0,1,0,0,0,'Mike: sounds good.\r\n\r\n'
0,0,0,1,0,'Here is the re worked patch, sorry for the formatting :s\r\n\r\nThis patch :\r\n - handles East AND West tiles when close to the prime meridian (either by 0 or by 180 values) where yours only catch East ones\r\n - reworks the logic to have algorithm in the code logic instead of in variables values\r\n - gives variavbles full names to better understand the code logic\r\n - separates the logic of handling the prime meridian which stays in getBoxShape from the logic of add boxes to the shape in addBoxes : addBoxes is called one time in normal case and two times for search area crossing the prime meridian (half by half)\r\n\r\nHope it better fits your need now'
0,0,0,0,0,'.\r\n\r\nAlso, both can be verbose and a pain in the ass at times when you know the code is right and they still complain... And they are long to execute s'
0,0,0,0,0,'I believe both pmd and findbugs are on maven repos so one could use ivy to fetch them automatically. '
0,0,0,0,0,'DecisionWe use both and I think FindBugs is slightly more useful than PMD but it\'s just a subjective opinion not anything I measured.\r\n\r\nAlso, both'
0,0,0,0,0,'\r\n+1 for PMD. '
0,0,0,0,0,'Patch which runs PMD much like RAT. '
0,0,0,0,0,'When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:\r\n'
0,0,0,1,0,'And they are long to execute so they should be part of jenkins nightly/ smoke tests I think, not regular builds .'
0,0,0,0,0,'lters. Can these just be reworked to be DV-specific instead?\r\n - can we consolidate the various Ints, Floats, Shorts et'
0,0,0,0,0,'p=lucene-solr.git;h=65a69b9] ]\r\n\r\n[-LUCENE-7197-|https://issues.apache.org/jira/browse/LUCENE-7197]: Fix two test failures and add more forensics that helped resolve the issue.'
0,0,0,0,0,'p=lucene-solr.git;h=d377e7f] ]\r\n\r\n[-LUCENE-7197-|https://issues.apache.org/jira/browse/LUCENE-7197]: Fix two test failures and add more forensics that helped resolve the issue.'
0,0,0,0,0,'nstead?\r\n - can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?\r\n - this still uses the global FieldCache. Should the caches be moved to'
0,0,0,1,0,'Iteration by the client code on this data structure is actually so darned easy; maybe this set iterator isn\'t really needed after all, or could be supplied with a comment so it\'s clear how to do it.\r\n'
0,0,0,0,0,'Improved patch which connects the PMD tasks to the ivy checks we have.'
0,0,0,0,0,'updated patch with 2 tweaks:\r\n # explicitly exclude pmd/ from binary dist patterns \r\n '
0,1,0,0,0,'Can we call the ant task something easier to remember, like ant lint? '
0,0,0,0,0,'Here\'s my patch for tooling for javac warnings. I don\'t think more tooling is needed on the javac side, no fancy ant macros, no bikesheds, instead just code fixing. Folks can either fix the warnings, or add @SuppressWarnings and fix them later. This way, any new warnings will fail the build if introduced.\r\n\r\n'
1,0,0,0,0,'here\'s a patch: there are two things,\r\n * the test is too slow in general (too many iterations)\r\n * the test is super-slow on windows because of syncd i/o: i wired it to use mmapdirectory.'
0,0,0,0,0,'Updated patch which includes:\r\n - Moved listing of the rules to their own ruleset file, which will allow us to customize them more and so I don\'t have to keep making changes to common-build.xml\r\n - Added full support for solr and top-level.'
0,0,0,1,0,'It\'s still needs a more complete ruleset to use, but is a reasonable starting point after 2 years of inactivity.'
0,0,0,0,0,'I took the latest patch available and brought it up to current trunk. '
0,1,0,0,0,'The patch places all the HTML files under lucene/build/pmd. '
0,1,0,0,0,'Still to do:\r\n * Figure out how to link from the reports to source code'
1,0,0,0,0,'This patch removes the specialized TermConjunctionScorer, making the TermConjunctionScorer algorithm ConjunctionScorer. I added a method to Scorer so that any scorer can return an estimated cost of how many postings it will read:\r\n * term: docFreq()\r\n * disjunction: sum(cost)\r\n * conjunction/phrase: min(cost) * numSubs\r\n'
0,0,0,0,1,'Especially on rules that we can trim for now to make the list more manageable.\r\n\r\n'
0,1,0,0,0,'The only downside of this patch is that we no longer get any warnings displayed that are currently disabled (rawtypes, unchecked). '
0,0,0,0,0,'DecisionBut unfortunately I think its a good deal of work and not easy to do immediately.\r\n\r\nAnyway I think these are'
0,1,0,0,0,'I don\'t think the name should be tied to the implementation.'
0,0,0,1,0,'Another attempt at the patch. '
0,0,0,0,0,'rs)? That makes things a lot cleaner. We\'d need a way'
0,0,0,1,0,'Note that this would also make fieldcache \"type insanity\" impossible. DecisionDecisionIt also allows a possibility for someone to easily control which'
0,0,0,1,0,'Maybe we can tackle this one module (or sets of modules) at a time, failing the build for that module once we\'ve gotten it clean. '
0,0,0,0,0,'.\r\n\r\nAnyway I think these are the three trickiest parts:\r\n # How can we make the FilterReader\'s fieldinfos consistent with the docvalues types? I think it needs to take this information up'
0,0,0,0,0,' field\"). In this case today lucene happily allows it, but with a typed-no-insanity-filterreader i think we should throw an exception in this case instead. It means someone specified the incorrect docvalues type for the field (sho'
0,0,0,1,0,'An alternative approach (and what I was doing before) is to have a pmd dir under each module\'s build dir.'
0,1,0,0,0,'\r\nI think so.'
0,0,0,0,0,'ProBut thinking about it, can we just put a map of fieldnames->XXXDocValues in a threadlocal on UninvertingFilterReader ? That '
0,0,0,0,0,'DecisionIt also allows a possibility for someone to easily control which fields are allowed to have fieldcaches built for them.\r\n # How can we prevent non-dense ordinals (e.g. the case where someone \"sorts on a multivalu'
0,1,0,0,0,'ase. Should simplify things like SortField a lot though.Pro'
0,0,0,0,0,' OOMs.\r\n\r\nSince this doesn\'t have any impact on the build unless someone explicitly runs it, I\'d like to commit this and then begin to tweak.Decision'
0,0,0,1,0,'noformat\r\nIndex: lucene/common-build.xml\r\n===================================================================\r\n--- lucene/common-build.xml\t\r\n+++ lucene/common-build.xml\t\r\n@@ -164,7 +164,8 @@\r\n   <property name=\"javac.debug\" value=\"on\"/>\r\n   <property name=\"javac.source\" value=\"1.8\"/>\r\n   <property name=\"javac.target\" value=\"1.8\"/>\r\n-  <property name=\"javac.args\" value=\"-Xlint -Xlint:-deprecation -Xlint:-serial -Xlint:-options\"/>\r\n+  <!-- all warnings, except deprecation. -->\r\n+  <property name=\"javac.args\" value=\"-Werror -Xlint:auxiliaryclass -Xlint:cast -Xlint:classfile -Xlint:-deprecation -Xlint:dep-ann -Xlint:divzero -Xlint:empty -Xlint:fallthrough -Xlint:finally -Xlint:options -Xlint:overloads -Xlint:overrides -Xlint:path -Xlint:processing -Xlint:rawtypes -Xlint:static -Xlint:try -Xlint:unchecked -Xlint:varargs\"/>\r\n   <property name=\"javadoc.link\" value=\"http://download.oracle.com/javase/8/docs/api/\"/>\r\n   <property name=\"javadoc.link.junit\" value=\"http://junit.sourceforge.net/javadoc/\"/>\r\n   <property name=\"javadoc.packagelist.dir\" location=\"$common.dir/toolssnoformat'
0,0,0,0,0,'uote\r\nHaving just spent an hour or so trying to cut things over, yes, that\'s probably a good idea  !https://issues.apache.org'
0,0,0,0,0,'DecisionBut I assume the plan is to remove oal.search.FieldCache entirely right ?\r\n'
0,0,0,0,0,'We\'d need a way to purge the map, though.\r\n'
0,0,0,0,0,+1
0,0,0,0,0,'ight=16!  It touches a lot of the codebase. Should sim'
0,0,0,0,0,'I\'ve been thinking about this in a lot of detail over the last few months, so I have a few more ideas :\r\n\r\nCurrently FC \"uses\" the docvalues apis, but violates them in a couple of ways. '
0,0,0,0,0,'I\'d like to do that with the spatial module – I just tried and observed a bunch of compile warnings and I\'d like to fix them, and just as importantly have the build fail if a new compile warning should show up in the future.'
0,0,0,0,0,'uote\r\nI think so ... this would happen \"for free\" once we cutover direct FieldCache consumers to DocValues.\r\nquotethis still uses the global FieldCache. Should the caches b'
0,0,0,0,0,'If we can do this, its nice as someone could call IndexWriter.addIndexes and \"upgrade\" from fieldcache to docvalues. '
0,0,0,0,1,'It would be simpler to having all the linting type functionality within a single task. '
0,0,0,0,0,'es types? I think it needs to take this information up-front: a mapping of field names from the underlying fieldinfos to docvalues types. Note'
0,0,0,0,0,'?\r\nquote\r\nI think they should all cutover to DV? Maybe we should make a branch '
0,0,0,0,0,'I was trying to think of ways we could do this long term that would give us a filterreader that would also pass checkindex. '
0,0,0,0,0,'+1, I think this  is a great idea, and the patch looks like a great start! '
0,0,0,0,0,'\r\nI wasn\'t going to initially - just make it package private. '
0,0,0,0,0,'.\r\nquote\r\nI think aol.index is the right place.\r\nquotethere are a b'
0,0,0,1,0,'I fixed this in this new patch by recording whether the\r\nSegmentReader is NRT, and then fixing the reopen logic to load\r\nliveDocs from disk in that case. '
0,0,0,0,1,'I skimmed the patch and can see that you removed the specializations of the dist-maven task in both modules/benchmark/ and lucene/contrib/demo/. For benchmark, the specialization was trivial and didn\'t change any behavior, so I assume that\'s why you dropped it. In the demo case, the specialization was introduced to be able to deploy the .war file, but since the .war file is no longer produced, the dist-maven specialization is no longer required. Later today I\'ll have time to apply the patch and do sanity checking on the maven outputs.'
0,1,0,0,0,'Then having a single folder for all the results which is outside of the module build dirs is probably safest. '
0,0,0,1,0,'It generates a report (HTML) file in build/findbugs/lucene.html.\r\n'
0,0,0,1,0,'Bulk close resolved issues after 6.2.0 release.'
1,0,0,0,0,'Maybe we should mark command line tools as special classes in some way in order to give them more freedom?'
0,0,0,0,0,'\r\nI think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way. there is also nothing to prevent someone from calling IW.addIndexes with it and making a truly corrupt index.\r\n\r\n'
0,0,0,0,1,'\r\nGood point. '
1,0,0,0,0,'It\'s weird, the variance on this test is indeed very high. I think it may have something to do with the fact that it spins many threads (that do i/o) so if you\'re running on a multicore and there are other parallel jvms running tests you\'re putting a load on the hardware. If ran in isolation things get much faster (for me).\r\n\r\nI\'ve replaced some of the random() calls with the non-asserting random; I see some difference but not that much.'
0,0,0,0,0,'ProUnless you think that we\'ll end up having to make major changes if we don\'t build this in from the beginning. I\'m new to a lot of this part of th'
0,0,0,0,0,'I\'d like to see the patches be focused on solving the specific issues and then we can open up a new issue for refactoring this to make for pluggable best fit, etc.'
0,0,0,0,0,'As you can see in the above table \'TileLength\' for Tier 9 is 48,63671875 not 24,31835938 and then the \'new bestFit number of Box to fetch\' becomes ... 4 ! '
0,1,0,0,0,'Resolving again as this issue will not be backported to 2.9/3.0 branches.'
0,0,1,0,0,'One possible downside to this change is that it changes a predictable branch (that is handled at the CPU level) into a method call... which if it\'s not monomorphic can be un-inlined at the point of the call and thus end up slower (method call vs predictable branch). '
0,0,0,1,0,'It means someone specified the incorrect docvalues type for the field (should have been SORTED_SET). DecisionDecisionAlso in the filterreader\'s ctor, we can try to use underlying statistics on the field to detect if any f'
0,0,0,1,0,'I\'m willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.'
0,1,0,0,0,'Yonik,\r\n\r\nIt is the case, but the points left out are for sure not in the search area.\r\n\r\nProDecisionGrant,\r\n\r\nYou were right ! DecisionProIt was a c&p error ! ProAs you can see in the above table \'TileLength\' for Tier 9 is 48,63671875 not 24,31835938 and then the \'new bestFit number of Box to fetch\' becomes ... 4'
0,0,0,0,0,'Also a basic test.\r\n\r\n'
0,0,0,1,0,'I don\'t think it needs to be inside Sim anymore, as I don\'t see any usefulness to making your own\r\nsubclass (how will this affect scoring versus just using the DocValueNorm impl)\r\n\r\nIs there any other use case to Norm being abstract? '
1,0,0,0,0,'\r\nAs as result attached is a static analysis version that will hate any GETSTATIC java/lang/System::(err|out) (I briefly looked at findbugs and did not find a working version for this idea)\r\n\r\nThis one uses ASM 3.3'
0,0,0,0,0,'DecisionAlso in the filterreader\'s ctor, we can try to use underlying statistics on the field to detect if any fields are actually multivalued up front and throw exception early.\r\n # How can we expose \"missing\" for NumericDocValues. One idea is just to see this \"bitset\" as another NumericDocValues field (that '
0,0,0,1,0,'The IDE setups do the same thing.'
0,0,0,1,0,'NumericDocValues doesn\'t work with 32-bit values yet (the test for IntField fails); I need to somehow detect at uninversion-time how wide the indexed values are.'
0,0,0,0,0,'Also, please edit your table to reflect the error'
0,0,0,0,0,'Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue? '
1,0,0,0,0,'Perhaps we could have some sort of filtering accuracy parameter that could give more precise control over the trade-off?'
0,0,0,1,0,'updated patch: I\'d rather just use newDirectory actually.\r\n\r\nThis means the test will be occasionally slow on windows, but I think thats ok. I dont want to start the path of losing test coverage because of certain os brokenness.'
0,0,0,1,0,'\r\nOh it results in long slivery KD cells, which means queries have to visit too many points.'
0,0,0,0,0,'insanity. Otherwise you\'re restricted to just the fields in the map, but you know you\'re not going to uninvert the wrong type. Applications like Solr '
0,0,0,0,1,'And eventually we tie it into precommit to fail, if we ever get to the magical zero warning threshold.'
0,1,0,0,0,'Attached proof-of-concept patch implements ant findbugs on Lucene (Solr not included); fails the build if any warnings are found.\r\n'
0,0,0,0,0,'I don\'t think we should do this. it also prevents it from working with anything that checks fieldinfos.\r\n'
0,0,0,0,0,'ing. I\'m new to a lot of this part of the codebase, so all advice is very welcome here  !https://issues.apache.org/jira/imag'
0,0,0,0,0,'New patch, with a NumericDocValues uninverter. '
0,0,0,0,0,'ProUsers of UninvertedFilterReader have to specify up-front the width of numeric fields they wish to uninvert.\r\n\r\nThis is hacky f'
0,0,0,0,0,'\r\nI was wondering about how to do this. '
0,0,0,0,0,'ong type. Applications like Solr or ES can manage the types outside of UFR using their own field type information.\r\nquoteHow can we expos'
0,0,0,0,0,'AlternativeWe could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it\'s absent, then you can uninvert any field you like, at the risk of fieldcache-insanit'
0,0,0,0,0,'I don\'t really like it, but it seems to work, so it\'s a start. '
0,0,0,0,0,'\r\nI was going to move the FieldCache#getDocsWithField method to AtomicReader, but I see that this doesn\'t actually work in the same way with DocValues at the moment.\r\n\r\n'
0,0,0,0,0,'ocValues. One idea is just to see this \"bitset\" as another NumericDocValues field  and provide sugar in the API that makes this happen automatically. Decision'
0,0,0,1,0,'I added a SegmentListener class which is set on IWC. '
0,0,0,0,0,'row exception early.\r\n # How can we expose \"missing\" for NumericDocValues. One idea is just to see this \"bitset\" as another NumericDocValues field  and provide sugar in the API that makes this happ'
1,0,0,0,0,'https://issues.apache.org/jira/images/icons/emoticons/smile.png|width=16,height=16!'
0,0,0,0,0,'New patch, copying the uninvert logic from FieldCacheImpl into a new Uninverter class , adding a cache to the UninvertingFilterReader itself and checking uninversions against a map of fieldnames to DocValuesTypes passed in as a constructor argument.'
0,0,0,0,0,'Something that will uninvert a LongField or DoubleField into a NumericDocValues representation.'
0,0,0,0,0,'OK, here\'s a patch that attempts to solve the problem of 32-bit vs 64-bit numeric values. '
0,1,0,0,0,'I still need to write unit tests and add an event for aborted merges.\r\n\r\n'
1,0,0,0,0,'Perhaps we want to enable a collection of segment listeners instead of only one?'
0,1,0,0,0,'A CompositeSegmentListener niftily removes the need for collection.'
0,0,0,0,0,'Patch, starting from Vitaly\'s test case  '
0,0,0,0,0,'Perhaps I\'m misreading the table? '
0,0,0,1,0,'I.e., most areas will still have many more small population towns than big ones.\r\n\r\n\r\n'
0,0,0,1,0,'I\'ll close the issue, but we should improve it more later. '
0,0,0,0,0,'Is this not the case?'
0,0,0,1,0,'Really each of these tests should have unit tests (using a MDW that failOn\'s openInput/createOutput/whereever). '
0,0,0,0,0,'\r\nI don\'t think so, because this is to do with reading the already-indexed bytes and converting them back to longs or ints. '
0,0,0,1,0,Done
0,1,0,0,0,'Reverted the last patch and the other related ones. '
1,0,0,0,0,'the issue is the document boost of zero.\r\n'
0,1,0,0,0,'Robert,\r\n\r\nHere is the patch updated to work on rev 1035291\r\n\r\nValidating Unit Tests are included'
1,0,0,0,0,'I verified it finds things like file leaks by introducing them to old codecs. But it currently sometimes trips assertions during IW.commit(). I\'m not sure everything is ok here. I\'m not trying to test IW so I\'m gonna neuter it and keep working.'
0,0,0,1,0,'Committed:\r\n - r1210020: trunk\r\n - r1210022: branch_3x'
0,1,0,0,0,'that way you only take up space relevant to the bits you are actually using...'
0,0,0,0,1,'Separately, we should decide what to do about norm values of zero. In my opinion, norm values of zero should not necessarily decode to a floating point value of zero'
0,0,0,1,0,'Bulk close for 3.1'
0,0,0,0,0,'I think it would even better. '
1,0,0,0,0,'findbugs? checkstyle?'
0,0,0,0,0,'But for now this is an improvement over the IW tests and fills missing coverage for backwards-codecs/'
0,0,0,1,0,', though. We could assume 64-bit values and fall back to 32-bit if we encounter an exception, but I worry that we\'re getting a performance hit here for every 32-bit field.Decision'
0,0,0,1,0,'Right, I removed both because I think  !https://issues.apache.org/jira/images/icons/emoticons/help_16.png|width=16,height=16! we don\'t need maven details in the build.xml (since the Maven POMs are now separately configured).. well, and because we don\'t make a WAR anymore.'
0,0,0,0,0,'I felt like for scoring purposes this is more useful than the number of documents, but thats just my opinion.'
0,0,0,1,0,'You should supply an error message, the one was copied from the forbids before:\r\n\"[Use NIO.2 instead]\" (thats simply wrong). The eror message can be appended with \"@\" after the signature.'
0,1,0,0,0,'A SegmentListener that has a number of children SLs and delegates eventHappened() calls to them.'
0,1,0,0,0,'I think start/endTime can be long and not Long?\r\n\r\n'
0,0,0,0,0,'Alan i didnt look closely, but could GrowableWriter be used to avoid the hack? '
0,0,0,0,0,'Here\'s a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.\r\n\r\n'
0,0,0,0,0,'nfo ones. There doesn\'t seem to be a good way of detecting this at uninvert-time from the FieldInfo data, though. We could assume 64-bit va'
0,0,0,0,0,'Actually I\'ve convinced myself that checking for a NumberFormatException is a better way of doing this...'
0,0,0,0,0,'nvert.\r\n\r\nThis is hacky for any number of reasons, not the least of which is I have to define a whole new set of DocValuesTypes on Uninverter, which is a lot less elegant than just using the existing FieldInfo ones. There doesn\'t seem to be '
0,0,0,1,0,'Maybe instead of adding init/start/endTime to OneMerge, you can pass a \'time\' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The \'time\' parameter can then be interpreted according to Type.\r\n'
0,0,0,1,0,'Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?\r\n\r\n'
0,0,1,0,0,'About this:\r\n'
1,0,0,0,0,'Updated patch. It now specializes both reading doc ids into an array and feeding a visitor, which seems to help get the performance back to what it is on master, or at least less than 1\% slower (not easy to distinguish minor slowdowns to noise at this stage).'
0,0,0,0,0,' DV? Maybe we should make a branch for this ... seems biggish  !Prohttps://issues.apache.org/jira/images/icons/emoticons/s'
0,0,0,0,0,'I will come up with a new prototype impl for [-LUCENE-4571-|https://issues.apache.org/jira/browse/LUCENE-4571] soon which will include and build upon this patch.'
0,0,0,1,0,'False alarm (fake IOE -> slowFileExists == false -> scary assertion). I disable it during commit (with a comment) as a workaround. But its still on during reopen which does not have this check. I think its good enough.'
1,0,0,0,0,'I\'ve updated the patch.'
1,0,0,0,0,'I added a new AdaptiveRateLimitedDirectoryWrapper: it watches the\r\naverage bytes/sec written by non-merges, and then (based on a\r\nmultiplier) sets the merge throttling accordingly. It uses a rolling\r\ntimestamps window of the last 1 GB of writes with 1 MB resolution, and\r\nlets you set min/max on the merge throttle.'
0,0,0,0,0,'Closed after release.'
1,0,0,0,0,'I added two more timings to the patch.'
0,1,0,0,0,'\r\nHowever no javadoc or source jars are downloaded with the patch (previously they were for jcl-over-slf4j).\r\n\r\n'
1,0,0,0,0,'\r\nWith the patch I still see \"CLIENT ERROR\" messages in the verbose output, e.g. from the Lucene test-framework module (scroll down to see the non-POM ant-parent artifacts Ivy can\'t find:\r\n'
0,0,0,1,0,'Manually correcting fixVersion per Step #S5'
1,0,0,0,0,'I think it\'s fine if tests write to the std streams, but not core Lucene code (lucene/core/src/java/*)?'
1,0,0,0,0,'Fix. Will commit today or tomorrow. Going to double check logic again.'
0,0,0,1,0,'\r\nThanks Vitaly, that\'s a good catch (opening to an older commit point\r\nfrom an NRT reader that\'s carrying deletes in RAM), and we don\'t\r\nproperly handle that case because SegmentReader doesn\'t know whether\r\nits liveDocs came from disk (matching the delGen from the\r\nSegmentCommitInfo) or were carried in RAM from IndexWriter.\r\n\r\n'
0,0,1,0,0,'I looked at JAVA_HOME on Windows for 1.5.0_22 and 1.6.0_23 (both 64 bit JDKs), and neither included Javadocs. Maybe they\'re separately downloadable?'
1,0,0,0,0,'here\'s a patch along the lines i suggested.'
0,1,0,0,0,'+1\r\n\r\nThis patch looks great!\r\n\r\nIt cleans up BS2 and specialized term conjunction scorer, and makes more accurate decisions about which sub-scorer to enumerate first (no more first docID heuristic).'
0,0,0,0,0,'If ReqExclScorer has a null \'req\' part, then something in the logic of BooleanWeight.'
0,0,0,0,0,'So advance, docId, fixedBitset and so on are totally unrelated to that.'
1,0,0,0,0,'Patch fixing the javax-activation:activation javadoc download problem, and also standardizing on the use of \"compile\" and \"test\" Ivy configurations in ivy.xmlfiles, mapped to remote \"master\" configurations, so that this problem won\'t recur. Also regularized indenting in all ivy.xml files to be two spaces per indent level. '
0,0,0,0,0,'Bulk close for 4.10.4 release'
0,0,0,1,0,'I don\'t like that cutting over to DTW would open up the thread hazard that we fail to catch the replace'
1,0,0,0,0,'I took a guess that one way would be to replace the System IO streams with ones that will throw exceptions.\r\n\r\nIt requires cglib, and might (due to the abuse of sun.reflect.Reflection to get the callee efficiently) be sun hotspot specific.'
0,0,0,0,0,'Uwe the current discussion is about not measuring count of documents, but instead i/o operations.\r\n\r\n'
0,0,0,0,0,'\r\nMan, I just wanted to make clear that the current patch and the current issue summary have thi'
0,0,0,1,0,'\r\nSegmentInfo caches sizeInBytes, so I think the \'nocommit\' can go away.\r\n\r\n'
0,0,0,1,0,'However, I did notice that SegmentInfo\'s cache is potentially buggy – it doesn\'t take into account \'includeDocStores\'. I.e., if you call it once w/ \'false\' and then with \'true\' (or vice versa), you won\'t get the right sizeInBytes. I\'ll open a separate issue to fix this.'
0,0,0,0,0,'Mike, if you have time, can you do calculations and adjust the patch? '
0,1,0,0,0,'\r\nSo I think its tested (there are several tests cranking fields like TestManyFields). But if we can do it better, I am all for that.'
0,0,0,0,0,'Commit 1687789 from [~mikemccand] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1687789] ]\r\n\r\n[-LUCENE-6325-|https://issues.apache.org/jira/browse/LUCENE-6325]: use array for number -> FieldInfo lookup, except in very sparse cases'
1,0,0,0,0,'I have been experimenting with the attached patch, which compresses doc ids based on the number of required bytes to store them (it only specializes 8, 16, 24 and 32 bits per doc id) and also adds delta-compression for blocks whose values are all the same. '
0,0,0,0,1,'Can we put back the non-wildcard imports?'
0,1,0,0,0,'+1, this looks like an easy, possibly high-impact win.\r\n\r\n'
1,0,0,0,0,'OK new patch with a completely different approach, moving the\r\ntracking under Directory.\r\n'
0,0,0,0,0,'Commit 1687792 from [~mikemccand] in branch \'dev/branches/branch_5x\'\r\n[ [https://svn.apache.org/r1687792] ]\r\n\r\n[-LUCENE-6325-|https://issues.apache.org/jira/browse/LUCENE-6325]: use array for number -> FieldInfo lookup, except in very sparse cases'
1,0,0,0,0,'New patch, using dense array when > 1/16th of the numbers are used:\r\n\r\nEach TreeMap$Entry has object header (8 or 16 bytes), 5 pointers (4 or\r\n8 bytes), and a boolean (likely rounded up to 4 bytes), times 2 for\r\nall the inner nodes of the tree, plus the overhead of Integer (object\r\nheader, int), so net/net each entry in the TreeMap costs 68 - 124 bytes.\r\n\r\n'
0,0,0,0,0,'I think we can be more aggressive about using the array: TreeMap has much more than 50\% overhead, since it also needs Integer key and pointer to that key, and object overhead holding that key pointer and value pointer. '
1,0,0,0,0,'Simple patch + test.'
0,0,0,1,0,'I\'ll fix ...'
0,0,0,1,0,'I also cleaned up all remnants/conditionals about not supporting offsets.'
0,0,0,0,0,'I think we can safely do this opto when it\'s > 10\% of '
0,1,0,0,0,'I think having the task name tied to the implementation is fine if we plan on adding other static analysis tools later.\r\n\r\nSo I imagi'
0,0,0,0,1,'them both together. IssueAnd eventually we tie it into precommit to fail, if we ever get to the magical zero warni'
0,0,0,0,0,'I don\'t want to think about overflowing.'
0,0,0,0,0,'OK I can try to update this & commit...'
0,1,0,0,0,'The array is 4 or 8 bytes per int.'
0,0,0,0,0,'Maybe we can add some fieldinfos tests to exercise this directly, and maybe there is a way to simplify the scary logic too.'
0,0,0,1,0,'\r\nOh, good catch! '
0,0,0,1,0,'Bulk close for 5.3.0 release'
1,0,0,0,0,'This is possible by verifying where System.out takes place at runtime via stack analysis. '
0,0,0,0,1,'Thanks, can you change 16 to 16L? '
0,0,0,0,0,'Cache. Should the caches be moved to the readers instead?Issue'
1,0,0,0,0,'btw, as far as whether the \"sparse\" case is tested, i added \"assert false\" to this and ran core tests (no @Slow, no @Nightly):\r\n'
0,1,0,0,0,'+1 to commit with that modification, thanks for doing the calculations here.'
1,0,0,0,0,'Here\'s a patch that removes the custom collector and the \"-paging false/streaming\" option, simplifying the demo a lot more.'
1,0,0,0,0,'Here was my first patch (moved from [-LUCENE-6320-|https://issues.apache.org/jira/browse/LUCENE-6320]).\r\n\r\n'
0,0,0,1,0,' if estimateDocCount() returns long, also advance() must take and return long, docId() must return long, FixedBitSet must take long as size and finally numDocs and maxDoc must be long.'
1,0,0,0,0,'This patch adds the following (and includes all of Mike\'s patch):\r\n * IntelliJ IDEA configuration adjustments\r\n * Maven POM adjustments\r\n * Source and regenerated site docs\r\n * contrib/demo/build.xml builds the Lucene core jar in the init target if it doesn\'t exist or if its .java source files are newer than the jar.\r\n * contrib/demo/build.xml default target is now jar-core\r\n * At Mike\'s suggestion on #lucene IRC, added javadoc comment to IndexFiles pointing out that 1 input doc per file is slow, and that good throughput can be gained by putting multiple docs in a single input file.\r\n * Other minor cleanups\r\n'
1,0,0,0,0,'Also, I removed RateLimitedDirectoryWrapper: I think it\'s dangerous\r\nhow it encourages you to throttle anything except merges, and\r\nencourages you to just set a fixed rate (one size does NOT fit\r\nall...). If you want \"similar\" behavior you can use\r\nAdaptiveRateLimitedDirectoryWrapper but set min and max to the same\r\nvalue.'
0,0,0,1,0,'Bulk close after 5.0 release.'
0,0,0,1,0,'I also include the sub-file name when inside a sliced II (CFS/CFX);\r\nI added a required arg (sliceDescription) to the sliceInput method\r\nfor this.'
1,0,0,0,0,'updated patch with a code comment'
1,0,0,0,0,'This is fixed in the pendig re work done by Chris Male\r\n\r\nShould be comitted in a whole soon'
0,0,0,1,0,'Reopening to backport to 6.0.2, 5.6 and 5.5.2'
0,0,0,1,0,'\r\nLong\'s used because it allows null.\r\n'
1,0,0,0,0,'I think the patch is ready...'
0,0,0,1,0,'\r\nI think it\'s generally useful to keep track of the time(s) in the OneMerge object. '
0,1,0,0,0,'It\'s implemented this way so that the, likely a user interface does not need to store the various times itself. '
1,0,0,0,0,'Actually, logging applications also need to print the durations.\r\n'
0,0,0,0,0,'\r\n...of the top of my head, i wasn\'t sure if an annotation would be as easy to use \r\n'
1,0,0,0,0,'\r\nNo reason ... it was just the first thing i thought of that seemed really generic. Anyplace we might normally write...\r\n'
0,0,0,0,0,'It _could_ be a rule that would intercept failures, check for groups annotations and then rethrow... but I honestly don\'t think many people would find it useful (or understand the principle under which it oper'
0,0,0,0,0,'\r\nMy suggestion was to try and keep it _reaallly_ generic ... so that with one \"option\" active you say \"i\'m a developer who is asking for trouble, try everything even if you think it\'s not valid on my system. '
0,0,0,1,0,'Closed after release.'
0,0,0,1,0,'Hi Uwe: you are right! obviously this one needs a comment, but this is the idea:\r\n\r\nnorm[0]=4.656613E-10\r\nnorm[1]=5.820766E-10\r\nnorm[2]=6.9849193E-10\r\nnorm[3]=8.1490725E-10'
0,0,0,1,0,'A faulty count accumulation can happen in this circumstance:\r\n * indexed data includes non-point shapes\r\n * heatmap region crosses the dateline\r\n * one of the indexed shapes is a good deal larger than the heatmap region such that it has leaf cells larger than the target heatmap cell size (so-called \"ancestors\" in the code).\r\n\r\n'
0,0,0,0,0,'Perhaps the ideal case would be specific annotations like you describe, which could be used as test groups for people who want to go out of their way to test specific suspicious stuff  '
0,0,0,0,0,'Making people know that their particular os/jvm/filesystem/jvm-opt-combos are suspicious therefore they need to explicitly ask for test group X and test group Y test group Z seems like it would make it overly hard for people to try everything.\r\n\r\n'
0,0,0,0,0,'quotebut if you set \"-Dtest.suspicious.shit=true\" then instead the runner will run those tests anyway,\r\nquote\r\nThis you can do already; test groups can be turned on and off by overriding their assigned system property, no problem with that.\r\nquotebut wrap any failures/exceptions it gets in another \"FailureUnderSuspiciousCircumstances\" exception whose getMessage would contain info about what assumption would have normally prevented that test from if you hadn\'t gone out of your way to run it.\r\nquote\r\nEven this description makes me feel dizzy... :) I get your idea but I don\'t know how to implement it in a sensible way. It _could_ be a rule that would intercept failures, check for groups annotations and then rethrow... but I honestly don\'t think many people would find it useful .\r\n\r\nDawid'
0,0,0,0,0,'\r\nEven this description makes me feel dizzy... :) I get your idea but I don\'t know how to implement it in a sensible way. '
0,0,0,0,0,'view=revision&revision=1421819]\r\n\r\nMerged revision 1421818 from lucene/dev/trunk:\r\nRevert the revert of the revert. '
1,0,0,0,0,'This is a real bug; the patch I\'m attaching fixes it and adds a test for the input that reproduced this circumstance.\r\n\r\n'
0,0,0,0,0,'\r\nThis you can do already; test groups can be turned on and off by overriding their assigned system property, no problem with that.\r\n'
1,0,0,0,0,'Here is a patch to import the CartesianPolyFilterBuilder:getBoxShape part of the code from the worked we have done with Chris @ GoogleCode\r\nI tried to keep it as minimal as possible\r\n\r\nNext step is to patch LatLngRectangle in the same way'
1,0,0,0,0,'I am fine with such a system property and also some code to warn user of several incompatible combinations, but I want to be able to run tests to find the problems behind the issue.\r\n\r\n'
0,1,0,0,0,+1
1,0,0,0,0,'code:java\r\nboolean someBoolean = ...something interesting...;\r\nif (someBoolean) \r\n  throw new AssumptionViolatedException(\"...why your system is suspicious...\")\r\n\r\ncode'
0,0,0,0,0,'but then have a feature in the runner that by default those groups are skipped with a clear AssumptionViolatedException but if you set \"-Dtest.suspicious.shit=true\" then instead the runner will run those tests anyway, but wrap any failures/exceptions it gets in another \"FailureUnderSuspiciousCircumstances\" exception whose getMessage would contain info about what assumption would have normally prevented that test from if you hadn\'t gone out of your way to run it.\r\n\r\n'
0,0,1,0,0,'\r\nThe only problem I see is that these need to be provided statically – if you need to detect them at runtime then I\'d either need to change the code of the runner or we\'d need to switch to assumptions inside a rule, for example.'
0,0,0,0,0,'And I like that we now take raw into account when figuring out which comparison to make to accept the term or not. '
0,0,0,0,1,'It might feel like a corner case when dimensions represent similar data like lontitudes and latitudes, but what happens eg. if you want to index all towns in the world alongside their population as a 3rd dimension. '
0,0,0,0,0,'Maybe we could improve it a bit: if raw is true we don\'t need to calcSimilarity right?\r\n\r\n'
0,1,0,0,0,'\r\nThis is a great idea – i\'ve spun off [LUCENE-4631|https://issues.apache.org/jira/browse/LUCENE-4631] to track that since it\'s kind of broader then if/how to allow people to run tests even under suspicious circumstances (but whatever we add there can probably be leveraged here)\r\n'
0,0,0,1,0,'I removed all the crazy 0 value writing in NormsConsumer and still all tests pass. '
1,0,0,0,0,'This patch:\r\n * fixes demo2.html to remove description of the streaming search removed from the SearchFiles demo app by Robert\'s patch\r\n * removes the obsolete \"<root directory>\" text from the usage message in the IndexFiles demo app, as well as the same info in demo.html\r\n * Stops the SearchFiles demo app in interactive mode from asking for which page to navigate to when there are zero results - instead asks for another query.'
0,0,0,0,1,'Did you comment out the \"@Repeat(iterations = 20)\" on purpose?'
1,0,0,0,0,'code:java\r\nboolean someBoolean = ...something interesting...;\r\nif (someBoolean || Boolean.getBoolean(\"tests.suspicious.shit\")) \r\n  throw new AssumptionViolatedException(\"...why your system is suspicious...\")\r\n\r\ncode'
1,0,0,0,0,'\r\n...could be replaced with...\r\n'
0,0,0,1,0,'In cases where the indices are very large, it wouldn\'t make sense to expose distance > 2 capability; but on small to medium indices, there are use cases that require it.'
0,0,0,1,0,'\r\nYes, that\'d be good to fix.'
1,0,0,0,0,'The test group annotations can be enabled/disabled via system properties and they also do display messages on assumption-ignored tests – wouldn\'t this be enough to cover your use case?\r\n'
0,0,0,0,0,'I think this is ready to commit.'
0,0,0,0,0,'So, in SlowFuzzyTermsEnum, similarity  was typically > minSimilarity no matter its value. '
0,0,0,0,0,'I\'ll commit t'
0,0,0,0,0,'First draft of patch attached. '
0,0,0,0,0,'Updated patch. '
0,0,0,1,0,'We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that  !'
0,0,0,1,0,'\r\nI\'m not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it\'d be nice if we can add more to it. '
0,0,0,0,0,'Also, can you put up a patch that doesn\'t have all the formatting changes and just has the bug fixes?'
0,0,0,0,1,'Mike/Robert,\r\n\r\nI have a follow-up question. I have backported the fix to 4.6 and now I believe I am seeing another serious issue here.  !'
0,0,0,0,0,'117784]). The classic QueryParser silently converts distances > 2 to 2.\r\n\r\nIf I understand Slow'
0,0,0,1,0,'If this is too much to handle at once, and we want to do it per-module, then put a javac.args=\"-Xlint:all\" before common-build.xml is imported, in each module that fails.\r\n'
0,0,0,0,0,'Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!'
0,0,1,0,0,'I ran some tests with this approach and I think it\'s no good.\r\n\r\nThis creates a tricky feedback system, where both CMS (via hard stalling of incoming threads) and this directory attempt to make change to let merges catch up. When CMS\'s hard stalls kick in, this lowers the indexing byte/sec rate, which causes this directory to (over simplistically) lower the merge IO throttling, which causes the merges to take longer.'
0,0,0,0,1,' do we even have a way to write a test for this sort of thing? '
1,0,0,0,0,'The attached patch implements the Contains algorithm for RecursivePrefixTreeStrategy; includes tests.\r\n\r\nUnlike Intersects & Within, this one doesn\'t share the same tree visiting code, but it is also a recursive trie algorithm. I expect for typical use cases that it\'ll be quite scalable and fast, though it does have to traverse all the way to the bottom detail at least once.\r\n\r\nThe existing test infrastructure made testing this easy. The ~260 lines of often complicated code that implements the algorithm and miraculously it passed right away. I upped the test repetitions to 1000 and set some break points at key points and indeed, it appears to work. '
0,0,0,0,0,'Bulk move 4.4 issues to 4.5 and 5.0'
1,0,0,0,0,'It works something like this:'
0,1,0,0,0,'\r\nI used DocValuesField as a delegate in this patch. '
0,0,0,0,0,'Move issue to Lucene 4.9.'
0,0,0,0,0,'Robert,\r\n\r\nI agree that it appears to, but if you want a distance > 2, the current levenshtein automaton doesn\'t allow that ([http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html?'
0,0,0,0,1,'Nicolas,\r\n\r\nHow does this compare to what I already committed? '
0,0,0,1,0,'No I didn\'t; I already caught that and will commit that unchanged.'
0,1,0,0,0,'Patch looks fine to me at first glance'
0,0,0,1,0,'\r\nAnd close the reader...'
0,1,0,0,0,'One thing less to think about.'
1,0,0,0,0,'here is a patch that adds #estimateDocCount to DISI. It still has some nocommits mainly related to BitSets and carnality but I this its fine as a start. I removed the TermConjunction specialization and changed the heuristic in FilteredQuery to use the estimated cost.'
0,0,0,0,0,'This looks like a great improvement: I like factoring out calcDistance from calcSimilarity.\r\n\r\n'
1,0,0,0,0,'Patch.\r\n\r\n'
0,0,0,0,0,'I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. '
0,0,0,0,0,'The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. '
0,0,0,0,0,'Bulk close resolved 4.4 issues'
0,0,0,0,0,'In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn\'t the determining factor in whether SlowFuzzyTerms accepted a term.'
0,0,0,0,0,'Corrected term.length to text.length in calcSimilarity call. '
0,0,0,0,0,'Added short circuits to avoid calculating similarity when not necessary. '
0,0,0,0,0,' to 2.\r\n\r\nIf I understand SlowFuzzyQuery correctly, it uses the levenshtein automaton for distances <= 2, but it runs brute force if the distance is > 2.\r\n\r\nMy personal preference would be to undep'
0,1,0,0,0,'You can remove the unused BytesRef spare in NormsConsumerPerField...'
1,0,0,0,0,'In my opinion, we should really warn users also on Solr startup, if they have jRockit (this JVM only works with Lucene if you pass -XnoOpt) or J9 (fails with Lucene 4.0+), so they don\'t corrumpt their index. '
0,0,0,0,0,'For my sanity ... where exactly was the bug in the original code?'
0,0,1,0,0,'Something doesn\'t fit right in my mind with the AspectJ approach, I have seen it not work in the past for obscure reasons and it feels that running the weaving in pretend to check the verbose output is not much further on from checking the source code in the first place.'
0,0,1,0,0,'Well would it sway the argument if I said that the ASM code near directly translates into a findbugs rule (I have done this before)'
0,1,0,0,0,'This is *_awesome_*. It won\'t make the tests go any faster, but now I will know whether I can walk away from the running tests to work on something else.'
1,0,0,0,0,'Worked again around the code in CartesianPolyFilterBuilder and finally set up this test (attached).\r\n\r\nHere lies the difference between the two patchs : as I reworked all the logic I did change the code in getShapeLoop that you did not touch.\r\n\r\nMaybe it shall be adressed in another issue.'
0,0,0,1,0,'Grant,\r\n\r\nYou were right ! '
0,0,1,0,0,'it appears that this change has caused a problem when running the license checker against the maven builds?\r\n\r\n'
0,0,0,1,0,'As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. '
0,0,0,0,1,'Robert: In your patch: Why use exactly that float (looks arbitrary) and not e.g. Float.MIN_VALUE (of course not NEGATIVE_INFINITY!)?\r\n'
0,0,0,1,0,'BTW, I think SegmentListener is not the proper name? '
0,1,0,0,0,'I.e., it does not listen on Segments, right? '
0,1,0,0,0,'Maybe a SegmentMergeListener, or simple a MergeListener? '
0,0,0,0,0,'This was introduced with [-LUCENE-5610-|https://issues.apache.org/jira/browse/LUCENE-5610]\r\n\r\nI\'ll fix the nightly Lucene benchmark to plot CheckIndex time ... we could have spotted this performance regression.'
0,1,0,0,0,'I think actually for SortedDocValues we should try to move things to the same thing long-term (instead of returning -1).'
0,1,0,0,0,'Nice idea! At least the FilteredQuery code looks fine to me.'
0,1,0,0,0,'\r\n+1'
0,0,0,0,0,'I think we should also backport to 3.x?\r\n '
0,0,0,0,0,'I also plotted the time to CheckIndex in the nightly benchmark: [https://people.apache.org/~mikemccand/lucenebench/checkIndexTime.html]\r\n\r\nHowever that index doesn\'t have term vectors so this issue shouldn\'t\r\naffect it ...'
0,0,0,1,0,'Bulk close for 5.2.0.'
1,0,0,0,0,'I disabled Terms.getMin/Max checking for TVs, fixed the \"test with the\r\none doc deleted\" to only run on the first doc, and only test 1\r\n\"advance\" doc.\r\n\r\n'
0,0,0,0,0,'new patch - fixed the code dup'
0,0,0,0,0,'\r\n+1\r\n\r\nThis would then make a default base impl work well for all leaf queries.'
0,0,1,0,0,'why the createDirectories call was introduced at that point in the code path in the first place'
1,0,0,0,0,'sorry, here is the correct patch.'
0,0,0,0,0,'Commit 1658832 from [~mikemccand] in branch \'dev/branches/branch_5x\'\r\n[ [https://svn.apache.org/r1658832] ]\r\n\r\n[-LUCENE-6233-|https://issues.apache.org/jira/browse/LUCENE-6233]: speed up CheckIndex when the index has term vectors'
0,0,0,0,0,'Commit 1658831 from [~mikemccand] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1658831] ]\r\n\r\n[-LUCENE-6233-|https://issues.apache.org/jira/browse/LUCENE-6233]: speed up CheckIndex when the index has term vectors'
0,0,0,0,0,'Patch looks great Simon, thanks! '
1,0,0,0,0,'New patch that changes MatchNoDocsQuery to rewrite to an empty BooleanQuery. '
0,0,0,0,0,'+1 to commit.'
1,0,0,0,0,'Patch guards the Files.createDirectories call with a Files.exists check. Files.exists only demands read access, so it succeeds if you only have read access.'
0,0,0,0,0,'Ported back to 3.x\r\n\r\nCommitted revision 991537.'
0,0,0,0,0,'Committed revision 991310\r\n\r\nThanks Mike'
0,0,0,0,1,'I\'m not sure why this is a quantization issue... SmallFloat handles underflow by mapping to the smallest number greater than 0, so the only way to get a 0 field norm is an explicit boost of 0.\r\n'
0,0,0,1,0,'Bulk close after 5.1 release'
0,0,0,1,0,'Patch just synced to trunk. Some of the previous stuff in it has been committed.'
0,1,0,0,0,'\r\nThis output does not appear when running the code above, unchanged. Hope this helps... I can\'t make further headway myself though.'
0,1,0,0,0,'+1 for this!'
0,1,0,0,0,'Michael,\r\n\r\nYour updated patch definitely fixes the issue. '
0,0,0,0,0,'Bulk close for 3.1'
0,0,0,0,1,'Yeah definitely, thats why I included that option.'
0,0,1,0,0,'I had assumed that your new algorithm was often less selective (allowed more points through the filter) than the old. '
1,0,0,0,0,'\r\n\r\nI expected some speedups for ant resolve with these changes, since Ivy should no longer attempt to download extraneous crap, but unfortunately my testing shows the difference to be in the noise.\r\n'
0,0,1,0,0,'hi Yonik,\r\n\r\nI do not aggre : as the 4 tiles requested go cover all the search area there is no gain in beeing less accurate.'
0,1,0,0,0,'We can\'t let such abuse prevent real features, thats just wrong.\r\n\r\n'
0,1,0,0,0,'DocValues is really the Codec side of things and it would be nice if we would not mix that stuff in the similarity API.\r\n'
0,0,0,1,0,'reopening for possible 2.9.4/3.0.3 backport.'
0,0,0,1,0,'I\'ll fix that and commit.'
0,1,0,0,0,'+1, this is a great cleanup: more understandable than what we have today.'
1,0,0,0,0,'\r\nTerm vectors checking is still slowish, but at least it\'s faster: on\r\nmy smallish test index the total CheckIndex time improves from 33.6\r\nseconds to 12.5 seconds.\r\n\r\n'
0,1,0,0,0,'These can be tricky, I agree. Nonetheless, I\'ve been using AspectJ for a long time and it always fits my needs and expectations. I\'m not saying it doesn\'t have any bugs – I\'m sure it has. But the right tool for the right job; it took me about 5 mins to write and apply that aspect (with follow ups, I sent an e-mail to the mailing list, JIRA didn\'t work at the time). '
1,0,0,0,0,'Here\'s the patch. I think its ready.'
0,0,0,0,0,'Thanks Tim, new patch looks great!'
0,1,0,0,0,'\r\nThat\'s a style thing. '
0,1,0,0,0,'I prefer null as it\'s conclusive and doesn\'t lead to incorrect calculations, just NPEs.\r\n'
0,0,0,1,0,'\r\nI had MergeListener, then changed it to be more generic. '
0,0,0,0,1,'I think we should add other segment events such as flush, open, clone, close, etc?'
0,0,0,1,0,'\r\nI committed a fix to trunk and branch_5x.\r\n\r\n'
0,1,0,0,0,'Yonik,\r\n\r\nIt is the case, but the points left out are for sure not in the search area.\r\n\r\n'
0,1,0,0,0,'I\'m ok with both then.'
0,0,0,1,0,'OK, although underflow is generally handled, Robert found at least one case where it doesn\'t work.\r\n'
0,0,0,1,0,'Trunk for patch with the same changes.'
0,0,0,1,0,'The issue was that the Ant task that puts dependencies into the Maven POMs by parsing dependencies from ivy.xml files (the \'mvndeps\' task) excluded dependecies with the Ivy \"start\" configuration by using an XPath selector for \"conf\" attributes that did not begin with the exact string \"start->\", where the \"->\" is how Ivy does configuration mapping.\r\n\r\n'
0,0,0,0,0,'alternativeAnother iter of this patch; I think it\'s ready to commit. I commented out BooleanScorer\'s handling of required=true since we never use it . I also added C'
0,0,0,0,1,'I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses? \r\n'
1,0,0,0,0,' it may be better to do this entirely inside a FilterDirectory.\r\n\r\nE.g. when IndexOutput is closed, and the IOContext is not MERGE, increment the bytes written ... and then that same directory instance could dynamically update the target merge throttling ... maybe.'
1,0,0,0,0,'Patch that adds the MatchNoDocsQuery and uses it for empty SimpleQueryParser queries as well as when a BooleanQuery is rewritten and has no clauses.'
0,1,0,0,0,'I think otherwise the patch is great.\r\n\r\n'
0,1,0,0,0,'Are all the changes in this patch necessary to get the build to pass? '
1,0,0,0,0,'I figured being close to a standard iterator would be more familiar/friendly, although if it worked similar to DocIdSetIterator it would be faster since the caller would check for the sentinal value instead of calling hasNext() (which wouldn\'t exist).'
0,0,0,0,0,'Closed after release.'
0,1,0,0,0,'Oh, btw. I think a FindBugs rule for detecting sysouts/syserrs would be a great addition to FindBugs – you should definitely file it as an improvement there. In reality at least class-level exclusions will be needed to avoid legitimate matches like the ones shown above (main methods, exception handlers), but these can be lived with.'
1,0,0,0,0,'Attached a patch that adds an inner class IntIterator that is consistent with the contract of a java.util.Iterator but next() returns a primitive int. '
0,0,0,1,0,'If at some point we want an iterator, the patch is here and it can be re-considered.'
1,0,0,0,0,'So instead of adding to the API I decided to enhance the documentation to make it more clear how to use this class. '
1,0,0,0,0,'I\'ll added events for flush, open, clone, close and the CompositeSegmentsListener.'
0,0,0,1,0,'I committed better javadocs, including sample code to iterate the values.\r\n\r\n'
0,0,0,1,0,'(attached)\r\n\r\nOf note I added a warning of the potential unsuitability of the lack of hashing of the key.'
0,0,0,1,0,'Here\'s a first cut including workarounds to avoid NPEs and file not found exceptions in SegmentInfo (when calling size in bytes). '
0,0,0,0,0,'Closing issue as won\'t fix . '
0,1,0,0,0,'I need to add one for abort.'
0,0,0,0,0,'s > 2.\r\n\r\nMy personal preference would be to undeprecate SlowFuzzyQuery  because it offers a capability that the current levenshtein automaton doesn\'t. In cases where the indices are very large, it wouldn\'t make '
0,1,0,0,0,'The aborted merge event is now generated and tested for.'
0,0,0,1,0,'I separated out a ReaderListener because it\'s tied to the ReaderPool which eventually will exist external to IW.'
0,0,0,1,0,'The ReaderEvent is never generated? '
0,1,0,0,0,'Is that still work-in-progress?\r\n'
0,1,0,0,0,'When would this be invoked? '
0,0,0,1,0,'Only if IW is pooling readers? '
0,1,0,0,0,'\r\n+1'
0,1,0,0,0,'Patch looks good, builds and passes for me, thanks for fixing this Shai.'
1,0,0,0,0,'Hi, new and improved (\"more correct\") patch: On my windows machine your patch broke because of the backslashes. '
0,0,0,1,0,'My build.xml is the same as upstream, the problem is my checkout path looks like this\r\n\r\n/home/buildserver/workspace/builds/\r\n\r\nsearch-engineering-solr-lucene-\\trunk\r\n\r\nThis means that the prepare-webpages target gets its paths in the buildpaths variable as a pipe separated list like so\r\n\r\n/home/buildserver/workspace/builds/\\search-engineering\r\n\r\n-solr-lucene-\r\n\r\ntrunk/lucene/analysis/common/build.xml|/home/buildserver/workspace/builds/\\search-engineering-solr-lucene-\\trunk\r\n\r\n/lucene/analysis/icu/build.xml|...'
1,0,0,0,0,'This creates a 100\% correct [file://|file:///] URI as XSL expects (XSL wants URIs, but most interpreters like XALAN also allow file names, but using [file:-URIs|file://-uris/] is more correct and straight-forward).\r\n\r\n'
0,0,0,1,0,'The most correct way to do this by creating a \"real\" file based URI. '
0,1,0,0,0,'Committed rev 1339097. '
0,0,0,0,0,'Hi,\r\n\r\nthat makes sense, thanks for reporting this! '
0,0,0,1,0,'Committed to trunk and branch4x.'
0,1,0,0,0,'Activated old tests that test for edit distance matches where the edit distances are greater than the query term length.'
0,0,0,1,0,'I will commit this now, thanks for reporting!'
0,0,1,0,0,'If you changed the Lucene-provided build files, this is not an issue, because those always build out of the box. '
1,0,0,0,0,'Second, it puts all throttling (hard stall of incoming index threads,\r\npause/unpause merges, IO rate limiting) responsibility in CMS, which\r\nmakes sense because it\'s the merge scheduler that \"knows\" whether\r\nmerges are falling behind, that sees how many merges need running,\r\netc. (Plus CMS is already doing throttling...).'
0,0,0,0,1,'If XSL does not like your own and modified build files, they are invalid XML, so fix those. '
0,0,0,1,0,'we really needed to split out fieldinfos implementations for 3.x and 4.x, for example\r\nthis will be useful to support offsets in the postings lists for lucene4xcodec ([-LUCENE-3684-|https://issues.apache.org/jira/browse/LUCENE-3684])\r\n\r\na few minor/trivial comments:\r\n\r\nin PreFlexRW and SimpleText\'s impl:\r\nthrow new IllegalArgumentException(\"Codec only supports single byte norm values. '
0,0,0,1,0,'But since I removed all configuration mappings in \"conf\" attributes under this issue (by moving them to '
1,0,0,0,0,'First, it removes all \"cross cutting\" merge abort checking and instead\r\ndoes it \"down low\" by wrapping all IndexOutputs created for merging:\r\nnice cleanup!\r\n'
0,0,0,1,0,'i added skipBlock() which for these fixed encoders, means they only read their header (typically a single vint) and skip their compressed payload. '
0,0,0,0,0,'I will fix this, thanks for the nice workaround patch.'
1,0,0,0,0,'\r\nFew comments:\r\n * CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).\r\n * New test:\r\n ** LTC.newDirectory() instead of new RAMDirectory().\r\n ** text messages in the asserts.\r\n * DTR:\r\n ** Would it be simpler to make close() synchronized (just like IR.close())\r\n ** Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.\r\n ** Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.'
0,0,1,0,0,'I think its confusing we have MatchAll but not MatchNone.\r\n\r\n'
0,0,0,0,1,'Is there a chance that this can also be applied to 3.0.2 / 3.1? '
0,0,0,1,0,'Hmm I can remove both actually, they do not bring value now that the detection of whether doc ids are sorted is based on the doc ids themselves rather than the fact that there is a single value in a block.  '
0,1,0,0,0,'Then it wouldn\'t need a weight and scorer.'
0,0,0,0,1,'Maybe we should rename to estimateDocsVisited?'
0,0,0,1,0,'(and so on)\r\n\r\nXSLT picks this up later and tries to load these paths, however XSLT assumes that they are URLS which makes the  character invalid and causes\r\n\r\ncom.sun.org.apache.xalan.internal.xsltc.TransletException: javax.xml.transform.TransformerException: com.sun.org.apache.xml.internal.utils.URI$MalformedURIException: Path contains invalid character: \r\n\r\nThis pattern is infrastructural to where I work and is not likely to change (I would like it too)\r\n\r\nNot sure if that makes sense'
0,0,1,0,0,'The attached patch seems to work only around your invalid XML.'
1,0,0,0,0,'Escape URI paths for XSL'
1,0,0,0,0,'Each merge is given its own MergeRateLimiter by IndexWriter, which\r\nhandles 1) checking for abort, 2) pausing/unpausing merges, 3)\r\noptionally \"io nice\" (write MB/sec rate limit) each merge thread.'
0,0,0,1,0,'Commit 1596646 from [~rcmuir] in branch \'dev/branches/branch_4x\''
0,0,0,1,0,'Maybe we\r\nshould hold off on that for a separate issue?\r\n\r\n'
0,1,0,0,0,'+1 to the patch.'
0,0,0,1,0,'Why were the added checks needed in SegmentInfo? '
0,0,0,0,1,'I think I\'d prefer that this SegmentInfo not\r\nbe published until the Type == COMPLETE.\r\n\r\n'
0,1,0,0,0,'How come merge is not also final in MergeEvent?\r\n\r\n'
1,0,0,0,0,'I agree we should change the name. '
0,1,0,0,0,'I don\'t think we need CompositeSegmentListener? '
0,0,0,0,0,'TermQuery and all other places in a followup.'
0,0,1,0,0,'\r\n\r\n\r\n\r\nRight, a filter for \"small population\" is costly, but I don\'t think we can do much about it since it is due to the fact it would match lots of documents. '
0,1,0,0,0,+1
0,0,0,1,0,'On the go, fixed close() to synchronize on this if the instance is not already closed. Otherwise, two threads that call close() concurrently might fail (one of them) in decRef().'
0,0,0,0,0,'Thanks Lee, I like this.\r\n\r\n'
0,1,0,0,0,'+1, looks awesome.'
1,0,0,0,0,'OK I noticed one case where live docs didn\'t confess how long it took  !'
0,0,0,0,0,'The .equals stuff is not required though, it can use the superclass implementation. '
0,0,0,0,0,'Robert: +1, I opened [-LUCENE-6333-|https://issues.apache.org/jira/browse/LUCENE-6333] for this, I\'ll work on a patch.'
0,0,0,1,0,'\r\nThe hashcode is required at least, because otherwise the QueryUtils.check(q) fails because both the MatchNoDocsQuery and the superclass Query have the same hashcode, and the anonymous \"WhackyQuery\" that QueryUtils creates shares the same hash code, so QueryUtils.checkUnequal() fails.\r\n\r\n'
0,0,0,0,0,'If it is controversial then I\'m happy with the previous patch that overrides both equals and hashCode.'
0,0,0,0,0,+1
0,0,0,1,0,'\r\n+1 to rewrite to an empty BooleanQuery'
0,0,0,0,0,'I think the 0x1AA71190 of MatchAllDocsQuery is here to avoid that all Query impls that only wrap a boost end up with the same hash code. '
0,0,0,0,0,'We should commit this one, and remove duplicated logic and hardcoded constants in e.g. '
0,0,0,1,0,'I think it is a better trade-off since it has a better worst case for selective queries. For instance the fact that today the geo dimensions would get 10 splits means that a selective geo query would have to visit 1/1024th of the index, but a selective query on population would have to perform a linear scan. '
1,0,0,0,0,'Also removes the nocommit as per Adrien\'s suggestion'
0,0,0,0,0,'They seem to already have this logic.\r\n\r\n'
0,0,0,1,0,'Bulk close after 5.0 release.'
0,0,0,0,0,'This will do some tests on hashcode/equals.'
0,0,0,0,0,'Maybe a cleaner way to do it would be to return Float.floatToIntBits) ^ getClass.hashCode.\r\n'
0,1,0,0,0,+1
0,0,0,0,1,' let\'s assume we have 1M docs, it would mean we need to split 10 times. In such a scenario, we would likely split 4 times on lat, 4 times on lon and 2 times on the population dimension.'
0,0,0,0,0,'I\'ve attached a new p'
0,0,0,0,0,'Feels wrong to me to override hashCode but not equals. '
0,0,0,0,0,'In the tests, i would add a call to QueryUtils.check to one of your matchnodocsqueries. '
0,0,0,0,1,'is the hashcode/equals stuff needed here or can the superclass impls in Query be used? '
0,0,0,1,0,'Bulk close after 5.1 release'
1,0,0,0,0,'Here is a patch that implements what I had in mind when opening this ticket. It ensures that every dimension gets split no less than 2x less than the dimension that had the most splits. '
0,0,0,1,0,'Adrien: I agree about having the hashCode.\r\n\r\n'
1,0,0,0,0,'Patch requiring resourceDescription (either datainput, or string). '
0,0,0,1,0,'Attached patch nulls out the Fieldable reference.'
0,0,0,0,0,'We had quite a few places missing this.'
0,0,0,0,1,'I think we should move this class part of hashCode() to Query.hashCode()? '
0,1,0,0,0,+1
0,1,0,0,0,'With this patch a selective geo query would have to visit 1/256th of the index (8 splits), which is slower, however a selective query on the population dimension would only have to visit 1/4th of the index (2 splits).'
0,1,0,0,0,'Also: are we sure this belongs in IWC?\r\n'
0,1,0,0,0,'This is analogous to \"infoStream\", which is on IW. '
0,0,0,1,0,'It\'s not a config\r\nparameter that affects indexing.\r\n\r\n'
1,0,0,0,0,Patch.
0,0,0,1,0,'Committed. '
0,1,0,0,0,'+1 Adrien.'
1,0,0,0,0,'Should we also track \"segment flushed/aborted\" events?\r\n\r\n'
0,1,0,0,0,'Can you add some jdocs and mark the API as experimental?'
0,0,0,0,1,'Here\'s another iteration. '
0,0,0,1,0,'Changed the name to IndexEventListener. '
0,0,0,1,0,'Added experimental to the Javadocs, and I probably need to add more. '
0,1,0,0,0,'There are some nocommits still, eg, for the reason a flush kicked off. '
0,0,0,0,1,'Reader events should be in a different issue as reader pool is moving out of IW soon? '
1,0,0,0,0,'All tests pass.'
0,1,0,0,0,'Here\'s an update, there\'s one nocommit as I\'m not sure how we want to capture and exception and rethrow (a Throwable). '
0,1,0,0,0,'This would reflect the fact that its much more expensive than a conjunction.'
0,0,0,1,0,'\r\nCommitted revision 1074357.\r\nCommitted revision 1074363.'
1,0,0,0,0,'Patch addresses Doron\'s comments.'
0,1,0,0,0,'I tend to agree with robert that using longs makes things a lot easier here too. We don\'t need to deal with int overflows.'
0,0,0,1,0,'Sure. (I didn\'t release that the ArrayUtil#oversize() was doing this)'
1,0,0,0,0,'I couldn\'t find any, so i tried to create a test that used a custom SecurityManager after building an index in an FSDirectory.'
0,0,0,0,0,'CheckHits assumes that if there is a single sub explanation, then its value is necessarily the same as the parent explanation. '
1,0,0,0,0,'Patch fixes the bug by moving to track reference count by DTR. Also, added a test which covers that bug .'
1,0,0,0,0,'Maybe ScoreMode should be a public enum inside the join package.'
0,0,0,1,0,'Adding the reason a flush occurred requires quite a bit of refactoring that we can probably leave for later if it\'s needed. '
0,0,0,0,0,'+1\r\n\r\nThank you for digging [~jpountz]!'
0,0,0,1,0,'Updated to trunk, and all tests pass.'
0,0,0,0,0,'\r\ni hav\'nt read hte whole issue, but as i recall this is just a default assumption in hte legacy base class, most use cases should be using the \"Complex\" suclass where the match property can be set explicitly.'
1,0,0,0,0,'This fails with dismax when there is a single sub that produces a negative score since in that case it uses 0 as a max score and multiplies the score with the tie breaker factor.'
0,0,0,0,0,'Bulk close resolved issues after 6.2.0 release.'
0,0,0,0,0,'This is a test bug. '
0,0,0,0,0,'When i see code overriding hashcode/equals and not calling super.hashcode/super.equals, its a bad sign.\r\n\r\n'
1,0,0,0,0,'OK here\'s yet another approach that is promising, but this is still a\r\nrough work in progress...'
0,1,0,0,0,'I agree, Uwe – I\'ll fold that into the patch. '
1,0,0,0,0,'Draft patch. Added ScoreMode as parameter to JoinUtil#createJoinQuery(...).\r\n\r\n'
1,0,0,0,0,'The other idea (just for discussion) would be \"number of i/os\".\r\n\r\nSo for example, phrasequery\'s formula would likely use totalTermFreq rather than docFreq.'
0,0,0,1,0,'I\'ll have a deeper look and see if it really make sense to \'fallback\' such calls or either it\'d be safer and reasonable to follow your suggestion.'
1,0,0,0,0,'Here\'s a patch implementing this check with [-LUCENE-4202-|https://issues.apache.org/jira/browse/LUCENE-4202]. It excludes any test code, but I didnt add any exceptions for legitimate command-line tools.\r\n\r\nCurrent list looks like:'
0,0,0,1,0,'So they work in lock-step, on the same actual underlying index input (sharing io buffer, etc).\r\n\r\n'
0,0,0,1,0,'This can be done with the Java API (new File(source) -> toURI()). '
0,0,0,1,0,'Closed after release.'
0,0,0,0,0,'This suite was added under [-LUCENE-7132-|https://issues.apache.org/jira/browse/LUCENE-7132]'
0,0,0,0,0,'alternativeAttaching a slightly new version of the patch incorporating roughly the idea of 3-distinct methods. Yet, having 3 methods instead of one was kind of ugly to implement and I didn\'t want to add switch case statements to visitSubScorer methods just to call a diff'
0,0,1,0,0,'When the typical group size gets a lot bigger than the number of bits in a long, another implementation might be faster. '
0,0,0,1,0,'New patch with small changes: renamed to BlockGroupingCollector, fixed it to set the totalGroupCount in the returned TopGroups, removed some dead code and shuffled some params from ctor -> getTopGroups.\r\n\r\n'
1,0,0,0,0,'Updated patch.\r\n * Started adding randomizing score mode in TestJoinUtil test class.\r\n * Made ScoreMode a public enum in join package.'
1,0,0,0,0,'patch\n'
0,0,0,0,0,'Bulk close for 3.1'
0,0,0,1,0,'the test cleans itself up in afterClass(), so there is in fact an issue.'
0,0,0,0,0,'Leaving this issue open to deal with Robert\'s patch.'
0,0,0,0,0,'crap - forgot to commit branch_3x part 3 port...'
1,0,0,0,0,'All \"merge abort checks\" are gone and instead handled by the per-merge\r\nrate limiter that IW sets up for each merge. This gives merge\r\nschedulers \"io nice\"-like control over each merge thread.'
0,0,0,0,0,'I think the site docs will also need some revision.'
1,0,0,0,0,'p=lucene-solr.git;h=5dfaf03] ]\r\n\r\n[-LUCENE-7132-|https://issues.apache.org/jira/browse/LUCENE-7132]: BooleanQuery sometimes assigned the wrong score when ranges of documents had only one clause matching while other ranges had more than one clause matchng'
0,0,0,1,0,'OK I\'ll backport.'
0,0,0,0,0,'Bulk close for 3.1'
1,0,0,0,0,'Patch with suggested changes.'
0,0,0,0,0,'decisionAnyway, I changed the 3 parameter methods to take a single argument holding all required information. This is way more extensible and flexible since we don\'t need to change a method signature just to add a n'
0,0,0,0,0,'alternativeAttached patch.\r\n\r\nThere were two things to fix. First, I added Scorer.freq so that\r\nyou could get the freq of the current match. Second, I added a way to\r\nvisit all sub-scorers for a g'
0,0,0,1,0,'As Tokenizers are reused, the analyzer holds also a reference to the last used Reader. The easy fix would be to unset the Reader in Tokenizer.close(). If this is the case for you, that may be easy to do. So Tokenizer.close() looks like this:\r\n'
0,1,0,0,0,'Thanks Steven!'
1,0,0,0,0,'I think that we should use a primitive iterator, e.g. facet collections have IntIterator interface. And so the method should be something like IntIterator getChildren(int ordinal)?'
0,0,0,1,0,'while investigating this, i also discovered some heinous bugs in CheckHits where match and non match were naively been asserted using the explanation value instead of actaully checking \"isMatch\" (which explained why some of my earlier attempts at testing for this bug in PhraseQUery explain didn\'t surface the problem)'
0,1,0,0,0,'\r\n+1. '
0,1,0,0,0,'Sure, drop the precommit idea. '
0,0,0,1,0,'Initial patch commited: trunk rev. 1072250, branch_3x rev. 1072386.\r\n\r\n'
0,1,0,0,0,'Will fix, thanks.'
0,1,0,0,0,'I like this, yet I think in that case we should go back to estimateCost rather than docId etc. since for a bitset this is way different that for a PhraseScorer. '
0,0,0,1,0,'Here\'s a patch: including fixes.\r\n\r\nI think its ready to commit'
0,0,0,1,0,'Thanks Markus, looks great, I\'ll commit shortly.'
1,0,0,0,0,'Here is a new patch that doesn\'t override equals or hashCode and changes Query to use the class in the hashCode method as Adrien suggested.\r\n '
0,0,0,0,0,'Interesting. '
0,0,0,1,0,'I think these should be fixed now.\r\n\r\nIt was a tricky concurrency hazard, where an indexing thread that\'s resolving DV updates thinks it\'s done just as a merge is wrapping up and in that case there was a window between the two threads where DV updates could be lost.'
0,0,0,1,0,'Added TaxoReader.getChildren(int ordinal) and corresponding test. I also migrated PrintTaxonomyStats to use getChildren, which removed all mentions of ParallelTaxonomyArrays from it.'
0,0,0,0,0,'\r\nI like that idea! '
0,0,0,1,0,'I guess it\'s manually possible now with -Dbuild.dir=my_branch_dir?'
0,0,0,0,0,'I prefer to git clean the build folder myself, actually .'
0,0,0,0,0,'We can add some property to disable it that you can put into your . At least I see those problem'
0,1,0,0,0,'+1, thanks Robert!'
0,1,0,0,0,'Please make this an opt-out feature. '
0,0,1,0,0,'\r\nThis is an interesting idea. '
0,0,0,1,0,'The current patch does not add much magic, it just does ensure that when you change the branch that you have no build artifacts (what does \"ant clean\" does otherwise?).\r\n\r\n'
0,0,0,1,0,'I also changed target name to \"ant git-autoclean\".\r\n\r\n'
0,0,0,0,0,'I guess it will force us to fix places in the build/python scripts/whatever that might have a hardcoded `build/` somewhere, and so on.'
0,0,0,0,0,'Can we iterate by committing this solution first, then working on the separate build directory as a followup.\r\n\r\n'
1,0,0,0,0,'The idea here was to be on safe side and do ant-clean on branch change automatically. '
1,0,0,0,0,'If so, it\'d be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder. This way you could actually switch a branch and then continue working as usual because it should work flawlessly.'
0,0,0,1,0,'That\'s a good point Rob. '
0,0,1,0,0,'[~dawid.weiss]: I think, the patch will only check a branch change, not a \"downgrade\" to earlier commit (have no tried, maybe its different). '
0,1,0,0,0,'This way you could actually switch a branch and then continue working as usual because it should work flawlessly.'
0,1,0,0,0,'this is a really good idea. otherwise we are going to get bug reports that look like data corruption:\r\n'
0,0,0,0,0,'\r\nThis removes everything  and restores a pristine checkout state. Just in case somebody finds it useful.'
0,0,0,0,0,'The patch here is just a good first stab!'
1,0,0,0,0,'Hi,\r\nI updated the patch a bit and renamed the property to disable. '
0,0,1,0,0,'This is interesting because I don\'t see this behavior. When I checkout a historical branch, it just updates timestamps on changed files in the working copy, but updates them to checkout time?\r\n'
0,0,0,1,0,'If so, it\'d be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder. '
0,0,0,1,0,'To disable ([~dweiss]), add git-autoclean.disabled=true to your ~/lucene.build.properties\r\n\r\nI\'ll commit this later (when back at home).'
0,0,0,0,0,'For me the checkout is always in some partly insane state with build artifacts after a branch change.'
0,0,0,0,0,'I like the idea, too. '
0,0,0,1,0,'I committed this. '
1,0,0,0,0,Patch.
0,0,0,1,0,'\r\nto make sure we have a leading 0 or rather two leading 0\'s to prevent the overflow.'
0,0,0,0,0,'Yes, absolutely. Ant clean should really clean everything. As a side note, I use this *very* often:\r\n'
1,0,0,0,0,'Initial patch.\r\n\r\n'
0,0,0,0,0,'have branch-dedicated build folder\n'
1,0,0,0,0,'I think the problem is also that when changing branches you may have still compiled artifacts somewhere laying around. '
0,0,0,0,0,'I think we should open another issues about the multiple build dir change.'
0,1,0,0,0,'I think it is ready to be committed.'
0,0,0,1,0,'We just used the wrong shift. '
0,0,0,0,0,'Again, the current situation is a real problem, because the errors you see look like corruption.'
0,0,0,1,0,'Committed revision 1234450 (3x), 1234451 (trunk).\r\n\r\nThanks Doron !'
0,1,0,0,0,'+1 to close this trap ASAP. '
1,0,0,0,0,'Updated patch.\r\n * Fixed random tests.\r\n * Added support for explain.\r\n * Added ScoreMode support for documents that relate to more than one document.'
0,0,0,1,0,'I\'m not against this patch, I just find it odd you\'re experiencing the issue because I\'ve never had any problem with it (yes, git will not wipe out any ignored files automatically – these are ignored after all – but it\'ll switch any versioned files and update timestamps properly).\r\n\r\n'
0,1,0,0,0,'\r\nThat is the main issue. '
1,0,0,0,0,'But if we do this, I think it should be `build/<branch>` and ant clean still removes build entirely (means it still *really* cleans). '
0,0,0,0,0,'codesummarizationIn class CustomAnalyzer the following methods has been changed: \nbuilder\ninitReader\ninitReaderForNormalization\ncreateComponents\nnormalize\ngetPositionIncrementGap\ngetOffsetGap\ngetCharFilterFactories\ngetTokenizerFactory\ngetTokenFilterFactories\ntoString\nwithDefaultMatchVersion\nwithPositionIncrementGap\nwithOffsetGap\nwithTokenizer\naddTokenFilter\naddCharFilter\nwhen\nwhenTerm\nshouldFilter\napplyDefaultParams\nparamsToMap\napplyResourceLoader\nendwhen\nIn class CarrotClusteringEngine the following methods has been changed: \nisAvailable\ninit\ncluster\ngetFieldsToLoad\ngetFieldsForClustering\ngetDocuments\ngetSearcher\ngetClusteringAlgorithmClass\ngetCustomFieldsMap\ngetConcatenated\nclustersToNamedList\nextractCarrotAttributes\nwithContextClassLoader\nIn class TestCoreContainer the following methods has been changed: \nbeforeClass\nafterClass\ninit\ntestShareSchema\ntestReloadSequential\ntestReloadThreaded\nrun\ntestNoCores\ntestLogWatcherEnabledByDefault\ntestDeleteBadCores\ntestClassLoaderHierarchy\ntestSharedLib\ntestCustomHandlers\nadd\ncreate\npersist\ndelete\nrename\nswap\ndiscover\ntestCoreInitFailuresFromEmptyContainer\ntestCoreInitFailuresOnReload\ngetCoreStartTime\nIn class SolrCLI the following methods has been changed: \ngetName\ngetOptions\nrunTool\nechoIfVerbose\nrunImpl\nrunCloudTool\nexit\nmain\nfindTool\nparseCmdLine\ncheckSslStoreSysProp\nraiseLogLevelUnlessVerbose\ngetCommonToolOptions\nnewTool\ndisplayToolOptions\ngetToolOptions\njoinCommonAndToolOptions\njoinOptions\nprocessCommandLineArgs\nfindToolClassesInPackage\nfindClasses\ncheckCommunicationError\nattemptHttpHead\nexceptionIsAuthRelated\ngetHttpClient\ncloseHttpClient\npostJsonToSolr\ngetJson\nhandleResponse\nasString\nasLong\nasList\nasMap\npathAs\natPath\nwaitToSeeSolrUp\ngetStatus\nreportStatus\ngetCloudStatus\nuptime\ntoString\nhashCode\nequals\ncompareTo\ngetShardState\nresolveSolrUrl\ngetZkHost\nsafeCheckCollectionExists\nsafeCheckCoreExists\noptionAsInt\ndeleteCollection\ndeleteCore\nrunDihExample\nrunExample\nrunCloudExample\nsetCollectionConfigProperty\nwaitToSeeLiveNodes\nstartSolr\ncheckPortConflict\nreadExtraArgs\ncreateCloudExampleCollection\nisValidConfig\ngetNodeStatus\nsetupExampleDir\ncopyIfNeeded\nisPortAvailable\npromptForPort\npromptForInt\nrunAssert\nassertSolrRunning\nassertSolrNotRunning\nassertSolrRunningInCloudMode\nassertSolrNotRunningInCloudMode\nsameUser\nassertFileExists\nassertFileNotExists\nassertRootUser\nassertNotRootUser\ncurrentUser\nuserForDir\nexitOrException\nisSolrRunningOn\nrunningSolrIsCloud\nensureArgumentIsValidBooleanIfPresent\nhandleKerberos\nhandleBasicAuth\nprintAuthEnablingInstructions\nupdateIncludeFileEnableAuth\nupdateIncludeFileDisableAuth\narchiveGcLogs\narchiveConsoleLogs\nrotateSolrLogs\nremoveOldSolrLogs\nout\nprepareLogsPath\nsetLogPath\nsetServerPath\nsetQuiet\nIn class ClasspathResourceLoader the following methods has been changed: \nopenResource\nfindClass\nnewInstance\nIn class SolrTestCaseJ4 the following methods has been changed: \nwriteCoreProperties\nsetupTestCases\nteardownTestCases\nassumeWorkingMockito\nclearObjectTrackerAndCheckEmpty\ninitClassLogLevels\ninitMethodLogLevels\nrestoreMethodLogLevels\nisSSLMode\nuseFactory\nresetFactory\nbuildSSLConfig\nbuildJettyConfig\nbuildUrl\nwhitespaceMockTokenizer\nsetupNoCoreTest\nnewRandomConfig\ngetWrappedException\nsetUp\ntearDown\ninitCore\nstartTrackingSearchers\nignoreException\nunIgnoreException\nresetExceptionIgnores\ngetClassName\ngetSimpleClassName\ngetSchemaFile\ngetSolrConfigFile\ncreateCore\ncreateCoreContainer\ncreateDefaultCoreContainer\nhasInitException\npostSetUp\npreTearDown\ndeleteCore\ngetNextAvailablePort\nassertU\nassertF...codesummarization'
0,0,0,1,0,'Commit 1516001 from [~mikemccand] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1516001] ]\r\n\r\n[-LUCENE-3849-|https://issues.apache.org/jira/browse/LUCENE-3849]: fix some more test only TokenStreams'
0,0,0,0,0,'decisionThe problem leading to this quiet heavy change was that I wanted to call a specific function for each Boolean.Occur relationship. That turned out to be ugly since I had to either add a switch / case statement to each of the visitSubScorers methods or add a visitSubScorers method for each relationship. I didn\'t like that at all so I moved forward to specify a callback for each relati'
0,0,0,1,0,'I also removed RateLimitedDirectoryWrapper: it\'s too simplistic and\r\ntoo dangerous. Finally I cleaned a few things up and improved verbose\r\ninfoStream logging so we can see more stats for each merge.'
0,0,1,0,0,'But if we do this, I think it should be `build/<branch>` and ant clean still removes build entirely (means it still really cleans). '
0,0,0,0,1,'Shouldn\'t next() throw NoSuchElementException if child is already INVALID_ORDINAL? '
0,0,0,1,0,'You don\'t need to use your own growFactor ... just call ArrayUtil.grow\r\ndirectly (it already oversizes under the hood for you).'
0,0,0,0,0,'decisionPassing it on the stack has one limitation which is that if I want to pass more information around in the future I need to change the interface while I only add a member to the \"context\" if I pass it that way. Another reason was that if I want to pass custom info from a custom query scorer to another I can not do that since there is no'
0,0,0,0,1,'Maybe explain added RAM requirements when scores are tracked in the\r\njavadocs?'
0,1,0,0,0,'This is an interesting idea. '
0,0,0,0,1,'Probably javadocs should somewhere explain about the \"first time doc\r\nis emitted it gets that score\"?'
0,0,0,0,1,'It shouldn\'t ever return INVALID_ORDINAL, right? '
0,1,0,0,0,'Patch looks great!'
0,0,0,1,0,'attached is a fix. '
0,0,0,0,0,'proI see your point of being to heavy though.\r\n\r\nI guess we should go back to the original approach since I don\'t want to decide it on a switch case ba'
0,0,0,0,0,'I\'m waiting for someone to enable Nexus access for Lucene: [-INFRA-4497-|https://issues.apache.org/jira/browse/INFRA-4497] - I had planned to hassle them after a week had gone by with no action .'
0,0,0,1,0,'I did some minor cleanups (typos, don\'t link to unversioned (trunk) resources since this is a versioned document, etc).'
1,0,0,0,0,'I guess it will force us to fix places in the build/python scripts/whatever that might have a hardcoded `build/` somewhere, and so on.'
0,0,0,0,1,'Maybe rename TermsWithScoreCollector.MV.Avg.ordScores -> .scoreCounts\r\n(and .scores -> .scoreSums?).'
1,0,0,0,0,'Added a lint task that currently just delegates to PMD. '
0,0,0,0,0,'alternativeAnother iter of this patch; I think it\'s ready to commit. I commented out BooleanScorer\'s handling of required=true since we never use it . I also added C'
0,0,0,0,0,'decisionI\'m ok with vi'
0,0,0,0,1,'Fix omitted to emitted in the comment on top of \"class MVInnerScorer\".'
1,0,0,0,0,'This adds \"enable/disableAutoIOThrottle\" methods to CMS, to have CMS\r\npick a reasonable IO throttle over time so merges don\'t fall behind\r\nbut also don\'t suck up all available IO. It\'s a live setting, and\r\ndefault is on. CMS.getIORateLimitMBPerSec returns the current\r\nauto-IO-rate.'
0,0,0,0,0,'I was referring to Uwe\'s email about stopping all builds due to 1.6 versus 1.6 issues in the jail.'
0,0,0,0,0,'\r\nYeah, the same credentials worked earlier, and now don\'t work.\r\n\r\n'
0,0,0,0,0,'the nexus log says, \'unable to authenticate sarowe\'.'
0,0,0,0,0,'Not that it\'s especially my business, but how did the snapshots get pushed historically to nexus if you didn\'t have access to nexus?'
0,0,0,0,0,'\r\nRighteous to the max.'
0,0,1,0,0,'Please don\'t warp this with Files.exists() because this is not really atomic and not a good check, because in fact we need *write* access to the directory at a later stage, too - because we call fsync on the directory itsself to ensure commits are written into the directory file itsself.'
0,0,0,0,0,'You should better check people.ao or SVN for credentials.'
0,0,0,0,0,'I think we just need to find out why is that scorer not initialized, or we could default the cost to 0  if there is null pointer exception '
0,0,0,0,1,'Maybe remove @throws IAE from createJoinQuery\'s javadocs? (But, still\r\nthrow it... in case we add a new ScoreMode and forget to fix this\r\ncode, in the future). Because today all ScoreMode enum values work...'
0,0,0,0,0,'codesummarizationcodesummarization'
0,0,0,0,0,'FYI, on lucene.zones.apache.org, where all of the Lucene/Solr Jenkins jobs run, /home/hudson/.m2/settings.xml does not exist , so we can\'t depend on pre-existing Jenkins credentials.'
0,0,0,0,0,'Problems with the patch i attached :\r\n\r\n1) we don\'t distribute dev-tools in our releases, so at a minimum we\'d need to find a new home for any package-list files we wanted to ship.\r\n\r\n2) the Java documentation from Oracle has some licensing/restrictions that affect redistribution which don\'t seem to be compatible with ASF 3rd party licensing policy so we can\'t include the java package-list files in our releases\r\n\r\n...we could still use the ideas in this patch to deal with package-list files for *non*java/oracle distrobutions, but at the time this patch was written the only other extneral javadocs we linked to where that might be useful was junit, and since this patch was created, that link has just been removed outright from our build.xml files.\r\n\r\n'
0,0,0,0,0,'I noticed also that solr uses an online link for junit javadocs... we should download this one and do the same, too.\r\n'
0,0,0,0,0,'Turns out my LDAP account was locked for \"excessive pw fails\" - Joe Schaefer unlocked my account for me, and I am now in the process of deploying Solr trunk snapshot artifacts.\r\n\r\n'
0,0,0,0,0,'I knew it was the Apache LDAP server, and not specifically Nexus, when SVN logins failed when I tried to commit the patch listed above.'
0,0,0,1,0,'Bulk close resolved issues after 6.2.0 release.'
0,0,0,0,0,'The Jenkins Maven trunk and branch_3x builds are now configured to deploy snapshot artifacts to the Apache snapshot repository, and both have successfully done so.'
0,1,0,0,0,'I don\'t mind throwing it though. '
0,0,0,0,0,'I\'ll work on switching the Jenkins jobs now.'
0,0,0,0,0,'\r\nI found the following comment on [-SOLR-586-|https://issues.apache.org/jira/browse/SOLR-586] that described the process by which Maven snapshots made it into the Apache snapshot repository: [https://issues.apache.org/jira/browse/SOLR-586?'
0,0,0,1,0,'Also, since it returns ordinals, it\'s kind of ok to return INVALID_ORDINAL. '
1,0,0,0,0,'here\'s just where i am so far, just scavenging stored tests from where i can find. I still havent ported all the ones from .compressing package yet.'
1,0,0,0,0,'New patch... I think it\'s close.'
0,0,0,0,0,focusedCommentId=12623985&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12623985].
0,0,0,0,1,'I think in the base test class, tesMostEqual() is a mistake? '
1,0,0,0,0,'I wished that we had a simple IntIterator interface with only next() for this case...'
0,0,0,0,0,'I also think this is a bug in explain!'
0,0,0,0,0,':). In any way, this unchecked exception should probably be caught somewhere.'
0,1,0,0,0,'I like this better than the last patch,  I think the optimization is more general. '
0,0,1,0,0,'BlockJoinQuery still needs hashCode/equals, and a javadoc note (as I remarked earlier at 2454) about the possible inefficiency of the use of OpenBitSet for larger group sizes. '
1,0,0,0,0,'Hmm i missed this: Can we simplify this and just tell users to download the source release?'
0,0,0,0,0,'I would imagine a proper repo manager is a more suitable official location than Jenkins.\r\n\r\n'
0,0,0,0,0,'Perhaps the last step is to now modify README.maven to point to [https://repository.apache.org/content/repositories/snapshots/] instead of Jenkins? '
0,0,0,0,0,'\r\nOkay, that makes sense. '
0,0,0,0,0,'DecisionI\'m not saying its impossible, but just tricky and took trying different things. If you don\'t know the answer, '
0,0,0,0,0,'I\'ll do the same for branch_3x Lucene&Solr.'
0,0,0,0,0,'I\'ve found https repos to be very problematic for maven when the client needs to use an HTTP proxy such as is common in corporate environments. '
1,0,0,0,0,'updated patch. I think this is close... adrien do you know of any other tests we should steal and put in here?'
1,0,0,0,0,'For correct Java 7 implementation and lock factory correctly working after the change (needs fully resolved absolute path), we have to ensure the directory exists *before* the FSDirectory ctor returns, other wise race conditions occur! The makeDirectories() enforces this.'
0,0,0,1,0,'I looked at other IntIterator impls and none throw NoSuchElementException, so I thought it\'s best to follow. '
0,0,0,0,0,'I added a [failing] test but maybe the test is bogus?'
0,0,0,0,0,'I\'m not even sure it needs fixing. '
1,0,0,0,0,'I think it\'s important CMS *not* have min/max MB/sec throttle control:\r\nI think this just invites disaster when apps set them to inappropriate\r\nvalues (but I added a protected CMS method \"escape hatch\" so a\r\nsubclass can override the control logic).'
0,0,0,1,0,'Bulk close after 5.0 release.'
0,0,0,0,0,'Patch looks good! '
0,0,0,0,0,'Commit 1514379 from [~rcmuir] in branch \'dev/branches/branch_4x\'\r\n[ [https://svn.apache.org/r1514379] ]\r\n\r\n[-LUCENE-5166-|https://issues.apache.org/jira/browse/LUCENE-5166]: also fix and test this case where tf > 1 within the passage for a term'
1,0,0,0,0,'Or we could return a not-Java-iterator, that just has int next() that returns INVALID_ORDINAL when it\'s done...'
0,0,0,1,0,'\r\n\r\nThe only query i changed was to make ConjunctionTermScorer algorithm our ConjunctionScorer, sorting subscorers on cost .'
0,0,0,0,0,'4.5 release -> bulk close'
1,0,0,0,0,'Just as a side note, you might be interested to know that we could already have a great static analysis feature... warnings from the javac compiler itself.\r\n\r\n'
0,0,0,0,0,'Oh I see we can\'t quite have Scorer impl this because it doesn\'t know the query. But maybe we can factor out a common method, that the subclass pass'
0,0,0,0,0,'on.\r\n\r\nCan we factor out that switch statement into super\'s visitSubScorers? This way t'
0,0,0,0,0,'Patch against branch_5x'
0,0,0,0,0,'FWIW, i thought this issue  had been resolved a long time ago based on some comments i remember people making, but evidently those comments where on irc/mail and folks didn\'t post them in Jira.\r\n\r\n'
0,0,0,0,0,'Optimizing is with recent Lucene versions not needed anymore. It\'s hard '
0,0,0,0,0,'I don\'t think there\'s anything left here but to resolve as Won\'t Fix'
0,0,0,1,0,'Committing shortly.'
1,0,0,0,0,'Here is the *real* patch against branch_5x.'
0,0,0,1,0,'Yes, if you really want to do this in a test:\r\n\r\nGuideline 9-4 / ACCESS-4: Know how to restrict privileges through doPrivileged\r\n[http://www.oracle.com/technetwork/java/seccodeguide-139067.html#9]'
0,1,0,0,0,'Unfortunately people are not even interested in looking at these, more or less fixing them. '
0,0,0,0,0,'Commit 1514367 from [~rcmuir] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1514367] ]\r\n\r\n[-LUCENE-5166-|https://issues.apache.org/jira/browse/LUCENE-5166]: also fix and test this case where tf > 1 within the passage for a term'
0,1,0,0,0,'+1, looks good.'
0,0,0,0,0,'alternativeAttaching a slightly new version of the patch incorporating roughly the idea of 3-distinct methods. Yet, having 3 methods instead of one was kind of ugly to implement and I didn\'t want to add switch case statements to visitSubScorer methods just to call a diff'
0,0,0,0,1,'So I\'ll have to work a bit harder to see why no intersection is detected.'
1,0,0,0,0,'I augmented [~mikemccand]\'s explain output to include relationship information'
0,0,0,0,0,'If there are no objections I will commit this tomorrow.'
0,0,1,0,0,'which then leads to the following line in the forensics dump from this failure'
0,1,0,0,0,'Good tip!  '
0,0,0,0,0,'alternativeHer'
0,0,0,1,0,'Patch updated to trunk (as Stefan needs this for [-LUCENE-4571-|https://issues.apache.org/jira/browse/LUCENE-4571]).\r\n\r\nI carefully reviewed all these costs and also integrated it into spans. Though these queries (e.g. nearquery) dont yet use it, they should.'
0,1,0,0,0,'From this it looked plausible that the cell is off the top of the world in z, and thus did not intersect with it for that reason.'
0,0,0,0,0,'These are places we can'
1,0,0,0,0,'Here is another alternative that still stops calling matches() after the first matching clause is found but now also pretends there is a single matching clause so that score() won\'t fail.\r\n\r\n'
1,0,0,0,0,'Anyway, this patch  fixes the SCM definition problem for trunk POMs. I\'ll do the same for branch_3x too.\r\n'
0,1,0,0,0,'+1 to commit this, i am interested in the list of \"bad guys\" too on some other issue. '
1,0,0,0,0,'I had previously thought that a single SCM section in the lucene-solr-grandparent POM would do the trick'
0,0,0,0,0,'This is the smallest viable fix I can think of for [https://builds.apache.org/job/Lucene-Solr-NightlyTests-5.x/775/]. '
0,1,0,0,0,'\r\n+1, I think that\'s best.'
0,0,0,0,0,'I had started seeing if we could implicitely wrap with ConstantScorer but there seem to be a couple of places that call score when needsScores is false. '
0,0,0,1,0,'Also, the iterator is wrong if a rehash happens right? '
0,0,0,1,0,'Bulk close after 5.1 release'
0,0,0,0,0,'I opened [-LUCENE-6330-|https://issues.apache.org/jira/browse/LUCENE-6330] to tackle BooleanScorer .'
0,0,1,0,0,'but upon inspection of the output from mvn help:effective-pom, I can see that the grandparent POM\'s section doesn\'t properly interpolate $module-directory (since the property isn\'t defined in that POM), and the values inherited in other POMs are all wrong, because path steps are added with artifact names instead of directory names, in addition to $module-directory interpolation...'
1,0,0,0,0,'Can you add to the javadocs that the set should not be modified while the iterator is in use, except using the iterator\'s remove method ?'
0,0,0,0,0,'AlternativeBut we can also do the javadocs too.\r\n\r\nThen i '
0,0,0,0,0,'hole bug! It should maybe some separate class like OutOfOrderDocIdReporter  that only implements collect. And the navigation api (ad'
0,0,0,0,0,'p=lucene-solr.git;h=9f8fe12] ]\r\n\r\n[-LUCENE-7085-|https://issues.apache.org/jira/browse/LUCENE-7085]: PointRangeQuery.equals sometimes returns false even if queries were in fact equal'
0,0,0,1,0,'I also hit and fixed a separate bug in InetAddressPoint.newSetQuery where it hit an exception if you tried to pass more than one InetAddress in the set :)Sheesh.'
1,0,0,0,0,'Patch includes a test in TestLockFactory and fix in NativeFSLock.'
0,1,0,0,0,'s/comparsion/comparison/ but otherwise +1 to the patch'
0,1,0,0,0,'nice find!'
0,1,0,0,0,'What a nice catch, thanks [~jpountz]!'
0,0,0,0,0,'alternativeok I see the bug. ReqExclScorer can indeed be buggy here, because it marks \'req\' as null for \'exhausted\'. So if you call cost after that, you '
0,0,1,0,0,'I wanted to avoid introducing another class (facet collections already use this primitive IntIterator), but maybe a ChildrenIterator with next() is simplest. '
0,0,0,0,0,'p=lucene-solr.git;h=770e508] ]\r\n\r\n[-LUCENE-7085-|https://issues.apache.org/jira/browse/LUCENE-7085]: PointRangeQuery.equals sometimes returns false even if queries were in fact equal'
0,0,0,1,0,'Passing it on the stack has one limitation which is that if I want to pass more information around in the future I need to change the interface while I only add a member to the \"context\" if I pass it that way. Another reason was that if I want to pass custom info from a custom query scorer to another I can not do that since there is not context.'
0,0,0,0,0,'Lets just commit it with this one and think about it for later!'
0,0,0,0,0,'p=lucene-solr.git;h=f0ed113] ]\r\n\r\n[-LUCENE-7085-|https://issues.apache.org/jira/browse/LUCENE-7085]: PointRangeQuery.equals sometimes returns false even if queries were in fact equal'
0,0,0,1,0,'I\'m ok with visitScorers'
0,0,0,0,0,'ProIn my opinion, BS1 should *not* implement the Scorer interface, that the whole bug! It should maybe some separate c'
0,0,0,1,0,'Committed to trunk and branch_3x.'
1,0,0,0,0,'Setting the right IO throttle is a fun control problem (see\r\n[http://en.wikipedia.org/wiki/Control_theory]), much like the fan in\r\nyour laptop that changes its speed depending on internal temperature,\r\nor a factory that must add more workers depending on incoming jobs.'
1,0,0,0,0,'Here is the test. I had to f*ck with the last Permission, because to run the code with restricted permissions you need the additional permission, otherwise it runs with no permissions at all  !https://issues.apache.org/jira/images/icons/emoticons/sad.png|width=16,height=16!  (see javadocs)\r\n\r\nI also added a helper method, we may move this to LTC later'
1,0,0,0,0,'Here\'s a patch I\'m testing (all tests seem to pass, but I will see if i can add some better ones).\r\nFieldTermStack is ordered by position, but the current code doesn\'t really handle position increments of 0 (synonyms). '
1,0,0,0,0,'Patch.\r\n\r\n'
0,0,0,0,1,'The last thing to verify is whether the edge points for the XYZSolid are correct.'
0,0,0,0,0,'alternativeFirst we should clarify/define, when it is ok to call these methods, and what the semantics are supposed to be if you can call it \"after consuming\" (is it ok to call it at any time? what about after it returns NO'
0,1,0,0,0,'+1, thanks Shai!'
0,0,1,0,0,'In general, there is no problem with NativeFSLock at all, because the removal of the lock file is just a \"cleanup\", which is not needed (not deleting the lock file would produce no problem - we only delete it to be interoperable with SimpleFSLockFactory). The existence of a lock file is no indice for a locked directory, only if a lock is \"on the file\". The JVM normally automatically removes lock files on exit.\r\n'
0,0,0,0,0,'alternativeThen, depending on that, we just have to fix and add direct tests against our scorers for this, because otherwise problems will continue to happen, since core scorers are generally just checking this up-front and wont find bugs in the current test suite. Maybe improving AssertingScorer to do t'
0,1,0,0,0,'+1, I really like this approach.'
0,0,1,0,0,'I don\'t see this as a bug: Lucene \"owns\" the directory, so it must be able to create/delete/fsync/whatever it. If the directory is on a read-only FS and you just open DirectoryReader, its perfectly fine if the directory exists (no OS error). It is just FilePermission that complains.'
0,1,0,0,0,'Nice speedups!'
0,1,0,0,0,'I verified that the shape appears to be properly built, with the right internal edges, and that when looking for edge intersections it examines the correct number and type of edges.'
0,0,0,0,0,'Both are helpful. '
0,1,0,0,0,'I see your point of being to heavy though.\r\n\r\nI guess we should go back to the original approach since I don\'t want to decide it on a switch case basis. '
0,0,0,0,0,'OK let\'s not try to address that on this issue ... I\'m not even s'
1,0,0,0,0,'Here\'s a better patch. This should be correct for all crazy combinations. '
1,0,0,0,0,'Patch with ChildrenIterator'
0,0,0,1,0,'This allows the current stack pop/push algorithm to work without a bunch of additional complexity to \"restore\" synonyms.'
0,0,0,1,0,'I upped the starting rate to 20 MB/sec (from 5 before): this helps it\r\nmove to less throttling more quickly before merges fall behind in the\r\nbeginning during heavy indexing.'
0,1,0,0,0,'I tested on spinning disks ... it seems to behave well: under intense\r\nindexing, the throttle moves to unlimited since the spinning disk\r\ncan\'t keep up. Under light indexing, it stays low.'
1,0,0,0,0,'First we should clarify/define, when it is ok to call these methods, and what the semantics are supposed to be if you can call it \"after consuming\" (is it ok to call it at any time? what about after it returns NO_MORE_DOCS?).'
1,0,0,0,0,'In this specific case, DrillsideWays calls cost() after nextDoc() already returned NO_MORE_DOCS. This is why ReqExclScorer sets req to null.'
1,0,0,0,0,'New patch, fixing previous nocommits / downgrading to TODOs. I also removed the specialized scorers since they seem not to help much.\r\n'
0,1,0,0,0,'So we should fix them asap (in a separate issue).\r\n\r\n'
1,0,0,0,0,'Then, depending on that, we just have to fix and add direct tests against our scorers for this, because otherwise problems will continue to happen, since core scorers are generally just checking this up-front and wont find bugs in the current test suite. Maybe improving AssertingScorer to do this can help.'
0,0,1,0,0,'I couldn\'t really make sense of your \"TestSecurity\" example – nothing in it seems to be enforcing the \"deny write access\" part of the issue, it\'s just allowing more things (notably: read access for all files) then the default policy'
1,0,0,0,0,'\r\nSo another option is to simply add SuppressCodecs(\"Lucene3x\") annotation to the classification module and document that you should run IndexUpgrader on any old 3.x segments you have lying around.'
0,0,0,0,1,'Hi,\r\nIBM JDK8+ should also use OpenJDK internally, so I dont\'t think hit has much different options. '
0,0,1,0,0,'thanks Robert, that may make sense but I\'m not really sure as in 4.x we still support 3x codecs in general.\r\n'
0,0,0,1,0,'But it should be simple to run the \"static analysis\" task (and remember the name), without having to run precommit (which does a lot of other stuff).'
0,0,0,1,0,'+1, we should roll our own, hopefully correctly this time  '
1,0,0,0,0,'New patch, fixing one nocommit, adding some more infoStream logging\r\naround applying deletes, fixing \"ant precommit\". I also fixed CFS\r\nbuilding to also throttle.\r\n'
1,0,0,0,0,'ok I see the bug. ReqExclScorer can indeed be buggy here, because it marks \'req\' as null for \'exhausted\'. So if you call cost() after that, you will get NPE.'
0,0,0,0,0,'This is truly a Lucene level bug. '
0,0,0,0,0,'Query instances returned from JoinUtil will implement equals and hashcode in future versions.'
0,0,0,0,0,'[~mikemccand], [~jpountz] - any theory as to what could cause diff scores to be produced between diff test runs even though the Query, Documents & Similarity are fixed in Ahmet\'s TestExplain.testRajeshData?\r\n\r\n'
0,0,0,0,0,'Lucene only patch. '
0,0,0,1,0,'Simple POC patch which adds a pmd target to lucene/common-build.xml. It currently uses iv'
0,0,0,0,0,'I still see one omitted  !'
0,0,0,0,0,'https://issues.apache.org/jira/images/icons/emoticons/smile.png|width=16,height=16!  '
0,0,0,0,0,'Hi David, I just committed [-LUCENE-4704-|https://issues.apache.org/jira/browse/LUCENE-4704]. '
0,0,0,0,0,'Thanks alot!\r\n\r\n'
0,0,0,0,0,'I\'ll try the patch when 4.2 get\'s released.'
0,0,0,0,0,'I think this happens b/c the Query that the JoinUtil returns doesn\'t override the equals and hashcode method . This should be fixed, otherwise this query can never be cached . Can you check if the following works:\r\n'
0,1,0,0,0,'Otherwise this looks great: +1 to commit!'
0,0,0,1,0,'lover.\r\n\r\nStill much to do but just showing where things are going.Decision'
0,0,0,0,1,'odule? I don\'t really want to latch this onto compile like clover.\r\n\r\nStill m'
1,0,0,0,0,'If you use SimpleFSLockFactory and call IndexWriter.unlock(), but SimpleFSLock.release() fails, you hit an exception. With NativeFSLock it\'s not the same, simply because NativeFSLockFactory.makeLock creates a new instance with a \'lock = null\', and therefore calling release() is always a no-op.'
1,0,0,0,0,' I expect to be able to open the writer after unlock returns. With NativeFSLock, I cannot fail to obtain the lock if it\'s there but nobody uses it, so if I hit the exception, I shouldn\'t be allowed to call unlock.'
0,0,0,0,1,'\r\nWhat I meant is that if instead of checking epoch on TR you check on DTW, you won\'t (I think!) '
0,0,0,1,0,'We\'ve been seeing this issue semi-regularly in our app. We\'ve been working around it so far by simplifying our queries to remove clauses (probably a good idea anyway) but we can never be sure our users won\'t find a way to break it!'
0,0,0,0,0,'ase. It can be reproduced with: ant test -Dtestcase=TestExplain -Dtests.method=testRajeshData -Dtests.seed=D5E55A7E84F4C82C -Dtests.slow=true -Dtests.locale=es-HN -Dtests.timezone=Asia/Samarkand -Dtests.asserts=true -Dtests.file.encoding=UT'
0,0,0,0,0,'Randomized test case for Lucene in hopes that it will trigger sometime. '
0,1,0,0,0,'\r\nTests pass ... I think it\'s ready.'
0,1,0,0,0,'+1\r\n\r\nThis patch looks great Uwe, generalizing that method into LTC sounds good to me.'
0,0,0,0,1,'Would it really be that much slower if it was slightly more reasonable, e.g. storing freqs\r\nin packed ints (with huper-duper fast options) instead of wasting so much on them?'
0,0,0,0,1,'I do think maybe this class needs some more warnings, to me it seems it will use crazy amounts of RAM.'
0,0,0,0,0,'DecisionI also think it would be good to separate that from the goal of throwing exceptions for bogus arguments  only does actual loading).\r\n\r\nThe challenge in fixing '
0,0,0,0,0,'I have fixed all the factories, but currently I just started fixing tests and adding tests for bogus parameters to all factories.\r\n\r\n'
0,0,0,1,0,'Bulk close after 5.0 release.'
0,0,0,0,1,'I also am not sure I like the name \"Direct\"... is it crazy to suggest \"Instantiated\"?'
0,0,0,0,0,'Which doesn\'t support mapping the scores from the `from` side to the `to` side. '
0,0,1,0,0,'If you use NativeFSLock you should never-ever try to forcefully unlock! If there is a lock, then the dir is used and it would be the badest thing you could do, to forcefully unlock. The O/S will take care of lock removal on JVM exit.'
0,0,0,0,0,'slow moving'
1,0,0,0,0,'this bug was fixed on trunk when reqScorer was modified to no longer be set to null\r\n\r\nthis patch is against 5.0, where ReqScorer can hit NPE if cost is called after nextDoc\r\n\r\npatch includes test and fix to call cost before nextDoc'
0,0,0,1,0,'Probably not that much slower? I think that\'s a good idea!\r\n\r\nBut I think we can explore this after committing? There are other things we can try too (eg collapse skip list into shared int[]: I think this one may give a perf gain, collapse positions, etc.).'
0,0,0,0,0,'I don\'t think the patch makes these factories any worse.'
0,0,0,0,0,'whew, made it thru the rest. all tests pass.'
0,0,0,0,0,'\r\n+1 to commit Robert\'s patch to trunk and iterate there.'
0,0,0,0,0,'I would like this change  to make it back to the stable branch, so I\'ll keep this issue open until everyone is happy with it. '
0,0,0,0,0,'But in general we should try to split up issues for these factories into steps: its not easy to make these sweeping changes unless you either have lots of time or are ok wi'
0,0,0,0,0,'I haven\'t looked into why this is, but yeah i would definitely prefer if this was a followup issue actually.\r\n\r\n'
0,0,0,0,0,'Solr uses a different joining implementation. '
0,0,0,0,0,'If you want to use the Lucene joining implementation you could wrap this in a Solr QParserPlugin extension.'
0,0,0,0,0,'I committed this to trunk. '
0,0,0,0,0,'I\'ll commit this one to trunk. '
0,0,0,0,0,'\r\nI don\'t like this either: but it would require more heavy duty stuff in solr to fix that. '
0,0,0,0,0,'I\'ve begun the \"horrible slave work\" part  !'
0,0,0,0,0,'Hope to have a patch soon.'
0,0,0,0,0,'I added some helpers to BaseTokenStreamTestCase and fixed TestArabicFilters.java as a prototype...\r\n\r\n'
0,0,0,0,0,'.\r\n\r\nThe challenge in fixing the ResourceLoader stuff is in SolrResourceLoader. when it loads these \"aware\" classes it only adds them to a list , and then at the end calls inform on everything in the list. I haven\'t looked into why this is,'
0,1,0,0,0,'I benchmarked it using IndexAndSearchOpenStreetMaps by temporarily using DocIdSetBuilder instead of MatchingPoints (I did not use luceneutil since its numeric range queries match too many docs). The QPS went from 33.4 (old DocIdSetBuilder.add) to 35.0 with this patch.\r\n'
0,0,0,0,0,'We can merge back to stable when we are happy.\r\n'
0,0,0,0,0,'In my case it happened when testing a re-ranking capability.\r\n'
0,0,0,0,0,'Will try to write Solr counterpart.'
0,0,0,0,0,'Commit 1649893 from [~tomasflobbe] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1649893] ]\r\n\r\n[-LUCENE-6149-|https://issues.apache.org/jira/browse/LUCENE-6149]: Infix suggesters\' highlighting and allTermsRequired can be set at the constructor for non-contextual lookup'
0,0,0,0,0,'* Why TFIDFSimilarity.IDFStats#value is set to IDF square?\r\n '
0,0,0,1,0,'moved the .cost() calls up before any .nextDoc()s. I\'ll commit shortly.\r\n'
0,1,0,0,0,'+1 good catch! '
0,0,0,0,0,'Commit 1650132 from [~tomasflobbe] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1650132] ]\r\n\r\n[-LUCENE-6149-|https://issues.apache.org/jira/browse/LUCENE-6149]: Fixed typo'
0,1,0,0,0,'Thanks jane, patch looks great!'
1,0,0,0,0,'\r\nhell, why bother with the sysprop? .. lets just commit the package-list files for all third party libs we use into dev-tools and completely eliminate the need for net when building javadocs.'
1,0,0,0,0,'\r\nYea, I like having them listed because it makes it easier to go back and look at them and decide which ones to add.\r\n\r\n'
0,1,0,0,0,'The auxiliary class warnings are the easiest of those, but still enough work that I felt like it should be a separate task.\r\n\r\n'
0,0,0,1,0,'Moved the runWithLowerPermissions to LTC.\r\n\r\nI also fixed the FSDirectory() ctor to do a mich simpler Files.isDirectory() check, because otherwise the basic TestDirectory test fails on Windows (of couse it fails...).'
0,0,0,0,1,'Yet, I think you should push the document iteration etc into DWPT to actually apply the delterm only once to make it really atomic. '
1,0,0,0,0,'But it\'s good to have them listed!\r\n\r\n'
0,0,0,1,0,'After discussion with [~hossman], I added an assume to the runWithRestrictedPermissions, so it cancels test execution if no security manager is available. Running those tests without a security manager makes no sense, because they would assert nothing (because they have all permissions).'
0,1,0,0,0,'nice simple idea! \r\nI like the refactorings into pre/postUpdate - looks much cleaner. '
0,0,0,1,0,'Each of those causes a _lot_ of errors that I\'d like to see eventually followed up on. '
0,0,0,0,0,'Commit 1649955 from [~tomasflobbe] in branch \'dev/branches/branch_5x\'\r\n[ [https://svn.apache.org/r1649955] ]\r\n\r\n[-LUCENE-6149-|https://issues.apache.org/jira/browse/LUCENE-6149]: Infix suggesters\' highlighting and allTermsRequired can be set at the constructor for non-contextual lookup'
0,0,0,0,0,'Thanks Tomás, good to see the patch making into the trunk and branch_5x. '
0,0,0,0,0,Cheers
0,0,0,0,0,'ProThe results in the response were wrongly scored and ranked.\r\nI\'ve never gone bac'
0,0,0,0,0,'ProAttached path includes a failing test case. It ca'
0,0,0,0,0,'and ranked.\r\nI\'ve never gone back to that and while I was testing, starting and restarting, the problem disappeared quite suddenly and I was not able to reproduce it.\r\nI know the information I adde'
0,0,0,0,0,'Lucene only patch. '
0,0,0,0,0,'* Why TFIDFSimilarity.IDFStats#value is need even though we have TFIDFSimilarity.IDFStats.idf.getValue;\r\n * TFIDFSimilarity.TFIDFSimScorer#score returns tf * IDFStats.value whi'
0,0,0,0,0,'It is really hard to decipher what is going on inside the good old TFIDFSimilarity.\r\n'
0,0,0,0,0,'    \r\ncode\r\n * Why query weight has a IDF multiplicand?\r\n * Why TFIDFSimilarit'
0,0,0,0,0,'Which can be reproduced with : ant test -Dtestcase=TestExplain -Dtests.method=testExplainScoreEquality -Dtests.seed=B90C674F754D524 -Dtests.locale=de -Dtests.timezone=Etc/GMT-12 -Dtests.asserts=true -Dtests.file.encoding=UTF-8 \r\nHowever, *testRajeshData* method fails more frequently.'
1,0,0,0,0,'Here is a patch.'
0,0,0,0,0,'Which can be reproduced with : ant test -Dtestcase=TestExplain -Dtests.method=testExplainScoreEquality -Dtests.seed=B90C674F754D524 -Dtests.locale=de -Dtests.timezone=Etc/GMT-12 -Dtests.asserts=true -Dtests.file.encoding=UTF-8 \r\nHowever, *testRajeshData* method fails more frequently.'
1,0,0,0,0,'The fix would be to add a javadocs comment to IW.unlock, that this method only works and is a hack just for SimpleFSLockFactory. The existence of a lock file in native fs lock is not the presence of a lock. The file is just a helper, to have a lock \"namespace\". Even if the file exists, the lock may not be obtained.'
0,0,0,1,0,'In Java 9 this fails more often, so be sure to always null out static non-final fields that point to \"more complex\" and internal Java objects.'
0,0,0,0,1,'the big question (I put it in a nocommit to his test case), is if formatter classes should really have to deal with these cases. '
0,0,0,1,0,'Committed and backported the lambdas to Java 7 in branch_5x.'
1,0,0,0,0,'I have no idea why a javadoc jar would be depended on - it\'s definitely not intentional - my first guess is that there is some form of transitive dependency resolution happening - again, unintentionally.\r\n\r\n'
1,0,0,0,0,'Attached is just a combined patch of Manuel\'s 2 patches.\r\n'
0,1,0,0,0,'+1 '
0,0,0,1,0,'+1, I like how simple the implementation is, just tracking how many times each dim was split \"above\" us.\r\n '
1,0,0,0,0,'Also, we can keep release() and impl in NativeFSLock to first obtain if it does not hold the lock.'
0,0,0,0,0,'Fixed. '
0,0,0,0,0,'Commit 1650134 from [~tomasflobbe] in branch \'dev/branches/branch_5x\'\r\n[ [https://svn.apache.org/r1650134] ]\r\n\r\n[-LUCENE-6149-|https://issues.apache.org/jira/browse/LUCENE-6149]: Fixed typo'
1,0,0,0,0,'Please find attached another test case. It is sort of bad luck to run into this in a real use case but it actually happened to me.'
0,1,0,0,0,+1
0,0,0,0,1,'Another fun one is if all indexed points are equidistant from an origin. I\'ve wondered whether cells should be \"shrink wrapped\" during indexing to handle this one...'
0,0,0,0,1,'Are you trying to address the adersarial case of indexing e.g. a narrow sliver of points?'
0,0,0,0,0,'[~dweiss], [~thetaphi] - any idea what i did to causes this weird AccessControlException in TestSimpleExplanations?'
0,0,0,0,0,'OK if I put back BQ\'s bulkScorer, but then use new IndexSearcher instead of newSearcher the first seed above gets further along: the scores agree before/after forceMerge, but the explanation is still incorrect...'
0,0,0,0,0,'ns of Path). So be sure to null all static fields on afte'
0,0,0,0,0,'noformat\r\nWith this perplexing seed  the Randomized IndexWriterConfig manages to pass all checks in the test – getting scores consistent with the Explanation both before and after the forceMerge – but the default IndexWriterConfig still demonstrates the pr'
0,0,0,0,0,'noformat\r\n   [junit4] <JUnit4> says hallo! Master seed: 29D39C39B6047864:5BA9313A359A1831\r\n   [junit4] Executing 1 suite with 1 JVM.\r\n   [junit4] \r\n   [junit4] Started J0 PID.\r\n   [junit4] Suite: org.apache.lucene.search.TestBooleanScoreConsistency\r\n   [junit4] OK      6.16s | TestBooleanScoreConsistency.testScoresWithRandomIWC\r\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBooleanScoreConsistency -Dtests.method=testScoresWithDefaultIWC -Dtests.seed=29D39C39B6047864 -Dtests.slow=true -Dtests.locale=uk-UA -Dtests.timezone=Indian/Comoro -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\r\n   [junit4] FAILURE 1.26s | TestBooleanScoreConsistency.testScoresWithDefaultIWC <<<\r\n   [junit4]    > Throwable #1: java.lang.AssertionError: 7614: score doesn\'t match  expected score for Microsoft Office 365 expected:<1.2357021570205688> but was:<0.41190072894096375>\r\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed\r\n   [junit4]    > \tat org.apache.lucene.search.TestBooleanScoreConsistency.checkScoresAgainstExpected\r\n   [junit4]    > \tat org.apache.lucene.search.TestBooleanScoreConsistency.checkScores\r\n   [junit4]    > \tat org.apache.lucene.search.TestBooleanScoreConsistency.testScoresWithDefaultIWC\r\n   [junit4]    > \tat java.lang.Thread.run\r\n   [junit4]   2> NOTE: test params are: codec=Asserting, sim=RandomSimilarity: , locale=uk-UA, timezone=Indian/Comoro\r\n   [junit4]   2> NOTE: Linux 3.19.0-51-generic amd64/Oracle Corporation 1.8.0_74 /cpus=4,threads=1,free=285815104,total=350748672\r\n   [junit4]   2> NOTE: All tests run in this JVM: [TestBooleanScoreConsistency]\r\n   [junit4] Completed [1/1 ] in 7.73s, 2 tests, 1 failure <<< FAILURES!\r\n   [junit4] \r\n   [junit4] \r\n   [junit4] Tests with failures [seed: 29D39C39B6047864:5BA9313A359A1831]:\r\n   [junit4]   - org.apache.lucene.search.TestBooleanScoreConsistency.testScoresWithDefaultIWC\r\n   [junit4] \r\n   [junit4] \r\n   [junit4] JVM J0:     0.37 ..     8.58 =     8.21s\r\n   [junit4] Execution time total: 8.62 sec.\r\n   [junit4] Tests summary: 1 suite, 2 tests,'
0,0,0,0,0,'A bit more progress ... it looks like somehow BooleanScorer gets confused and is collecting a window with more than one sub-scorer matching, yet failed to tell the top collector to reset back to the fake scorer .'
0,0,0,0,0,'The stack trace is not very helpful to figure out that the \"bad\" guy was the non-nulled writer field.'
0,0,0,0,0,'I added [https://github.com/randomizedtesting/randomizedtesting/issues/227] to make this easier to debug. '
0,0,0,0,0,'IssueThe leak detetctor measures static fields and their heap usage. Unfortunately'
0,0,0,0,0,'Here is what\'s wrong in the test: You have to null the writer field in BaseExplanationTestCase like the searcher in afterClass.'
0,0,0,0,0,'I was reminded of this issue today and pinged [~mikemccand] about it on IRC.\r\n\r\n'
0,0,0,0,0,'s\r\nnoformat\r\nWith this seed, the pre-merge and single-segment indexes in both tests return identical scores for all matching docs, but the Explanations  don\'t match the scores...\r\nnoformat\r\n   [junit4] <JUnit4> says salu'
0,0,1,0,0,'I\'m not sure that the fix is just javadocs ... perhaps the fix should be to add an unlock() method to Lock and impl it in SimpleFSLock by calling release(), but on NativeFSLock to first obtain the lock and if that succeeds, release it. That way, the obtain would fail and we can throw an exception.'
0,0,0,0,0,'Decision\r\n * The Explanations for every document matching this BooleanQuery do *not*change based on if/how the segments in the index have been merged – such that the Explanations can be drastically different then the scores\r\n\r\nCouched that way, and with a request from mike to try and confirm if the bug was dependent on using LuceneTestCase.newIndexWriterConfig  I revamped the previous test class to attempt to more straight forwardly demonstrate the crux of the matter.\r\n\r\nThere are '
0,0,0,0,0,'nfigs.\r\n\r\n\"checkScores\" is the meat of both test methods. It builds up an index using the same \"RajeshData.txt\" and then executes a fixed query against the index, recording the scores of every mathcing document. It then does a forceMerge on the IndexWriter, and re-executes the query comparing the scores of every matching document against the scores from the previous query execution.\r\n\r\nIf the tes'
0,0,0,0,0,'In reviewing the issue myself, to try and describe it to him succinctly, I realized the key bullet points of this bug is:\r\n * It is possible to build an index, _with *no* deleted documents_ such that a particular BooleanQuery returns drastically different scores for some documents depending on if/how the segments in the index have been merged'
1,0,0,0,0,'I think perhaps like a Hadoop input format split, we can define meta-data at the segment level as to where the documents live so that if one is \'splitting\' the index, as is being implemented with HBase, the \'splitter\' can be \'smart\'.'
0,0,0,0,0,'.\r\n\r\nThere are now two test methods: testScoresWithDefaultIWC & testScoresWithRandomIWC – both delegating to the same helpe method for the meat of the test, just using different IndexWriterConfigs.\r\n\r\n\"checkScores\" is the meat of b'
0,0,0,0,0,'\r\nThe problem is caused by the static leak detector . '
0,0,0,0,0,'linking to [-LUCENE-2198-|https://issues.apache.org/jira/browse/LUCENE-2198], when we replace some of these buggy stemfilters with snowball ones, we will need it to implement the protected word support already exposed by the relevant analyzers.'
0,0,0,0,0,'this way we are not taking any functionality away.'
0,0,0,0,0,'i\'d like to work on getting these bugs fixed, but I\'m not sure the best way to proceed.\r\n\r\n'
0,0,0,0,0,'Decisionlooking at the different possibilities i came up with two good options, although maybe there are other ways:\r\n * option 1, deprecate and keep the old broken impls and apis, but depending on Version use the correct ones instead: api and index back compat, but we keep the buggy code and support it for at least some time.\r\n * opti'
1,0,0,0,0,'Patch attached + a couple of unit tests for allTermsRequired=false'
1,0,0,0,0,'the lsat patch you attached only included \"modified\" files, not the \"added\" files from previous patches, so it was missing the new tests. '
0,0,0,1,0,'Removed impl from CommitPoint (which also removed a redundant duplicate \'gen\' member).\r\n\r\n'
0,0,0,1,0,'Bulk close for 5.3.0 release'
0,0,0,0,0,'setting to 3.1, because I would like to make use of simon\'s stopword-handling improvements to tie in the snowball stoplists.\r\n\r\n'
0,0,0,0,0,'\r\n * option 2, deprecate the old apis, but implement it in terms of the correct one: api back compat only, but we drop the buggy code so m'
0,0,0,1,0,'Bulk close for 3.1'
0,1,0,0,0,'I agree with hossman too. I\'m just a javadocs dummy and was doing what I could to stop the 30minute builds.\r\n\r\nI cant figure out this linkoffline (at least with my experiments its confusing)... but this sounds great.'
0,0,0,0,0,'+1 - lets lose it.'
1,0,0,0,0,'In BooleanScorer.scoreWindowSingleScorer, we try to \"reset the scorer that should be used for the general case\" by calling collector.setScorer(fakeScorer).\r\n\r\n'
1,0,0,0,0,'If forceful removal is not allowed, should we throw an exception\r\nnoting that you failed to forcefully remove the lock? Seems like\r\nwe should?'
0,0,0,1,0,'Patch\r\n - Uses componentdef to load the ECJ compiler, so the classloader can be reused like with taskdef'
0,0,0,1,0,'The problem is collector at that point is the singleClauseCollector not the original collector ... '
0,0,1,0,0,'I dont think i like all the duplication/extra tracking in every indexinput impl...\r\n\r\n'
0,0,0,0,0,'At first i though maybe that was intentional on your part, that once you had diagnosed the root cause you had modified an existing test to include some checks for this specific situation so we wouldn\'t need my the crufty TestBooleanScoreConsistency with it\'s hardcoded data that we probably can\'t commit as is anyway – but i couldn\'t find any test modifications you had made to reliably reproduce this problem.\r\n\r\n'
0,0,0,1,0,'I think this attached patch fixes it.'
1,0,0,0,0,'there may be cleanup we want to do to - for now i avoided adding more sys properties for the package-list dirs, but maybe we want them? i dunno. there \'s also some existing instances of the \"<link>\" tag that look totally bogus and broken (see the WTF comments i added) but i didn\'t test what changes if i remove them'
0,1,0,0,0,'I vote for the second option (i.e. not adding another API, but use what\'s there and include a \"first obtain then release\" logic in an \'else\' part in NativeFSLock).'
1,0,0,0,0,'patch for v4.10.3 release'
0,0,1,0,0,'The problem: NativeFSLockFactory is never possible to get the lock and release it. This is only allowed for the code, that holds the lock (its the same like with a sysnchonized mutex using j.l.concurrent.ReentrantLock, only code that holds the lock can free it).'
0,1,0,0,0,'It is baffling :)'
0,0,0,1,0,'OK I found another test bug: you have to use a LogMergePolicy when building the single segment searcher else the docIDs can be jumbled since otherwise merge policies are allowed (and, do!) '
0,0,1,0,0,'I didn\'t see this patch yet, but as I said in [-SOLR-6648-|https://issues.apache.org/jira/browse/SOLR-6648], I think it makes sense to set those defaults in the constructor.'
0,0,0,1,0,'I think this bug is serious enough that we should be sure to get it into 6.1.0 ... '
1,0,0,0,0,'Hmm, testQueries10 changes the similarity of searcher temporarily, but fails to also change singleSegmentSearcher\'s similarity ... when I fix that, then this seed passes.\r\n\r\n'
1,0,0,0,0,'Here\'s one way we could improve it. There are probably other alternatives that might be better, too.\r\n\r\nI only fixed one factory as its better to decide this before going thru all of them.'
0,0,0,0,0,'Commit 1556483 from [~jpountz] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1556483] ]\r\n\r\n[-LUCENE-5361-|https://issues.apache.org/jira/browse/LUCENE-5361]: Fixed handling of query boosts in FastVectorHighlighter.'
1,0,0,0,0,'here\'s a rough patch showing how the link offline stuff works. (as far as i understand it anyway)'
0,0,0,0,0,'make StemmerOverrideFilter final, and hardcode it as case-sensitive '
0,0,0,0,0,'alternativeOne thing I would like to fix: these packages have stoplists, but our snowball implementation is missing the stoplists from the s'
0,0,0,1,0,'I think we shouldn\'t send \"out of bounds\" matches to the formatter? '
0,1,0,0,0,'Because all it can do is bounds check and skip it?\r\n\r\nI think maybe we also shouldn\'t even send the passage if it was \"truncated\", even if some matches were before the truncation?'
0,0,0,1,0,'I also confirmed that if I revert the fix in BooleanScorer.java that TestBoolean2does fail on some seeds ... thanks [~hossman_lucene@fucit.org]!'
1,0,0,0,0,' I added some more tests to the new code and renamed the field highlighting->highlight'
0,1,0,0,0,+1
0,0,0,0,1,'We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption... it\'s nice not to have false scares even if the app is doing something it shouldn\'t...'
1,0,0,0,0,'to merge segments out of order.\r\n\r\n'
0,1,0,0,0,'The patch looks good.'
0,0,0,1,0,'Here\'s Hoss\'s last patch with those two test bugs fixed ... seems to survive beasting of TestBoolean2 for a while, and Lucene\'s core tests passed once.'
0,0,0,1,0,'Except, on the one seed I was debugging before, the original index was a single segment, so no matter the merge policy, the docID order would not change ...\r\n\r\n'
0,1,0,0,0,'Thanks [~hossman], I can reproduce that failure with your patch ... '
0,1,0,0,0,'+1 to the patch\r\n'
0,1,0,0,0,'Maybe this also explains the original \"force merge changed results\" issue? '
0,1,0,0,0,'+1 on exception.'
1,0,0,0,0,'Reopening to fix the fixVersion.'
1,0,0,0,0,'OK here\'s a patch. the cause of the bug is that we only know startOffsets are always increasing (the algorithm relies on this, and merges them across terms).'
0,0,0,0,0,'Commit 1650595 from [~mikemccand] in branch \'dev/trunk\'\r\n[ [https://svn.apache.org/r1650595] ]\r\n\r\n[-LUCENE-6119-|https://issues.apache.org/jira/browse/LUCENE-6119]: must check merge for abort even when we are not rate limiting; don\'t wrap rate limiter when doing addIndexes ; don\'t leak file handle when wrapping'
0,1,0,0,0,'Thanks [~jpountz] and [~hossman], I\'ll commit the last patch.'
0,1,0,0,0,'\r\n+1'
0,0,0,0,0,'+1 also on having separate \"default analyzer\" classes for each language.'
1,0,0,0,0,'p=lucene-solr.git;h=707bcc9b] ]\r\n\r\n[-LUCENE-7132-|https://issues.apache.org/jira/browse/LUCENE-7132]: BooleanQuery sometimes assigned the wrong score when ranges of documents had only one clause matching while other ranges had more than one clause matchng\r\n\r\n(Cherry-picked from commit 5dfaf0392fcd3b7e4b529dce0cd1035b766880a7)'
0,1,0,0,0,'+1, patch looks good.'
1,0,0,0,0,'delete the old stemmers and in the analyzers replace them with SnowballStemFilters (it does fix the bug, as they now become correct)'
1,0,0,0,0,'p=lucene-solr.git;h=9f513d5] ]\r\n\r\n[-LUCENE-7132-|https://issues.apache.org/jira/browse/LUCENE-7132]: BooleanQuery sometimes assigned the wrong score when ranges of documents had only one clause matching while other ranges had more than one clause matchng'
0,0,0,1,0,'\r\n * hook into build.xml as optional targets that can be run if you have the neccessary libs installed, don\'t fail the build just generate the XML report files\r\n * put the needed libs on builds.apache.org, and hook it into the jenkins nightly target, and configure jenkins to display it\'s pretty version of the xml reports so people can at least see what\'s going on.\r\n * start adding/tweaking custom rule sets in dev-tools to eliminate rules we don\'t care about, add rules we want that don\'t exist, or change the severity of rules we think are more/less important\r\n * tweak the build.xml to fail if anything above some arbitrary severity is tripped\r\n * worry about maven'
0,0,0,0,0,'will commit in a day or two if no one objects.'
0,0,0,0,0,'Thanks [~jpountz], here\'s a new patch with those fixes.'
1,0,0,0,0,' keep the buggy code and behavior with version'
0,0,0,0,0,'Committed revision 1028779 to 3.0.x\r\nCommitted revision 1028782 to 2.9.x'
0,0,0,0,0,'+1, gogogo'
0,0,0,0,0,'reopening for possible 2.9.4/3.0.3 backport.\r\n\r\n'
1,0,0,0,0,'p=lucene-solr.git;h=c8570ed] ]\r\n\r\n[-LUCENE-7132-|https://issues.apache.org/jira/browse/LUCENE-7132]: BooleanQuery sometimes assigned the wrong score when ranges of documents had only one clause matching while other ranges had more than one clause matchng'
0,0,0,0,0,'i will commit this monster soon if no one objects.'
0,0,0,0,0,''
0,0,0,0,0,'I committed some documentation that if possible, its better to use the snowball versions for Dutch and French.\r\n\r\n'
0,0,0,0,0,'patch addressing Simon\'s comments, and also fixing javadoc warnings.\r\n\r\n'
0,1,0,0,0,'I think we should go with this patch so we aren\'t downloading this junk anymore, it causes false build failures, the only trick I can think of is how to ensure lucene source releases build by themself without reaching back to dev-tools (i think this is broken on trunk at the moment, but it does work on 3.x right now)'
0,0,0,0,0,'Commit 1650594 from [~mikemccand] in branch \'dev/branches/branch_5x\'\r\n[ [https://svn.apache.org/r1650594] ]\r\n\r\n[-LUCENE-6119-|https://issues.apache.org/jira/browse/LUCENE-6119]: must check merge for abort even when we are not rate limiting; don\'t wrap rate limiter when doing addIndexes ; don\'t leak file handle when wrapping'
0,0,0,0,0,'I think auto-tuning merge thread count would be a great addition!'
1,0,0,0,0,'I propose switching to the oracle.com'
0,1,0,0,0,'This would be lots simpler than trying to figure out dev-tools etc., assuming that this link is indeed reliable.'
1,0,0,0,0,'patch updated w.r.t. trunk 05/01/15'
1,0,0,0,0,'Minor changes to the test and made the new fields private. I\'ll commit this soon'
0,1,0,0,0,+1
1,0,0,0,0,'I know the information I added is almost null, hopefully we can get more evidence from other people !\r\n\r\n'
1,0,0,0,0,'Improved patch, thank you Mike  '
0,0,0,0,1,'Wait, couldn\'t we fix passageQueue.offer(current) to not offer it if current.endOffset == contentLength?'
0,1,0,0,0,'Patch looks good! '
0,1,0,0,0,'For me the latest patch looks good as it no longer enforces to unlock (which is not possible with NativeFSLock) and throws Exception. '
1,0,0,0,0,'updated patch: tests in analysis/common are done. I\'ll look at the other analysis modules and solr tomorrow.'
0,1,0,0,0,+1
0,1,0,0,0,'Not only is this better because it uses far less RAM, it\'s also more\r\ngeneral than the 2-pass collector in that the app is free to\r\narbitrarily set the groups by indexing the right doc blocks. '
1,0,0,0,0,'Patch.\r\n\r\n'
0,0,0,0,0,'\r\nNo – it was already fixed on 3x , probably because we branched 3x off after this was committed.'
0,0,0,1,0,'Committed revision 891205 '
0,0,0,1,0,'Bulk close after 5.0 release.'
0,0,0,1,0,'Well the passage may not be truncated: for example depending on the analyzer (e.g. ngrams or something), it could just be that the term \"spans sentences\".'
0,0,0,1,0,'\r\nAhh good point – it\'s wrong just passing that delTerm down N times, too. I\'ll fix.\r\n'
0,0,0,1,0,'If you change on a new IW session... they will be merged according to\r\nthe normal type promotion rules of doc values right?\r\n\r\n'
0,1,0,0,0,'Performance is good ~ 25-28\% faster than the two-pass collector with\r\ncaching.'
0,0,0,0,1,'How do I add these to the forbidden API list?'
1,0,0,0,0,'Maybe similarly interleave payload/positions packets?\r\n\r\n'
0,0,0,1,0,'Done.\r\n'
0,1,0,0,0,'+1 let\'s try this and see if it is indeed reliable.'
0,0,1,0,0,'The bug Manuel found here is \"matching term spanning the content boundary\". so lets call this \"truncated term\". This patch fixes this so formatter doesnt have to deal with it, and there is no AIOOBE or strange checks in the formatter.'
0,0,0,1,0,'I am currently browsing to find a good reference.'
0,0,1,0,0,'The test you write is for different behavior: it saying, if the passage itself spans the content boundary, don\'t present it to the formatter at all. But, this is sorta a different issue, its already handled here today by Math.min and the formatter never has to deal with it:'
0,0,1,0,0,'Well the patch doesnt attempt to change anything about the breakiterator logic: so your test is \"valid\" but testing something different.'
0,0,0,1,0,' * BrazilianStemmer looks suspiciously like the Snowball Portuguese algorithm except with different diacritics handling, need to look further\r\n * ChineseAnalyzer (the one that does individual chinese characters) does essentially what StandardAnalyzer does with chinese text, I do not see any other features'
0,0,0,0,0,'Slightly improved generics in the patch to make the helper method more universal .'
0,0,0,0,1,'\r\nThen at least the first seed above passes...'
0,0,0,0,0,'plz review the latest patch I added?'
0,0,0,1,0,'# only download jars for pmd (not source and javadocs, they are massive)'
0,1,0,0,0,+1
1,0,0,0,0,'One thing I would like to fix: these packages have stoplists, but our snowball implementation is missing the stoplists from the snowball dist.'
0,0,0,0,0,'Will post updated patch.'
0,0,0,0,0,'Commit 1515994 from [~mikemccand] in branch \'dev/branches/branch_4x\'\r\n[ [https://svn.apache.org/r1515994] ]\r\n\r\n[-LUCENE-3849-|https://issues.apache.org/jira/browse/LUCENE-3849]: fix some more test only TokenStreams'
0,0,0,0,0,'Should I backport?'
0,0,0,1,0,'I think as this stands it\'s useful and we should commit it and iterate later?\r\n\r\nI.e. devs can now ru'
0,0,0,0,1,'I use pmd as part of my pre-commit process, but using custom python wrapper; this would already be a step up for me!'
0,0,0,1,0,'Closed after release.'
1,0,0,0,0,'Just one other idea:\r\n - We already have JAVA_HOME set (direct or implicitely set by ANT)\r\n - The Javadocs are always at same location in $JAVA_HOME\r\n\r\nCould we not use this to point to the package list (at least fpr the JDK part). I don\'t like the hardcoded package list.'
0,1,0,0,0,'+1 to rolling our own in any case.'
1,0,0,0,0,'Committed changes to the spatial3d module for this purpose. '
1,0,0,0,0,'I don\'t like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?\r\n\r\nI think AbstractAnalysisFactory could use additional param parsing methods:\r\n * get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods. Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?\r\n * getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?\r\n * getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.\r\n * getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?\r\n\r\nA few nits:\r\n * TestMappingCharFilterFactory\'s factory could switch to being instantiated using the charFilterFactory() method\r\n * EdgeNgramTokenizerFactory\'s gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer\r\n * LimitTokenCountFilterFactory\'s maxTokenCount param should be required; this is a pre-existing problem though\r\n * PatternTokenizerFactory\'s group param should use the getInt() method with a default of -1.\r\n'
1,0,0,0,0,'There was a bug in my patch: I added another unit test for this!\r\n\r\nI think its ready.'
0,0,0,1,0,'Also, I now throw UOE from get/setBoost, stating that you should do so against the child query instead.'
0,0,0,1,0,'Usually yes, however at this stage we just released 3.6 .\r\n\r\nSo currently we have not yet cut a branch_4x for stable 4.0 and are only working on trunk.\r\n\r\n'
0,0,0,0,0,'\r\nproI think we allow this for OR as well anyway, so it would be ambiguou'
0,0,0,0,0,'I can\'t see a reason why this would be controversial, so I\'ll start with a patch for that.'
0,0,0,1,0,'\r\n\r\nI think we should just add this to our forbidden API list, then see what fails (because it\'s using these APIs), and correct them...\r\n\r\n \r\n\r\n'
0,1,0,0,0,'+1 to all of this!'
0,1,0,0,0,'+1\r\n\r\nTricky!'
0,0,0,1,0,'Patch, adding equals and hashCode and clone to BlockJoinQuery. '
1,0,0,0,0,'this patch does the following:\r\n * deprecates RussianTokenizer, RussianStemmer, RussianStemFilter, DutchStemmer, DutchStemFilter, FrenchStemmer, FrenchStemFilter\r\n * use snowball in the above analyzers instead, depending upon version.\r\n * doesn\'t deprecate germanstemmer, but uses snowball instead (which is maintained and relevance-tested and supports things like u+umlaut = ue, etc). the old stemmer is kept because it is a different algorithm (alternate).\r\n * the dutchstemmer had \'dictionary based stemming override\' support, so to implement this, add StemmerOverrideFilter which does this in a generic way with KeywordAttribute\r\n * adds KeywordAttribute support to SnowballFilter\r\n * deprecates SnowballAnalyzer in favor of language-specific analyzers.\r\n * adds Romanian and Turkish stopword lists, since snowball is missing them.\r\n * implements language-specific analyzers in place of all the ones snowball tried to do at once before.'
1,0,0,0,0,'updated patch:\r\n * implement StemmerOverrideFilter with CharArrayMap\r\n * fix casing problems in french and dutch: they did not call lowercasefilter, instead relying upon the stemmer to lowercase things. this causes inconsistencies with stopwords, dictionary-based stemming, exclusion sets, you name it. the old broken behavior is preserved depending on Version\r\n * add missing standardfilter to greek (depending on Version).'
0,0,0,0,1,'I think better integration to the build / failing if new warnings show up, etc., can come later?\r\n\r\n'
0,0,0,0,0,'agreed, I\'ve reopened and will backport it.'
0,0,0,0,0,'proBy the way this patch looks g'
0,0,0,0,0,'alternativeSupport D-m-q in th'
0,0,0,0,0,'At the risk of drifting off topic ...\r\n\r\n'
0,0,0,0,0,'Indeed I don\'t quite know what sort of syntax would fly for the tieBreaker, I\'m off to study the jj file for the classic parser and see if anything analogous presents itself.\r\n\r\n'
0,0,0,0,0,'son! I plan to commit this in a'
0,0,0,0,0,'I\'m +1 to adding syntax to this to the classic/flexible/qps as well.\r\n\r\n'
0,0,0,0,0,'I think it makes more sense, myself, but if others see value in continuing the line in here I\'m game.'
0,0,0,0,0,'But I like your idea of adding it to the xml one first. '
0,0,0,0,0,'FWIW, that\'s ||, not |, but I still don\'t want to burn | on disjunction.'
0,0,0,0,0,'Maybe its an easier iteration to then boil down the best\r\nsyntax for the other ones .'
0,0,0,0,0,'Lots of things with no javadoc, with the experimental tag, etc. '
0,0,0,0,0,'From where I sit, 4.0 looks a long way from a release. '
0,0,0,0,0,'Please see [https://issues.apache.org/jira/browse/LUCENE-4012] for an alternative to adding syntax to any of the existing end-user-facing parsers. '
0,0,0,0,0,'Am I underestimating the speed at which the entire lodge of beavers pulls a major release together?'
0,0,0,0,0,'add the boost param I forgot the first time.'
0,0,0,0,0,'So currently we have not yet cut a branch_4x for stable 4.0 and are only working on trunk.\r\n\r\n'
0,0,0,0,0,'Usually yes, however at this stage we just released 3.6 .\r\n\r\n'
0,0,0,0,0,'I note that the xml parser doesn\'t do DisjunctionMaxQuery. '
0,0,0,0,0,'Commit 1709780 from [~teofili] in branch \'dev/branches/branch_5x\'\r\n[ [https://svn.apache.org/r1709780] ]\r\n\r\n[-LUCENE-6821-|https://issues.apache.org/jira/browse/LUCENE-6821] - TermQuery\'s constructors should clone the incoming term '
0,0,0,0,0,'The patch of 14 October LGTM, and tests pass here.'
0,0,0,0,0,'\r\nOK thanks for confirming – I\'ll backport to 3.0.x as well.\r\n\r\n'
0,0,0,0,1,'If we have both, matches that do and matches that don\'t span the content boundary the formatter is asked to highlight the spanning match.\r\nPlease find attached additional tests and a possible fix for this.'
0,0,0,0,0,'backported to branch 5.x'
1,0,0,0,0,'The usage of mixed CharArraySet and a conventional dictionary map is buggy. The CAS is using a different contains algo and lowercasing, you can get a hit in the CAS but the Map returns null -> NPE. I would not use a CAS for the beginning and always cast to String for now and I will open an issue for extending CAS to be a CharArrayMap'
1,0,0,0,0,' StemOverrideFilter should be final or have final incrementToken()'
0,1,0,0,0,'+1 on the current patch'
0,0,0,0,0,'I will run another round of testing and inspections and commit the latest patch if no issues come up.'
0,0,0,0,0,'I had a look at the core code for the use of the TermQuery constructors, and I agree that it would be better to do the clone in the constructor.\r\n'
0,0,0,1,0,'To explain: I had totally forgotten about this little loop on tf within the passage (i had removed this optimization in [LUCENE-4909|https://issues.apache.org/jira/browse/LUCENE-4909], which didnt turn out to work that great, so wasn\'t committed).\r\n\r\nWe might at some point want to still just remove the optimization just based on the reason that it makes this thing more complicated, it was just intended to speed up the worst case (where someone has very common stopwords and stuff like that).'
0,0,0,1,0,'No. Instead i will force it to be case sensitive to ignore this, it is stilly to have dutch stem filter be horribly slow because of some theoretical case like this.'
0,0,0,1,0,'ok, thanks, will fix this now.'
0,0,0,0,0,' problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupte'
0,0,0,0,0,' problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.Decision'
0,0,1,0,0,'Map<?,? extends String> does not make sense as String is final. Map<?,String> and the same for CharArrayMap<String>.'
0,0,0,1,0,'For 3.2 it would be a minor backwards break, but code using SegmentInfos whould still compile. '
0,0,0,1,0,'29x version of this patch.'
0,0,0,1,0,'updated patch for the generics policeman'
0,0,0,0,0,'Do we want to backport it to 5.x? '
0,0,0,0,0,'The explain debug ranking was the correct one, with correct and expected scores.\r\n'
1,0,0,0,0,'Another iter of this patch; I think it\'s ready to commit. I commented out BooleanScorer\'s handling of required=true since we never use it (but left TODOs in there to re-enable this code if/when we go back to sending required clauses to BooleanScorer). I also added CHANGES entry.'
0,0,0,1,0,'While doing a final review, I noticed that you mistakenly modified the boost of the original query instead of the clone. I took the liberty to fix it before committing'
1,0,0,0,0,'Fix the issue by pushing boosts from parent queries to child queries when the parent queries are flattened. I clone the child queries before setting their boost so I don\'t break anything that expects them unchanged. '
0,0,1,0,0,'I\'m not super happy that I have to clone the queries but it seemed like the simplest solution.'
0,0,0,1,0,'\r\nCommitted, thanks!'
0,0,0,0,0,'One way of doing it would be adding the scorers Weight as a protected final member since Weight already has a #getQuery method we can easily access it or throw an UnsupportedOperationException if the weight is null .\r\n\r\nSince the most of the scorers know their Weight anyway and would need to call the visitor we can also factor it out.\r\n'
0,0,0,1,0,'Committed revision 907125. Thanks to the reviews and help Simon/Uwe!'
0,0,0,0,0,' 99.66\%!\r\nThe problem is this stemmer incorrectly creates some uppercase stems from lowercase words. examples:\r\nxviii -> xviI expected: xvii\r\nvouer -> voU expected: vou\r\ntranquille -> tranqUill expected:'
0,1,0,0,0,'Thanks Nik, your fix looks good!'
0,1,0,0,0,' I don\'t think cloning the queries is an issue, it happens all the time when doing rewrites, and it\'s definitely better than modifying those queries in-place.'
0,0,0,1,0,'I\'m attaching a unified patch that inlcudes all the previous test code along with mike\'s fixes ... but obviously we still need to improve this test to not include this user data (see nocommit) ... i\'ll work on that.'
0,1,0,0,0,'see the original missing values issue for discussion.'
0,0,0,0,0,'Here is a patch.\r\n\r\n'
0,0,0,0,0,'DecisionDo we have a benchmark that could be used to validate this change? I just checked o'
0,0,0,0,0,'s change? I just checked out luceneutil but it only seems to have tasks for queries, not sorting?Decision'
0,0,0,0,0,'The null check, Robert mentions, was done like this to optimize missing values.\r\n\r\n'
0,0,0,1,0,'\r\n * Added a test case to TestIndexReader'
1,0,0,0,0,'Attached patch.\r\n\r\nThere were two things to fix. First, I added Scorer.freq() so that\r\nyou could get the freq of the current match. Second, I added a way to\r\nvisit all sub-scorers for a given Scorer. '
0,1,0,0,0,'Shai, looks good to me!\r\n\r\n'
0,0,1,0,0,'We should really think through the consequences of this though.\r\n\r\nIf core features of lucene become implemented in a way that they rely upon these sequential docids, we then lock ourselves out of future optimizations such as reordering docids for optimal index compression. '
1,0,0,0,0,'Patch of 3 Oct 2015. This\r\n - adds a call to BytesRef.deepCopyOf in the Term constructor,\r\n - removes such calls where the Term constructor is used, and\r\n - documents that the result of Term.bytes() should not be modified.'
0,0,0,0,0,'If no document has values, then they will all return the missing value?'
0,0,0,1,0,'I don\'t think a jdoc warning is really required for this... but I\'m fine if you want to add one?\r\n\r\n'
0,0,0,0,0,'I dont understand how the MatchNoBits case is safe.'
1,0,0,0,0,'Warnings for -Xlint:-auxiliaryclass -Xlint:-deprecation -Xlint:-rawtypes -Xlint:-serial -Xlint:-unchecked are all disabled. '
0,0,0,0,0,'I think the current null check is effective? '
0,0,0,0,0,'i benchmarked the first version of the patch with the little benchmark in luceneutil, but saw no improvement.\r\n\r\n'
0,0,0,0,0,'You set the long missingValue to 0L and the specialization NumericDocValuesInstance returns missingValue - which is exactly the same as if the missingValue was null in old code .\r\n\r\n'
0,0,0,0,0,'using. Especially because the generics enfore a real type which is removed here. At least make'
0,0,0,0,0,'\r\nIt is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues. and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.'
0,0,0,0,0,'I don\'t like the crazy API around the missingValue declared as \"long\" in the abstract base class. '
0,0,0,0,0,'\r\nIn fact you are removing the null check, which is the extra branch to check for missing values - just look at the old code . '
0,0,0,0,0,'ved here. At least make the constructor of the abstract base class hidden - or hide the whole abstract base class . I am not sure if it needs to be public at al'
0,0,0,0,0,'DecisionThis is very confusing. Especially'
0,0,0,0,0,'Prothe null check has to be done anyways by the jvm, so removing it brings nothing.\r\n\r\nsee the origina'
0,0,0,0,0,'or). I am not sure if it needs to be public at all.\r\n\r\nIf this does not slow down, go for it!\r\n\r\nHow to '
0,0,0,0,0,'lue. So there is no need to check the missing values in the comparator.\r\n\r\nI\'m surprised that you think '
0,0,0,0,0,'t all.\r\n\r\nIf this does not slow down, go for it!\r\n\r\nHow to handle that in Lucen'
0,0,0,0,0,'It was done exactly like this to not slow down - hotspot can optimize that away, if it finds out that it is null - it does this very fast. '
0,0,0,0,0,'I didn\'t see it happening with the currently assembly generated.\r\n\r\n'
0,0,0,0,0,'Hi Adrien,\r\n\r\nit is somehow a specialization - on the docvalues instance. '
0,0,0,0,1,'e 4.x? The API still uses FieldCache.DEFAULT there and the order of calls for getDocsWithField is important.Issue'
0,0,0,0,0,'Decisionit has to be handled anyway. And personally'
0,0,0,0,0,'The reason why I came up with this issue is that I\'m writing a selector in order to sort based on the values of a block of documents. '
0,0,0,0,0,'or it!\r\n\r\nHow to handle that in Lucene 4.x? The API st'
0,0,0,0,0,'\r\nDoesn\'t it happen already if you have two fields that have different compression?'
0,0,0,0,0,'Still it seems like apps like Solr might be generating these disjunctions with minShouldmatch = optional.size and we should handle them as conjunctions always.'
0,0,0,0,0,'DecisionI am not 100\% sure if the current code does the right thing - but if tests pass I am fine.\r\n\r\nI don\'t like the crazy API around th'
0,0,0,0,0,'You may be right - you can also see it as simplifcication. '
0,0,0,0,0,'ProTo make it work efficiently I need to write a NumericDocValues instance that already returns the missing value when there are either no child documents in the block or if none of them have a value. So there is no need'
0,0,0,0,0,'looks good to me while you could add that minShouldMatch only applies to optional scorers so nobody needs to hunt that down again'
0,0,0,0,0,'.\r\n\r\nIn fact your patch would only work in Lucene trunk, in 4.x this cannot be done like that.Pro'
0,0,0,0,0,'I dont think there is specialization needed. '
0,0,0,0,0,'way simpler though. I think this is close to commit.'
0,0,0,0,0,'.\r\n\r\nI\'m surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing v'
0,0,0,0,0,' simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially'
0,1,0,0,0,'nce. And it makes it easier  to write selectors.Pro'
0,0,0,0,0,'way. And personally i would be wary of overspecialization here...Pro'
0,0,0,0,0,'Another iteration with Weight as a protected member or Scorer. '
0,0,0,0,0,'hat. We compared the two implementations - without missing values and the new one with missing values - and they were exactly the same speed. The same that Robert discovere'
0,0,0,0,0,'Uwe, please read the issue again: the goal was not to remove the null check, but the check for missing values.\r\n\r\n'
0,0,0,0,0,'\r\nYour code does the right thing. '
0,0,0,0,0,'cool nice little optimization! '
0,0,0,1,0,'Patch against 3x:\r\n * Changes listCommits() signature to return a List<IndexCommit>\r\n * DirReader.listCommits() sorts the list in the end'
0,1,0,0,0,'One thing: I would remove the variable assignment in the compareXxx methods and make them one-liners.'
1,0,0,0,0,'\r\n\r\nNew patch, I think it\'s ready to commit but it could use some healthy reviewing...\r\n\r\nI fang\'d up TestNRThreads to add/update doc blocks and verify the docs in each block remain adjacent, and also added a couple other test cases to make sure we test non-aborting exceptions when adding a doc block.'
1,0,0,0,0,'Could we not declare both methods in 3.x and deprecate the old one? '
1,0,0,0,0,'I wonder if we really need to pass the BooleanClause.Occur to the visit method of if we rather defined a visit method for each boolean relationship. We could make it an abstract class and if you are not interested in Prohibited or Optional scorers you can simply leave the method empty. JIT compiler might be able to optimize this method calls away. if we go this way I\'d make VisitSubScorer abstract\r\n\r\n\r\ncode:java\r\npublic '
0,0,0,0,0,'Updated patch... I tried to speed up BS2 some more here in this patch:\r\n'
0,0,0,0,0,'r though. I think this is close '
0,0,0,0,1,'I\'m ok with the issue if we see a performance increase, just not seeing it.'
0,0,0,0,0,'I removed reference to the SOLR issue, i thought for some reason it was a performance problem, but that was just faulty brain cells .\r\n\r\n'
0,0,0,1,0,'Fixed.\r\n\r\nOnly hitch was in 3.x the APIs take Collection<Document> (vs trunk\'s Iterable<Document>). If we backport DWPT we can put it back to Iterable<Document>...'
0,0,0,1,0,'The problem leading to this quiet heavy change was that I wanted to call a specific function for each Boolean.Occur relationship. That turned out to be ugly since I had to either add a switch / case statement to each of the visitSubScorers methods or add a visitSubScorers method for each relationship. I didn\'t like that at all so I moved forward to specify a callback for each relationship. \r\n'
0,0,0,0,0,'\r\nIt would also be nice to run the \"eg.filter.tasks\" but these are currently broken in luceneutil.'
0,0,0,0,0,'Bulk move 4.4 issues to 4.5 and 5.0'
0,0,0,0,0,'Move issue to Lucene 4.9.'
0,0,0,0,0,'If we have a need somewhere else we can open it up.\r\n '
0,0,0,0,0,'Commit 1596607 from [~rcmuir] in branch \'dev/branches/branch_4x\'\r\n[ [https://svn.apache.org/r1596607] ]\r\n\r\n[-LUCENE-4236-|https://issues.apache.org/jira/browse/LUCENE-4236]: add a new test for crazy corner cases of coord handling'
0,0,0,0,1,'Due to backwards tests failure, I kept the method signature as returning Collection, and only documented the new behavior.\r\n'
0,0,0,1,0,'Bulk closing for 3.2 '
0,0,0,1,0,'Committed revision 1034080 + 1034144 (3x). '
0,0,0,1,0,' 99.66\%!\r\nThe problem is this stemmer incorrectly creates some uppercase stems from lowercase words. examples:\r\nxviii -> xviI expected: xvii\r\nvouer -> voU expected: vou\r\ntranquille -> tranqUill expected: tranquillDecision'
0,0,0,0,0,'I tried to clean this up as much as I can...\r\n\r\n'
0,0,0,0,0,'noformat\r\nIt could use some code comments and cleanup but its time for a break  !https://issues'
0,0,0,0,0,'Updated patch... I tried to speed up BS2 some more here in this patch:\r\nnoformat\r\n               Task   QPS trunkStdDev trunk   QPS patchStdDev patch      Pct diff\r\n             Respell       76.69        1.75       74.06        1.36   -7\% -    0\%\r\n          AndHighMed       89.69        1.85       86.70        2.83   -8\% -    1\%\r\n            SpanNear        2.75        0.09        2.70        0.07   -7\% -    4\%\r\n              Fuzzy2       34.94        0.66       34.46        0.54   -4\% -    2\%\r\n              Fuzzy1      115.28        2.24      113.81        1.59   -4\% -    2\%\r\n         AndHighHigh       12.97        0.27       12.84        0.34   -5\% -    3\%\r\n        TermBGroup1M       47.87        0.27       47.42        0.51   -2\% -    0\%\r\n      TermBGroup1M1P       53.84        1.08       53.35        0.81   -4\% -    2\%\r\n         TermGroup1M       42.55        0.51       42.36        0.56   -2\% -    2\%\r\n            PKLookup      298.51        0.80      297.81        2.68   -1\% -    0\%\r\n            Wildcard       20.66        1.13       20.75        1.09   -9\% -   11\%\r\n              Phrase        6.64        0.26        6.67        0.31   -7\% -    9\%\r\n             Prefix3       32.82        1.69       33.07        1.72   -9\% -   11\%\r\n        SloppyPhrase       26.12        0.38       26.46        0.42   -1\% -    4\%\r\n              IntNRQ       11.24        1.55       11.39        1.38  -21\% -   31\%\r\n                Term      138.34        0.93      141.27        8.71   -4\% -    9\%\r\n          OrHighHigh        7.28        0.38        7.71        0.50   -5\% -   19\%\r\n           OrHighMed       20.29        1.21       21.81        1.61   -6\% -   22\%\r\nnoformat\r\nIt could use some code comments and cleanup but its time for a break  !https://issues.apache.org/jira/images/icons/emoticons/smile.png|width=16,height=16!'
0,0,0,0,0,'Good idea. '
0,0,0,1,0,'The basic impl is working, I think (the random test passes),\r\nbut I have alot of nocommits still!'
1,0,0,0,0,'The patch also removes a call to the term constructor in BlendedTermQuery, which was actually making a clone.\r\nThis might have gone too far, but I think it should work because the boost is in BoostQuery now.'
0,0,0,1,0,'This has bitten a few people at least, including me, and I\'d rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.'
0,0,0,0,0,'Though, can\'t BS and BS2 just call super.visitSubScorers first, and then visit their subs? '
0,0,0,1,0,'ed and the term stats?\r\n\r\nDecisionThings i\'ve added/revised in this new vers'
0,0,0,1,0,'I feel down the rabit hole a bit looking into this, and I still have no concrete idea what the underlying problem is, but i have a few uneducated guesses...\r\n * the problem '
1,0,0,0,0,'I would prefer to use single pass and for now I only need the parent docs. '
1,0,0,0,0,'This patch superceeds my \"test\" patch by actually including the fix as well.'
1,0,0,0,0,'Maybe might it be advisable to have an alternative constructor that doesn\'t clone so that users like Solr can exploit the fact that their code won\'t be making any further use of the input term?'
0,0,0,0,1,'have an alternative constructor that doesn\'t clone'
0,0,1,0,0,'Won\'t this change have the prospect of increasing the amount of GC due to all these extra objects?'
0,1,0,0,0,'OK I like the 3 methods instead of the single method with BooleanClause.OCCUR param.'
1,0,0,0,0,'Patch for branch_3x. Features:\r\n # Adds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.\r\n # When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.\r\n # When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.\r\n # When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.\r\n # When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.\r\n # Links from Solr javadocs to Lucene\'s javadocs are enabled. When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss\'s patch on this issue. When building a release version, links are to the same-versioned Lucene release javadocs.'
1,0,0,0,0,'sed in this new version of the patch...\r\n * added some more asserts to TestExplain.testRajeshData to help demonstrate that it\'s the score that has changed between successful vs failing runs\r\n * fixed a bug in TestExplain.testExplainScoreEquality that was causing false failures\r\n ** randomSimpleString) can produce the empty string, which was causing the test to not match the numDocs it was expecting since a whitespace based analyzer is being used\r\n ** NOTE: these false failures where the only case i\'ve ever seen TestExplain.testExplainScoreEquality fail\r\n * added a TestBaseExplanationTestCase to the test-framework\r\n ** I had some initial concerns that maybe some old changes/refactorings to BaseExplanationTestCase had completely eliminated the checks that were suppose to be done in that test, so i added this class to ensure it would fail as expected when an Explanation didn\'t match scores\r\n * added more coord randomization in TestSimpleExplanations\r\n ** even w/these changes, i\'ve never seen these tests fail\r\n * added a new TestSimpleExplanationsWithHeavyIDF\r\n ** this was one of the first things i tried, based on Ahmet\'s concerns about IDFStats.normalize\r\n ** since TestExplain.testRajeshData was failing so consistently, but we\'ve never seen failures like this from any of of the existing BaseExplanationTestCase, i speculated that maybe IDF stats or index size were a big factor, and tried to bang out a subclass of TestSimpleExplanations that added a lot of  docs with the same terms to make the doc freq stats more interesting for the terms being searched on\r\n ** i\'ve never seen this test fail\r\n ** some refactoring in BaseExplanationTestCase was needed to write this test\r\n\r\n----\r\nAs things stand now with the current patch, here are some test seeds for TestExplain that pas'
0,0,0,1,0,'bulk move 3.2 -> 3.3'
1,0,0,0,0,'2nd patch of 3 October 2015.\r\nIn addition to the previous patch, this also\r\n - deletes the Term cloning in PhraseQuery,\r\n - adds a Term constructor from a BytesRefBuilder, and\r\n - removes BytesRef copying at Term construction from Solr\'s FieldType, SolrIndexSearcher, FacetField and SimpleMLTQParser.'
0,0,0,1,0,'\r\nWell, we can worry about that later I guess... you already have a pmd.excludes so if we want to tweak things like\r\nthat we could just define pmd.excludes in analyzers-common or whatever.'
0,0,0,0,0,'\r\nNah, good point mike  !'
0,0,0,0,0,'- will fix that soon.'
0,0,0,1,0,'W/o really having firm idea what\'s going on here i spent some time reviewing the tests that Ahmet posted, and then started poking at things with a stick – mainly to see if I could figure out the missing piece of the puzzle that caused things to fail in \"testRajeshData\" but not in any of our existing randomized tests.\r\n\r\nI feel down the rabi'
0,0,0,1,0,' this JVM: [TestExplain]\r\nnoformat\r\nMy refactoring of BaseExplanationTestCase also seems to have somehow introduced/tickled a stranage bug in the test framework that trips our SecurityManager settings – note that the seed below causes an odd AccessControlException at the suite level *after* all TestSimpleExplanations test methods have finished s'
0,0,0,0,1,' Can you also remove the explicit cloning '
0,0,0,1,0,'I tried, but then a test fails.\r\nPerhaps this is because a BytesRef is passed ClassificationResult there.'
0,1,0,0,0,'I like how the patch makes things simpler.'
1,0,0,0,0,'Simple patch:\r\n * removes GeoPointTestUtil from TestGeoPointQuery\r\n * fixes a range corner case in GeoPointPrefixTermsEnum\r\n * adds an explicit test for the corner case'
0,1,0,0,0,'/Scorer optimization code?Procolor\r\n *** perhaps because a segment may not even contain one of the optional terms, so it'
0,0,0,0,1,'it doesn\'t seem removing the deep copy in favour of creating new BytesRef would improve anything, actually it\'d be slightly worse. I would say let\'s keep that.'
0,1,0,0,0,'good point indeed, in the end the underlying methods all create a new Term from the same BytesRef and field name, so that should be better than the current solution, so we should pass the Term created from within the loop to the methods to calculate prior and likelihood in SimpleNaiveBayesClassifier.'
0,0,0,1,0,'One could also create the Term in the loop and pass that, or its Term.bytes(), around to the other methods.\r\nTerm.bytes() can also be passed to the ClassificationResult.'
1,0,0,0,0,'patch modified to pass the Term instead of the BytesRef in SimpleNaiveBayesClassifier.'
0,0,0,0,0,'decisionAnyway, I changed the 3 parameter methods to take a single argument holding all required information. This is way more extensible and flexible since we don\'t need to change a method signature just to add a n'
0,0,0,0,0,'How do we proceed with this? '
0,0,0,1,0,'this puts my simple64 indexes smaller than standardcodec (and speeds up the queries too)'
0,1,0,0,0,'Robert, patch looks good except of one thing.'
1,0,0,0,0,'\r\n\r\n\r\nI wonder if it wouldn\'t be better to leave it, but commented out - with a short explanation.\r\n\r\nOptimizing is not necessary, but it clearly has benefits to query perf! If you are not updating often, I think it can make perfect sense.\r\n\r\nSo I\'m fine with just dropping, but not sure if commenting it out and putting something like:\r\n// for an index that is not updated often, we might optimize now\r\n\r\nor variation...'
0,0,0,1,0,'Anyway, I changed the 3 parameter methods to take a single argument holding all required information. This is way more extensible and flexible since we don\'t need to change a method signature just to add a new param. '
0,1,0,0,0,'Procolor\r\n *** perhaps because a segment may not even contain one of the optional terms, so it\'s Scorer is null?\r\n ** if'
0,0,0,1,0,'noformat\r\nMy refactoring of BaseExplanationTestCase also seems to have somehow introduced/tickled a stranage bug in the test framework that trips our SecurityManager settings – note that the seed below causes an odd AccessControlException at the suite level *after* all TestSimpleExplanations test methods have finished successfully...\r\nnoformat'
0,0,0,1,0,'DecisionThings i\'ve added/revised in this new version of the patch...\r\n * ad'
1,0,0,0,0,'Attaching a slightly new version of the patch incorporating roughly the idea of 3-distinct methods. Yet, having 3 methods instead of one was kind of ugly to implement and I didn\'t want to add switch case statements to visitSubScorer methods just to call a different method.'
0,0,0,1,0,'Committed Revision: 900160 '
0,0,0,1,0,'New patch, making a few more classes package private...'
0,0,0,1,0,'I changed this method to use _TestUtil.createTempFile... '
0,1,0,0,0,+1
1,0,0,0,0,patch.
0,0,0,0,1,'My first question is, what about backward compatibility requirements? '
1,0,0,0,0,'Once you make a parser for something, you\'ve promised that it has (at least) a particular set of inputs until the next major version, yes?\r\n\r\n'
1,0,0,0,0,'Support D-m-q in the XML parser.'
1,0,0,0,0,'Right now, what\'s in parens are boolean clauses (with +/-). The insides of a disjunct aren\'t boolean clauses, they are queries. This could be pretty confusing all around. It would be really better to introduce some syntax that allows for various sorts of grouping, but I don\'t want to step on the Solr parser\'s use of . Further, DisjunctionMaxQuery is just one thing, and using up | for it seems ill-advised.'
0,1,0,0,0,'I think we allow this for OR as well anyway, so it would be ambiguous...?'
1,0,0,0,0,'Note that the patch is to trunk, I\'m sort of assuming that you all do \'patch the trunk and then backport as appropriate\'.'
0,0,0,1,0,'Committed revision 935014.'
0,0,0,1,0,'Committed an additional fix: 935048, this allows you to run the contrib/ant tests from eclipse too.'
0,1,0,0,0,'I love the copyToWorkDir...'
0,0,0,1,0,'Instead of looking for stuff on your hard drive, it creates some randomish documents using a selection of strings that will match the test queries, combined with some random unicode strings ala TestStressIndexing2.\r\n\r\n'
0,1,0,0,0,'This is a great cleanup Robert! '
0,0,0,1,0,'the attached patch refactors the benchmark tests:\r\n * logic to run a benchmark test is moved to BenchmarkTestCase\r\n * this forces them all to respect LuceneTestCase.TEMP_DIR for all file operations\r\n * lucene.common.dir is removed'