@relation QueryResult-weka.filters.unsupervised.attribute.NominalToString-C1-weka.filters.unsupervised.attribute.NumericToNominal-Rfirst-last-weka.filters.unsupervised.attribute.Remove-weka.filters.unsupervised.attribute.Remove-R3

@attribute sentence string
@attribute isRelevant {0,1}

@data
'Another non-reproducing failure, from my Jenkins:\n\nChecking out Revision 85a27a231fdddb118ee178baac170da0097a02c0 (refs/remotes/origin/master)\n[...]\n   [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates\n   [junit4] IGNOR/A 0.00s J0 | TestBinaryDocValuesUpdates.testTonsOfUpdates\n   [junit4]    > Assumption #1: \'nightly\' test group is disabled (@Nightly())\n   [junit4]   1> TEST: isNRT=true reader1=StandardDirectoryReader(segments:3:nrt _0(7.0.0):c2)\n   [junit4]   1> TEST: now reopen\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=61A674A498110EC0 -Dtests.slow=true -Dtests.locale=ja-JP -Dtests.timezone=Greenwich -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] FAILURE 0.07s J0 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f0, reader=_4(7.0.0):c19 expected:<4> but was:<3>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([61A674A498110EC0:575A168B19E46DDC]:0)\n   [junit4]    > \tat org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=false): {}, locale=ja-JP, timezone=Greenwich\n   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=220134656,total=306184192\n   [junit4]   2> NOTE: All tests run in this JVM: [TestStringMSBRadixSorter, TestSpanTermQuery, TestOmitPositions, TestIndexableField, TestHighCompressionMode, TestDeterminizeLexicon, TestPackedTokenAttributeImpl, TestTopDocsCollector, TestIndexOrDocValuesQuery, TestDocValuesRewriteMethod, TestDocument, TestCrash, TestWildcardRandom, TestDocIdSetBuilder, TestFilterLeafReader, TestMergedIterator, TestMultiThreadTermVectors, TestAtomicUpdate, TestNorms, Test4GBStoredFields, TestFixedLengthBytesRefArray, TestFieldInvertState, TestBoolean2ScorerSupplier, TestLevenshteinAutomata, TestGraphTokenStreamFiniteStrings, TestStandardAnalyzer, TestSegmentReader, TestScorerPerf, TestBoostQuery, TestMergePolicyWrapper, TestComplexExplanations, TestPointQueries, TestMixedCodecs, TestPointValues, TestMultiMMap, TestLazyProxSkipping, TestTerms, TestIndexWriterThreadsToSegments, TestFilterWeight, TestDocumentsWriterDeleteQueue, TestCharFilter, TestDocInverterPerFieldErrorInfo, TestSimilarityProvider, LimitedFiniteStringsIteratorTest, TestNewestSegment, TestFSTs, TestClassicSimilarity, TestUnicodeUtil, TestQueryBuilder, TestSwappedIndexFiles, TestTimSorterWorstCase, TestBinaryDocValuesUpdates]',1
'Another non-reproducing failure, from https://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/19961/ (log - and commit sha - no longer available; the notification email arrived on June 24 at 10:37PM):\n\n[...]\n  [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates\n  [junit4]   1> TEST: isNRT=false reader1=StandardDirectoryReader(segments_1:4 _0(7.0.0):C2)\n  [junit4]   1> TEST: now reopen\n  [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=3A4BC284D906CE1A -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=hr-HR -Dtests.timezone=Pacific/Pitcairn -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n  [junit4] FAILURE 0.65s J2 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<\n  [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f4, reader=_28(7.0.0):C936 expected:<12> but was:<11>\n  [junit4]    > \tat __randomizedtesting.SeedInfo.seed([3A4BC284D906CE1A:CB7A0AB58F3AD06]:0)\n  [junit4]    > \tat org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)\n  [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n  [junit4] IGNOR/A 0.00s J2 | TestBinaryDocValuesUpdates.testTonsOfUpdates\n  [junit4]    > Assumption #1: \'nightly\' test group is disabled (@Nightly())\n  [junit4]   2> NOTE: test params are: codec=HighCompressionCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, maxDocsPerChunk=1, blockSize=26), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, blockSize=26)), sim=RandomSimilarity(queryNorm=false): {}, locale=hr-HR, timezone=Pacific/Pitcairn\n  [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 1.8.0_131 (64-bit)/cpus=8,threads=1,free=302943312,total=518979584\n  [junit4]   2> NOTE: All tests run in this JVM: [TestDirectPacked, TestFieldCacheRewriteMethod, TestBagOfPostings, TestEarlyTermination, TestReaderWrapperDVTypeCheck, TestNeedsScores, TestRoaringDocIdSet, TestShardSearching, TestSpansEnum, TestSegmentTermEnum, TestLongPostings, TestIndexReaderClose, TestLucene70NormsFormat, TestReqExclBulkScorer, TestField, TestSegmentTermDocs, TestSimilarityBase, TestGeoEncodingUtils, TestPayloadsOnVectors, TestCharTermAttributeImpl, TestDisjunctionMaxQuery, TestTermRangeQuery, TestLongValuesSource, TestCachingTokenFilter, TestOfflineSorter, TestTopDocsCollector, TestBufferedIndexInput, TestTermScorer, TestPerFieldPostingsFormat2, TestConsistentFieldNumbers, TestFieldsReader, TestConjunctions, TestSloppyPhraseQuery2, TestSetOnce, TestRollingUpdates, TestIndexWriterLockRelease, TestIndexWriterMergePolicy, TestRollingBuffer, TestBinaryDocument, TestSimpleFSLockFactory, TestIndexingSequenceNumbers, FiniteStringsIteratorTest, TestGraphTokenStreamFiniteStrings, TestSentinelIntSet, TestHugeRamFile, TestSortedNumericSortField, TestMultiCollector, TestSpanNotQuery, TestAllFilesHaveCodecHeader, TestTrackingDirectoryWrapper, TestControlledRealTimeReopenThread, TestDirectoryReader, TestDocValues, TestDoubleRangeFieldQueries, TestSpanCollection, TestDemoParallelLeafReader, TestSpans, TestTerms, Test2BBinaryDocValues, TestParallelCompositeReader, TestArrayUtil, TestPrefixQuery, TestAttributeSource, TestByteBlockPool, TestCompiledAutomaton, TestSimpleExplanationsOfNonMatches, TestDocValuesScoring, TestExceedMaxTermLength, TestNRTThreads, TestLazyProxSkipping, TestSimilarity2, TestSearchWithThreads, TestPolygon2D, TestGrowableByteArrayDataOutput, TestIndexCommit, TestBasics, TestSearcherManager, TestNorms, TestStandardAnalyzer, TestTopDocsMerge, TestMinimize, TestNRTReaderWithThreads, TestIndexWriterForceMerge, TestPerFieldPostingsFormat, TestCollectionUtil, TestFastDecompressionMode, TestSort, TestMultiDocValues, TestCustomSearcherSort, TestTermsEnum2, Test2BDocs, TestMixedCodecs, TestSpanExplanations, TestFastCompressionMode, TestStressIndexing2, TestMultiPhraseQuery, TestDeterminism, TestMergeSchedulerExternal, TestForceMergeForever, TestSameScoresWithThreads, TestMultiFields, TestLiveFieldValues, TestSpanSearchEquivalence, TestPayloads, TestDoc, TestFieldMaskingSpanQuery, TestExternalCodecs, TestRegexpQuery, TestIntBlockPool, TestComplexExplanationsOfNonMatches, TestParallelReaderEmptyIndex, TestDocument, TestFileSwitchDirectory, TestDirectory, TestRegexpRandom, TestMultiLevelSkipList, TestCheckIndex, TestBooleanQueryVisitSubscorers, TestMatchAllDocsQuery, TestSubScorerFreqs, TestIndexWriterConfig, TestPositionIncrement, TestSpanExplanationsOfNonMatches, TestFilterLeafReader, TestSameTokenSamePosition, TestAutomatonQueryUnicode, TestRamUsageEstimator, TestSpanFirstQuery, TestIsCurrent, TestNoMergePolicy, TestNoMergeScheduler, TestNamedSPILoader, TestBytesRef, TestCharFilter, TestTwoPhaseCommitTool, TestCloseableThreadLocal, TestVersion, TestReaderClosed, TestNGramPhraseQuery, TestIntsRef, Test2BPositions, Test2BPostingsBytes, Test2BTerms, TestByteArrayDataInput, Test2BPagedBytes, TestCharArraySet, TestDelegatingAnalyzerWrapper, TestStopFilter, TestBlockPostingsFormat, TestLucene50TermVectorsFormat, Test2BSortedDocValuesOrds, TestAllFilesCheckIndexHeader, TestAllFilesHaveChecksumFooter, TestBinaryDocValuesUpdates]\n  [junit4] Completed [362/453 (1!)]',1
'on J2 in 6.22s, 29 tests, 1 failure, 1 skipped <<< FAILURES!',0
'I\'ll tackle this.',0
'Commit eaf1d45a1cad74a1037c7c4178fd2379a903f8cc in lucene-solr\'s branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45 ]\nLUCENE-7888: fix concurrency hazards between merge completing and DV updates applying',0
'I think these should be fixed now.',0
'It was a tricky concurrency hazard, where an indexing thread that\'s resolving DV updates thinks it\'s done just as a merge is wrapping up and in that case there was a window between the two threads where DV updates could be lost.',1
'Thanks Steve Rowe.',0
'Mike, any reason not to backport to branch_7x and branch_7_0?',0
'There was a recent failure on a Jenkins branch_7x job https://jenkins.thetaphi.de/job/Lucene-Solr-7.x-Linux/9/:\n\nChecking out Revision 758cbd98a7aa020ad67aea775028badf0be6418c (refs/remotes/origin/branch_7x)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMixedDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D57106AE532F4164 -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=qu-PE -Dtests.timezone=America/Yakutat -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 0.44s J2 | TestMixedDocValuesUpdates.testManyReopensAndFields <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: invalid binary value for doc=0, field=f2, reader=_y(7.1.0):C435:fieldInfosGen=2:dvGen=2 expected:<7> but was:<6>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([D57106AE532F4164:E38D6481D2DA2278]:0)\n   [junit4]    > \tat org.apache.lucene.index.TestMixedDocValuesUpdates.testManyReopensAndFields(TestMixedDocValuesUpdates.java:141)\n   [junit4]    > \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n   [junit4]    > \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n   [junit4]    > \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n   [junit4]    > \tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n   [junit4]    > \tat java.base/java.lang.Thread.run(Thread.java:844)\n   [junit4] IGNOR/A 0.00s J2 | TestMixedDocValuesUpdates.testTonsOfUpdates\n   [junit4]    > Assumption #1: \'nightly\' test group is disabled (@Nightly())\n   [junit4]   2> NOTE: leaving temporary files on disk at: /home/jenkins/workspace/Lucene-Solr-7.x-Linux/lucene/build/core/test/J2/temp/lucene.index.TestMixedDocValuesUpdates_D57106AE532F4164-001\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70), sim=RandomSimilarity(queryNorm=true): {}, locale=qu-PE, timezone=America/Yakutat\n   [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 9 (64-bit)/cpus=8,threads=1,free=165322832,total=342360064',1
'More non-reproducing master failures from my Jenkins, commit shas are all after Mike\'s commit on this issue:\n\nChecking out Revision 48b4960e0c093b480b8328f324992a7006054f17 (refs/remotes/origin/master)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=6A2FDBE9B9C2F59C -Dtests.slow=true -Dtests.locale=ar-YE -Dtests.timezone=Asia/Beirut -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 0.31s J1 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=63, field=f1, reader=_l(8.0.0):c88:fieldInfosGen=2:dvGen=2 expected:<4> but was:<3>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([6A2FDBE9B9C2F59C:5CD3B9C638379680]:0)\n   [junit4]    > \tat org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   1> TEST: isNRT=false reader1=StandardDirectoryReader(segments_1:4 _0(8.0.0):c2)\n   [junit4]   1> TEST: now reopen\n   [junit4] IGNOR/A 0.00s J1 | TestBinaryDocValuesUpdates.testTonsOfUpdates\n   [junit4]    > Assumption #1: \'nightly\' test group is disabled (@Nightly())\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {bdv=FSTOrd50, f=PostingsFormat(name=LuceneVarGapDocFreqInterval), k1=PostingsFormat(name=LuceneVarGapDocFreqInterval), dvUpdateKey=PostingsFormat(name=LuceneVarGapDocFreqInterval), k2=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), foo=PostingsFormat(name=LuceneVarGapDocFreqInterval), upd=PostingsFormat(name=Direct), updKey=FSTOrd50, id=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), key=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128)))}, docValues:{val=DocValuesFormat(name=Lucene70), ndv=DocValuesFormat(name=Direct), cf=DocValuesFormat(name=Lucene70), ssdv=DocValuesFormat(name=Asserting), sdv=DocValuesFormat(name=Lucene70), f=DocValuesFormat(name=Asserting), f0=DocValuesFormat(name=Asserting), control=DocValuesFormat(name=Lucene70), f1=DocValuesFormat(name=Lucene70), sort=DocValuesFormat(name=Asserting), f2=DocValuesFormat(name=Direct), cf0=DocValuesFormat(name=Lucene70), f3=DocValuesFormat(name=Lucene70), f4=DocValuesFormat(name=Asserting), f5=DocValuesFormat(name=Lucene70), cf1=DocValuesFormat(name=Asserting), bdv2=DocValuesFormat(name=Asserting), number=DocValuesFormat(name=Lucene70), bdv1=DocValuesFormat(name=Lucene70), bdv=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Lucene70), key=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1142, maxMBSortInHeap=7.285443710546513, sim=RandomSimilarity(queryNorm=false): {}, locale=ar-YE, timezone=Asia/Beirut\n   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=296202568,total=395313152\n\n\n\nChecking out Revision cb23fa9b4efa5fc7c17f215f507901d459e9aa6f (refs/remotes/origin/master)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D695B86B920AF645 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=en-ZA -Dtests.timezone=America/Inuvik -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n   [junit4] FAILURE 0.89s J6  | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f3, reader=_o(8.0.0):c417:fieldInfosGen=2:dvGen=2 expected:<5> but was:<4>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([D695B86B920AF645:E069DA4413FF9559]:0)\n   [junit4]    > \tat org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-Nightly-master/workspace/lucene/build/core/test/J6/temp/lucene.index.TestBinaryDocValuesUpdates_D695B86B920AF645-001\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {bdv=FSTOrd50, k1=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), f=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), dvUpdateKey=FSTOrd50, foo=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), k2=PostingsFormat(name=LuceneFixedGap), upd=Lucene50(blocksize=128), updKey=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), id=PostingsFormat(name=LuceneFixedGap), key=PostingsFormat(name=LuceneFixedGap)}, docValues:{ndv=DocValuesFormat(name=Memory), f10=DocValuesFormat(name=Asserting), f12=DocValuesFormat(name=Direct), f11=DocValuesFormat(name=Lucene70), f14=DocValuesFormat(name=Asserting), f13=DocValuesFormat(name=Memory), f0=DocValuesFormat(name=Lucene70), f16=DocValuesFormat(name=Direct), f1=DocValuesFormat(name=Direct), f15=DocValuesFormat(name=Lucene70), f2=DocValuesFormat(name=Memory), f18=DocValuesFormat(name=Asserting), f3=DocValuesFormat(name=Asserting), f17=DocValuesFormat(name=Memory), f4=DocValuesFormat(name=Lucene70), f19=DocValuesFormat(name=Lucene70), f5=DocValuesFormat(name=Direct), bdv2=DocValuesFormat(name=Lucene70), f6=DocValuesFormat(name=Memory), number=DocValuesFormat(name=Direct), f7=DocValuesFormat(name=Asserting), f8=DocValuesFormat(name=Lucene70), bdv1=DocValuesFormat(name=Asserting), f9=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Direct), val=DocValuesFormat(name=Asserting), f21=DocValuesFormat(name=Asserting), f20=DocValuesFormat(name=Memory), f23=DocValuesFormat(name=Direct), f22=DocValuesFormat(name=Lucene70), f25=DocValuesFormat(name=Asserting), f24=DocValuesFormat(name=Memory), sort=DocValuesFormat(name=Lucene70), cf0=DocValuesFormat(name=Asserting), cf2=DocValuesFormat(name=Direct), cf1=DocValuesFormat(name=Lucene70), cf4=DocValuesFormat(name=Asserting), cf3=DocValuesFormat(name=Memory), cf6=DocValuesFormat(name=Direct), cf5=DocValuesFormat(name=Lucene70), cf8=DocValuesFormat(name=Asserting), cf7=DocValuesFormat(name=Memory), cf9=DocValuesFormat(name=Lucene70), ssdv=DocValuesFormat(name=Lucene70), sdv=DocValuesFormat(name=Asserting), cf25=DocValuesFormat(name=Lucene70), cf23=DocValuesFormat(name=Memory), cf24=DocValuesFormat(name=Asserting), cf21=DocValuesFormat(name=Lucene70), cf22=DocValuesFormat(name=Direct), cf20=DocValuesFormat(name=Asserting), key=DocValuesFormat(name=Direct), cf=DocValuesFormat(name=Direct), cf18=DocValuesFormat(name=Lucene70), cf19=DocValuesFormat(name=Direct), f=DocValuesFormat(name=Lucene70), cf16=DocValuesFormat(name=Memory), cf17=DocValuesFormat(name=Asserting), cf14=DocValuesFormat(name=Lucene70), cf15=DocValuesFormat(name=Direct), control=DocValuesFormat(name=Asserting), cf12=DocValuesFormat(name=Memory), cf13=DocValuesFormat(name=Asserting), cf10=DocValuesFormat(name=Lucene70), cf11=DocValuesFormat(name=Direct), bdv=DocValuesFormat(name=Memory)}, maxPointsInLeafNode=74, maxMBSortInHeap=7.963102974639169, sim=RandomSimilarity(queryNorm=true): {}, locale=en-ZA, timezone=America/Inuvik\n   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=73914040,total=520617984\n\n\n\nChecking out Revision cb23fa9b4efa5fc7c17f215f507901d459e9aa6f (refs/remotes/origin/master)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMixedDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D695B86B920AF645 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=pt-BR -Dtests.timezone=Africa/Porto-Novo -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n   [junit4] FAILURE 0.17s J6  | TestMixedDocValuesUpdates.testManyReopensAndFields <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: invalid binary value for doc=0, field=f1, reader=_7(8.0.0):c118:fieldInfosGen=2:dvGen=2 expected:<3> but was:<2>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([D695B86B920AF645:E069DA4413FF9559]:0)\n   [junit4]    > \tat org.apache.lucene.index.TestMixedDocValuesUpdates.testManyReopensAndFields(TestMixedDocValuesUpdates.java:141)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-Nightly-master/workspace/lucene/build/core/test/J6/temp/lucene.index.TestMixedDocValuesUpdates_D695B86B920AF645-001\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {upd=Lucene50(blocksize=128), updKey=PostingsFormat(name=LuceneVarGapDocFreqInterval), id=PostingsFormat(name=LuceneVarGapFixedInterval), key=PostingsFormat(name=LuceneVarGapFixedInterval)}, docValues:{f10=DocValuesFormat(name=Memory), f12=DocValuesFormat(name=Lucene70), f11=DocValuesFormat(name=Lucene70), f14=DocValuesFormat(name=Memory), f13=DocValuesFormat(name=Asserting), f0=DocValuesFormat(name=Lucene70), f16=DocValuesFormat(name=Lucene70), f15=DocValuesFormat(name=Lucene70), f1=DocValuesFormat(name=Lucene70), f2=DocValuesFormat(name=Asserting), f18=DocValuesFormat(name=Memory), f17=DocValuesFormat(name=Asserting), f3=DocValuesFormat(name=Memory), f4=DocValuesFormat(name=Lucene70), f19=DocValuesFormat(name=Lucene70), f5=DocValuesFormat(name=Lucene70), f6=DocValuesFormat(name=Asserting), f7=DocValuesFormat(name=Memory), f8=DocValuesFormat(name=Lucene70), f9=DocValuesFormat(name=Lucene70), id=DocValuesFormat(name=Lucene70), f21=DocValuesFormat(name=Memory), f20=DocValuesFormat(name=Asserting), f23=DocValuesFormat(name=Lucene70), f22=DocValuesFormat(name=Lucene70), f25=DocValuesFormat(name=Memory), f24=DocValuesFormat(name=Asserting), cf0=DocValuesFormat(name=Memory), cf2=DocValuesFormat(name=Lucene70), cf1=DocValuesFormat(name=Lucene70), cf4=DocValuesFormat(name=Memory), cf3=DocValuesFormat(name=Asserting), cf6=DocValuesFormat(name=Lucene70), cf5=DocValuesFormat(name=Lucene70), cf8=DocValuesFormat(name=Memory), cf7=DocValuesFormat(name=Asserting), cf9=DocValuesFormat(name=Lucene70), cf25=DocValuesFormat(name=Lucene70), cf23=DocValuesFormat(name=Asserting), cf24=DocValuesFormat(name=Memory), cf21=DocValuesFormat(name=Lucene70), cf22=DocValuesFormat(name=Lucene70), cf20=DocValuesFormat(name=Memory), key=DocValuesFormat(name=Lucene70), cf=DocValuesFormat(name=Lucene70), cf18=DocValuesFormat(name=Lucene70), cf19=DocValuesFormat(name=Lucene70), f=DocValuesFormat(name=Lucene70), cf16=DocValuesFormat(name=Asserting), cf17=DocValuesFormat(name=Memory), cf14=DocValuesFormat(name=Lucene70), cf15=DocValuesFormat(name=Lucene70), cf12=DocValuesFormat(name=Asserting), cf13=DocValuesFormat(name=Memory), cf10=DocValuesFormat(name=Lucene70), cf11=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1397, maxMBSortInHeap=7.760564666966891, sim=RandomSimilarity(queryNorm=false): {}, locale=pt-BR, timezone=Africa/Porto-Novo\n   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=283409888,total=478150656\n\n\n\nChecking out Revision 6c163658bbca15b1e4ff81d16b25e07df78468e8 (refs/remotes/origin/master)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=85C41F1E0BBEB082 -Dtests.slow=true -Dtests.locale=ca-ES -Dtests.timezone=Kwajalein -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 0.71s J0 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f2, reader=_d(8.0.0):c55:fieldInfosGen=2:dvGen=2 expected:<4> but was:<3>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([85C41F1E0BBEB082:B3387D318A4BD39E]:0)\n   [junit4]    > \tat org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4] IGNOR/A 0.00s J0 | TestBinaryDocValuesUpdates.testTonsOfUpdates\n   [junit4]    > Assumption #1: \'nightly\' test group is disabled (@Nightly())\n   [junit4]   1> TEST: isNRT=true reader1=StandardDirectoryReader(segments:3:nrt _0(8.0.0):c2)\n   [junit4]   1> TEST: now reopen\n   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=true): {}, locale=ca-ES, timezone=Kwajalein\n   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=312487000,total=502792192',1
'Mike, any reason not to backport to branch_7x and branch_7_0?',0
'Ugh, I thought my commit went in before 7.x/7.0 branched; I\'ll back port tomorrow, and look into the new test failures!',0
'Mike, any reason not to backport to branch_7x and branch_7_0?',0
'OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I\'m not losing my mind as much as I previously thought.',0
'I\'ll dig on the new failures.',0
'\nMike, any reason not to backport to branch_7x and branch_7_0?',0
'OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I\'m not losing my mind as much as I previously thought.',0
'Crap, sorry for wasting your time.',0
'I think I looked at git log for one of those branches and didn\'t see your commit, and then assumed, given the close timing of the branches\' being cut, that the commit didn\'t make it.',0
'But looking now I see it on both branches\' logs.',0
'No worries Steve Rowe!',0
'Also, I think the 4 non-reproducing seeds above (https://issues.apache.org/jira/browse/LUCENE-7888?focusedCommentId=16077449&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16077449) were before my last commit (7c704d5258b3be8c383ccb96bf4a30be441f091c) fixing a race ... so I\'m hoping there are no more failures in this challenging test',1
'I can look into the clustering plugin\'s use of it.',1
'I recall it was unfortunately required, but will have to go into this again to remind myself why.',1
'Patch removing context classloader usage.',1
'Tests seem to pass, unfortunately Solr trunk is very unstable.',1
'Some unrelated tests also fail on Jenkins, so I cannot be sure all is fine.',1
'This patch also adds context class loaders on te forbidden api list.',1
'Because of that I used the withContextClassLoader(ClassLoader, () -> { ... }) lambda method.',1
'Looks good to me.',1
'I\'ll check why the context classloader is required in clustering later on.',1
'I think the case was that clustering was under shared libraries and resources loaded per-core couldn\'t figure where to load classes from.',1
'I did some live test with the standalone techproducts example.',1
'I have seen no issues, so I think this should be fine to commit.',1
'I will add a CHANGES entry in both Lucene and Solr, because this affects both projects.',0
'Unfortunately the latest test failures on master make it hard to differentiate between failures caused by my changes and ones already there.',1
'But all failures in tests that I see here, look like the ones Jenkins is drinking with his beers!',0
'This is not a good state, the test suite should pass for a clean checkout.',0
'Commit 5de15ff403fbf4afe68718151617e6104f7e3888 in lucene-solr\'s branch refs/heads/master from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5de15ff ]\nLUCENE-7883: Lucene/Solr no longer uses the context class loader when resolving resources',0
'I added changes and migrate entries and committed to master (7.0).',1
'Here is a patch.',1
'+1 good catch!',1
+1,1
'Commit 3a0c2a691d4fea1670b3d071032fc54c716b5d1a in lucene-solr\'s branch refs/heads/branch_6_5 from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3a0c2a6 ]\nLUCENE-7755: Join queries should not reference IndexReaders.',0
'Commit bd2ec8e40e83e4712062c37ed121132054409918 in lucene-solr\'s branch refs/heads/branch_6x from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=bd2ec8e ]\nLUCENE-7755: Join queries should not reference IndexReaders.',0
'Commit edafcbad14482f3cd2f072fdca0c89600e72885d in lucene-solr\'s branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=edafcba ]\nLUCENE-7755: Join queries should not reference IndexReaders.',0
'Patch that ensures we won\'t add any new compiler warnings in several categories (that we\'re pretty good on already) in the future.',1
'We can deal with fixing existing rawtypes or a few other classes of warning in the future.',1
'David Smiley - you seemed interested in this conversation last time it came up.',0
'Thanks for filing this issue!',0
'Are all the changes in this patch necessary to get the build to pass?',1
'So to clarify... no code (outside what the patch touches) needs adjustments?',1
'Yes, the build passes for me with only the two additional changes in WordDictionary and SimpleServer.',1
'Warnings for -Xlint:-auxiliaryclass -Xlint:-deprecation -Xlint:-rawtypes -Xlint:-serial -Xlint:-unchecked are all disabled.',1
'Each of those causes a lot of errors that I\'d like to see eventually followed up on.',1
'The auxiliary class warnings are the easiest of those, but still enough work that I felt like it should be a separate task.',1
'I also have a sneaking suspicion that this only affects lucene and solr is somehow ignoring it, but couldn\'t find anything to confirm that.',1
'Yes on Solr your change is not enabled: https://github.com/apache/lucene-solr/blob/master/solr/common-build.xml#L30\nWe should also review Solr (maybe in a separate issue).',1
'I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint.',1
'But it\'s good to have them listed!',1
'Yea, I like having them listed because it makes it easier to go back and look at them and decide which ones to add.',1
'I don\'t have access to an IBM jdk to check if that produces different output or not.',0
'Uwe Schindler - do you think this is fine to commit or we should tackle more work in this issue?',0
'Hi,\nIBM JDK8+ should also use OpenJDK internally, so I dont\'t think hit has much different options.',1
'I can try later, I have one installed.',0
'What do we do with Solr?',1
'Keep it as it is (it overrides to do no Xlint warnings at all and don\'t fail on warning).',1
'Otherwise I am fine with committing this.',1
'But we should really work on removing unsafe and rawtypes warnings from functions module.',1
'Now those are completely undetected (no warning, no error).',1
'Patch of 2 Jan 2017.',1
'This can be used as proximity subquery whenever SynonymQuery is used now, i.e.',1
'for synonym terms.',1
'I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.',1
'Implementation:\nSynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.',1
'Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.',1
'Aside: how about renaming Terms to FieldTerms?',1
'This takes DisjunctionSpans out of SpanOrQuery.',1
'This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.',1
'PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.',1
'Some plans for using this:\nIn LUCENE-7580 to get real synonym scoring behaviour.',1
'In Surround to score truncations.',1
'In the patch of 2 Jan 2017 the term contexts are extracted twice, once in SynonymWeight and once to create the SpanSynonymWeight.',1
'I\'ll post a fix later.',0
'Patch of 3 Jan 2017.',1
'Compared to yesterday, this adds getTermContexts() in SynonymWeight for use in SpanSynonymQuery.',1
'In SpanSynonymQuery.java here, this is not used:\n\n\nimport org.apache.lucene.search.similarities.Similarity;',1
'GitHub user PaulElschot opened a pull request:\n https://github.com/apache/lucene-solr/pull/165\n LUCENE-7615 of 8 March 2017.',0
'Adds support for SpanSynonymQuery in xml queryparser.',0
'You can merge this pull request into a Git repository by running:\n    $ git pull https://github.com/PaulElschot/lucene-solr lucene7615-20170308\nAlternatively you can review and apply these changes as the patch at:\n https://github.com/apache/lucene-solr/pull/165.patch\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n    This closes #165\n\ncommit 676c13c0c70e3f344ad6fb430eb5868270be83aa\nAuthor: Paul Elschot <paul.j.elschot@gmail.com>\nDate:   2017-03-08T22:10:40Z\n LUCENE-7615 of 8 March 2017.',0
'Adds support for SpanSynonymQuery in xml queryparser.',0
'GitHub user tballison opened a pull request:\n https://github.com/apache/lucene-solr/pull/75\n LUCENE-7434, first draft\n LUCENE-7434, first draft\nYou can merge this pull request into a Git repository by running:\n    $ git pull https://github.com/tballison/lucene-solr master\nAlternatively you can review and apply these changes as the patch at:\n https://github.com/apache/lucene-solr/pull/75.patch\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n    This closes #75\n\ncommit c37f1e0d66f1f28a5c83033d9496cc33c55f265e\nAuthor: tballison <tallison@mitre.org>\nDate:   2016-09-01T19:33:55Z\n LUCENE-7434, first draft',0
'But this allow to create Span Disjunction Query, which is considered as a black sheep in Lucene herd.',1
'I don\'t know why exactly, but have an idea.',0
'Sorry, I\'ve been away from Lucene for too long.',0
'Can you explain a bit more?',0
'Thanks Tim Allison for creating this issue.',0
'In order to merge threads, I want to clarify that my original question was about limiting the search window as well as the number of matches.',1
'The slop  parameter sets the maximum distance allowed between each of the subspans and I was looking to add another parameter for the maximum window in which multiple the sub spans should appear together - between the beginning of the first, to the beginning/end of the last one.',1
'As it happens I am in slow progress making something for the case minNumberShouldMatch=2, all pairs.',1
'In case there is interest in an early version of that, please let me know.',0
'For the maximum window there will be similar restrictions as for LUCENE-7398.',1
'This all pairs thing is useful here anyway, so here it is.',1
'There is a nocommit for an unfinished corner.',0
'It splits off DisjunctionSpans from SpanOr and uses that to determine the matching pairs.',1
Interesting.,1
'Whoa on LUCENE-7398!',0
'What\'s the use case for minNumberShouldMatch=2, all pairs?',0
'Apologies for my daftness...',0
'What\'s the use case for minNumberShouldMatch=2, all pairs?',0
'This might generalize to higher values of minNumberShouldMatch, and replacing the slop by a window is easy.',1
'For higher values of minNumberShouldMatch it would probably be good to reuse the implementation from boolean queries.',1
'Ah, ok.',0
'Thank you.',0
'Is my proposed approach flawed for the minNumberShouldMatch component of Saar Carmi\'s request?',0
'Is my proposed approach flawed for the minNumberShouldMatch component ... ?',0
'Looking at the code on github here https://github.com/apache/lucene-solr/pull/75/commits/c37f1e0d66f1f28a5c83033d9496cc33c55f265e\nit uses NearSpansOrdered and NearSpansUnOrdered with all subSpans, as usual, see lines 277/278.',0
'I think that is too strict when more than the required number of subSpans are actually present in the segment.',1
'The check for presence of subSpans should be at document level, and even then fewer than present might match for the given slop/window.',1
'The (untested) all pairs code above tries to do that, but only for pairs of subSpans.',1
'If I get it right, this is a minor difference.',1
'For my case it should be fine.',1
'Dupe of LUCENE-3369?',0
Y.,0
'Thank you.',0
'Thanks to Trejkaz for pointing out that LUCENE-7434 is a duplicate of LUCENE-3369.',0
'Very cool.',1
'I\'ve been meaning to look into TermAutomatonQueries for a while.',1
'My two concerns: I\'m not sure how this could play nicely with the other SpanQueries, and we\'d want to integrate multiterm handling.',1
'I think so, if we blow up the seed sequence to a a a B B B c c c d d d.',1
'attaching a proof for x a a a and a terrific pic for it',0
'The most intriguing question about TermAutomatonQuery is its\' efficiency.',1
'Can it load term positions only for those docs which passed term level minShouldMatch condition?',1
'eg if someone can experiment, it would be great to search with this query on a huge index and then compare it to to the same query intersected with minShouldMatch disjunction with plain term queries.',1
'Will the later speedups TAQ by faster dragging?',1
'The most intriguing question about TermAutomatonQuery is its\' efficiency.',1
'I think there\'s plenty of work to be done here.',0
E.g.,0
'LUCENE-6824 would at least rewrite TAQ to simpler queries when possible.',1
'Commit 0c1cab71920a540807555501f7198ca402e16740 in lucene-solr\'s branch refs/heads/branch_6x from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0c1cab7 ]\nLUCENE-7401: Make sure BKD trees index all dimensions.',0
'Commit ba47f530d1165d4518569422472bc9e4f1c04b26 in lucene-solr\'s branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ba47f53 ]\nLUCENE-7401: Make sure BKD trees index all dimensions.',0
'Thanks Mike for having a look.',0
'I am looking into it.',0
'It does not seem to be related to BS1 this time since the test still fails when I disable BS1.',0
'This is a test bug.',0
'CheckHits assumes that if there is a single sub explanation, then its value is necessarily the same as the parent explanation.',1
'This fails with dismax when there is a single sub that produces a negative score since in that case it uses 0 as a max score and multiplies the score with the tie breaker factor.',1
'+1\nThank you for digging Adrien Grand!',0
'Commit c5defadd70a9f91bc31012b7c31c39f16d883849 in lucene-solr\'s branch refs/heads/branch_6x from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c5defad ]\nLUCENE-7352: Fix CheckHits for DisjunctionMax queries that generate negative scores.',0
'Commit 1e4d51f4085664ef073ecac18dd572b0a9a02757 in lucene-solr\'s branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1e4d51f ]\nLUCENE-7352: Fix CheckHits for DisjunctionMax queries that generate negative scores.',0
'Bulk close resolved issues after 6.2.0 release.',1
'I have been experimenting with the attached patch, which compresses doc ids based on the number of required bytes to store them (it only specializes 8, 16, 24 and 32 bits per doc id) and also adds delta-compression for blocks whose values are all the same.',1
'The IndexAndSearchOpenStreetMaps reported a slow down of 1.7\\\% for the box benchmark (72.3 QPS -> 71.1 QPS) but storage requirements decreased by 9.1\\\% (635MB -> 577MB).',1
'The storage requirements improve even more with types that require fewer bytes (LatLonPoint requires 8 bytes per value).',1
'For instance indexing 10M random half floats with the patch requires 28MB vs 43MB on master (-35\\\%).',1
'Updated patch.',1
'It now specializes both reading doc ids into an array and feeding a visitor, which seems to help get the performance back to what it is on master, or at least less than 1\\\% slower (not easy to distinguish minor slowdowns to noise at this stage).',1
'It has 3 cases:\n\nincreasing doc ids, which is expected to happen for either sorted segments or when all docs in a block have the same value.',1
'In that case, we delta-encode using vints.',1
'doc ids requiring less than 24 bits, which are encoded on 3 bytes.',1
'doc ids requiring less than 32 bits, which are encoded on 4 bytes like on master today.',1
'I think it\'s ready to go?',0
'I like this better than the last patch, I think the optimization is more general.',1
'I think in the base test class, tesMostEqual() is a mistake?',1
'Hmm I can remove both actually, they do not bring value now that the detection of whether doc ids are sorted is based on the doc ids themselves rather than the fact that there is a single value in a block.',1
'Commit 01de73bc0a1b315bbbe4df046b5c0661cec8de2e in lucene-solr\'s branch refs/heads/branch_6x from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=01de73b ]\nLUCENE-7351: Doc id compression for points.',0
'Commit d66e9935c39ed859659de46d3d5cfb66f2279bd4 in lucene-solr\'s branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d66e993 ]\nLUCENE-7351: Doc id compression for points.',0
'Bulk close resolved issues after 6.2.0 release.',1
'Simple patch:\n\nremoves GeoPointTestUtil from TestGeoPointQuery\nfixes a range corner case in GeoPointPrefixTermsEnum\nadds an explicit test for the corner case',1
+1,1
'Commit 1bbac6bbd896c110d08656f79fef3ce6d7828d6b in lucene-solr\'s branch refs/heads/branch_6_1 from Nicholas Knize\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1bbac6b ]\nLUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.',0
'Commit f767855da30e8d8b070b7566cb6eebb29af63334 in lucene-solr\'s branch refs/heads/master from Nicholas Knize\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f767855 ]\nLUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.',0
'Nicholas Knize Did you forget to cherry-pick to branch_6x?',0
'Commit 7448abb3bca7b8204e56a52fc115f7a2d813884d in lucene-solr\'s branch refs/heads/branch_6x from Nicholas Knize\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7448abb ]\nLUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.',0
'David Smiley Pinging you in case you want to have a chance to look into it before we release 6.1.',0
'FYI the seed still reproduces for me on master.',0
'No I didn\'t; I already caught that and will commit that unchanged.',0
'Commit b33d7176aa3624df2de1708b17919f20d034872f in lucene-solr\'s branch refs/heads/master from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b33d717 ]\nLUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes',0
'Commit 7520d79e040c16c5ab666f1ad28c8665fb0ceb40 in lucene-solr\'s branch refs/heads/branch_6x from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7520d79 ]\nLUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes\n(cherry picked from commit b33d717)',0
'Commit 6372c0b4042ec2c8d94e50e5c2b9c1df469414e2 in lucene-solr\'s branch refs/heads/branch_6_1 from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6372c0b ]\nLUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes\n(cherry picked from commit 7520d79)',0
'Reopening to backport to 6.0.2, 5.6 and 5.5.2',1
'Commit 5c546537d7b8130c05263832baff4946260f6a31 in lucene-solr\'s branch refs/heads/branch_5_5 from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5c54653 ]\nLUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes\n(cherry picked from commit 7520d79)',0
'Commit 1d7ad90947699e103de39fded5b78f76a30e449b in lucene-solr\'s branch refs/heads/branch_5_5 from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1d7ad90 ]\nLUCENE-7291: Add 5.5.2 CHANGES entry',0
'Commit f6b0fb95dea43f9f508b613cf32f489aaa263c4e in lucene-solr\'s branch refs/heads/branch_5x from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f6b0fb9 ]\nLUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes\n(cherry picked from commit 7520d79)',0
'Commit a7f2876ec5ce9ca5ef271cad97027a5cb5e43619 in lucene-solr\'s branch refs/heads/branch_6_0 from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a7f2876 ]\nLUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes\n(cherry picked from commit 7520d79)',0
'This patch changes the DocIdSetBuilder API.',1
'add() is gone.',1
'Instead, grow() returns a new BulkAdder object that can be used to add up to the number of documents that have been passed to the grow() method.',1
'This helps save conditionals since the add method never needs to care about whether the buffer is large enough or whether to upgrade to a bitset since everything is done up-front in the grow() call.',1
'One possible downside to this change is that it changes a predictable branch (that is handled at the CPU level) into a method call... which if it\'s not monomorphic can be un-inlined at the point of the call and thus end up slower (method call vs predictable branch).',1
'Will be interesting to see the benchmark results.',0
'I benchmarked it using IndexAndSearchOpenStreetMaps by temporarily using DocIdSetBuilder instead of MatchingPoints (I did not use luceneutil since its numeric range queries match too many docs).',1
'The QPS went from 33.4 (old DocIdSetBuilder.add) to 35.0 with this patch.',1
'In that case I think it works well since the base class is an abstract class and there are only two impls, which the JVM can efficiently optimize.',1
'(For the record, most queries of the benchmark upgrade to a bitset so both impls are used.)',1
'Ah, thanks for that reference... need to update my mental models',0
'Commit 95c360d053a35486aa12498c6fd319aef0377bb8 in lucene-solr\'s branch refs/heads/branch_6x from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=95c360d ]\nLUCENE-7264: Fewer conditionals in DocIdSetBuilder.',0
'Commit aa81ba8642a57181a4eaa017b52d0d3c3462544b in lucene-solr\'s branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=aa81ba8 ]\nLUCENE-7264: Fewer conditionals in DocIdSetBuilder.',0
'Commit 14b42fe10ba64bb4468ea8ef298e54a751f16dd3 in lucene-solr\'s branch refs/heads/branch_6x from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=14b42fe ]\nLUCENE-7264: Fix test bug in TestReqExclBulkScorer.',0
'Commit f7b333f10583639ee3d0f2631fee41c577c60452 in lucene-solr\'s branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f7b333f ]\nLUCENE-7264: Fix test bug in TestReqExclBulkScorer.',0
'Manually correcting fixVersion per Step #S5 of LUCENE-7271',1
'Another failure, on 6.x:\n\nSuite: org.apache.lucene.spatial3d.TestGeo3DPoint\n   [smoker]    [junit4] IGNOR/A 0.01s J2 | TestGeo3DPoint.testRandomBig\n   [smoker]    [junit4]    > Assumption #1: \'nightly\' test group is disabled (@Nightly())\n   [smoker]    [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestGeo3DPoint -Dtests.method=testRandomMedium -Dtests.seed=AB1C87AA82F2EF89 -Dtests.multiplier=2 -Dtests.locale=es-VE -Dtests.timezone=Africa/Niamey -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [smoker]    [junit4] FAILURE 2.55s J2 | TestGeo3DPoint.testRandomMedium <<<\n   [smoker]    [junit4]    > Throwable #1: java.lang.AssertionError: FAIL: id=8110 should have matched but did not\n   [smoker]    [junit4]    >   shape=GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=0.7742755548509384, lon=1.4555370543705286], [lat=-0.8402448215271041, lon=-3.1033465832913087]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.27564487296038953, lon=2.5713811980617303], [lat=-0.8678809123816704, lon=1.4382377289499255]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.8711205032246575, lon=-2.816839332961], [lat=-0.14706304488488503, lon=2.5605745305340144]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.8678809123816704, lon=1.4382377289499255], [lat=0.018587409980192347, lon=0.2620880396809507], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 1, 4}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.14706304488488503, lon=2.5605745305340144], [lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}]}\n   [smoker]    [junit4]    >   point=[X=-0.20200947969927532, Y=-0.4709330291599749, Z=0.8571474154523655]\n   [smoker]    [junit4]    >   docID=7962 deleted?=false\n   [smoker]    [junit4]    >   query=PointInGeo3DShapeQuery: field=point: Shape: GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=0.7742755548509384, lon=1.4555370543705286], [lat=-0.8402448215271041, lon=-3.1033465832913087]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.27564487296038953, lon=2.5713811980617303], [lat=-0.8678809123816704, lon=1.4382377289499255]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.8711205032246575, lon=-2.816839332961], [lat=-0.14706304488488503, lon=2.5605745305340144]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.8678809123816704, lon=1.4382377289499255], [lat=0.018587409980192347, lon=0.2620880396809507], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 1, 4}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.14706304488488503, lon=2.5605745305340144], [lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}]}\n   [smoker]    [junit4]    >   explanation:\n   [smoker]    [junit4]    >     target is in leaf _1(6.1.0):C20016 of full reader StandardDirectoryReader(segments:5:nrt _1(6.1.0):C20016)\n   [smoker]    [junit4]    >     full BKD path to target doc:\n   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.7975667675246342 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true\n   [smoker]    [junit4]    >       Cell(x=-0.7975667679908164 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true\n   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.01197272304385482 TO 0.35595394953475784); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true\n   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.3559539490685756 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true\n   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.6956355809209895); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true\n   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.6956355813871717 TO 0.10900276736191666); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true\n   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.10900276689573439 TO 0.7189540192817846); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true\n   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.7189540188156024 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true\n   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.68605594687831 z=-0.9977622808698339 TO -0.0069162292381555225); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true\n   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-0.6860559473444922 TO -0.32059454789968617 z=-0.9977622808698339 TO -0.0069162292381555225); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true\n   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.32059454789968617 z=-0.006916229704337817 TO 0.6718162758370178); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true\n   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.32059454789968617 z=0.6718162753708355 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true\n   [smoker]    [junit4]    >     on cell Cell(x=-1.0010902294672446 TO -0.7975667675246342 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true, wrapped visitor returned CELL_OUTSIDE_QUERY  on cell Cell(x=-0.7975667679908164 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true, wrapped visitor returned CELL_CROSSES_QUERY\n   [smoker]    [junit4]    > \tat __randomizedtesting.SeedInfo.seed([AB1C87AA82F2EF89:16C2B002C3978CEF]:0)\n   [smoker]    [junit4]    > \tat org.apache.lucene.spatial3d.TestGeo3DPoint.verify(TestGeo3DPoint.java:796)\n   [smoker]    [junit4]    > \tat org.apache.lucene.spatial3d.TestGeo3DPoint.doTestRandom(TestGeo3DPoint.java:518)\n   [smoker]    [junit4]    > \tat org.apache.lucene.spatial3d.TestGeo3DPoint.testRandomMedium(TestGeo3DPoint.java:445)\n   [smoker]    [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [smoker]    [junit4]   2> NOTE: test params are: codec=Asserting(Lucene60): {id=Lucene50(blocksize=128)}, docValues:{id=DocValuesFormat(name=Memory)}, maxPointsInLeafNode=421, maxMBSortInHeap=6.865543172299073, sim=ClassicSimilarity, locale=es-VE, timezone=Africa/Niamey\n   [smoker]    [junit4]   2> NOTE: Linux 3.13.0-52-generic amd64/Oracle Corporation 1.8.0_74 (64-bit)/cpus=4,threads=1,free=292169440,total=351797248\n   [smoker]    [junit4]   2> NOTE: All tests run in this JVM: [TestGeo3DPoint]',1
'Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:\n\n\n   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest\n   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()?',1
'false\n   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()?',0
'false\n   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()?',0
'false\n   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()?',0
'false\n   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()?',0
'false\n   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()?',0
'false\n   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()?',0
'false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512\n   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError\n   [junit4]    > \tat org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)\n\n\nBasically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.',1
'I\'m wondering if this is a result of granularity?',1
'Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid\'s boundaries.',1
'It does seem odd that all failures we\'ve seen have been for very complex polygons, though.',1
'I\'ll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.',1
'I verified that the shape appears to be properly built, with the right internal edges, and that when looking for edge intersections it examines the correct number and type of edges.',1
'The last thing to verify is whether the edge points for the XYZSolid are correct.',1
'Added more diagnostics to figure out if scaling the point to the surface would result in it leaving the cell.',1
'Short answer: no:\n\n\n   [junit4]    >       Cell(x=-1.0011088352687925 TO 1.0008262509460042 y=-1.000763585323458 TO 0.007099080851812687 z=0.9826918170382273 TO 0.9944024015823575); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true; Scaled point within cell = true; Scaled point within shape = true\n\n\nSo something is definitely wrong with the intersection logic, but I\'m not yet clear what that is.',1
'When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:\n\n\n   [junit4]   2> Is point within p1 section?',1
'false\n   [junit4]   2> Is point within p2 section?',0
'true\n   [junit4]   2> Is point within p3 section?',0
'false\n   [junit4]   2> Is point within concave section?',0
'false\n   [junit4]   2> Is point within p5 section?',0
'true\n\n\nThe point is within TWO parts of the composite polygon.',1
'This is, of course, nonsense unless the point happens to be on a shared edge between the two.',1
'But these two don\'t even share an edge.',1
'I\'m forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.',1
'I\'ll look at that next.',0
'Ok, found the problem.',0
'Cut and paste error in complex polygon building.',1
'Resolves the first problem, confirming the second.',1
'Commit d377e7fd34b4ace829ee6d4ba0486500aaef506b in lucene-solr\'s branch refs/heads/master from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d377e7f ]\nLUCENE-7197: Fix two test failures and add more forensics that helped resolve the issue.',0
'Commit 65a69b9757a6cd0a8e8be0ed08797643054634b1 in lucene-solr\'s branch refs/heads/branch_6x from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=65a69b9 ]\nLUCENE-7197: Fix two test failures and add more forensics that helped resolve the issue.',0
'I augmented Michael McCandless\'s explain output to include relationship information, which then leads to the following line in the forensics dump from this failure:\n\n\n   [junit4]    >       Cell(x=-1.0011088352687925 TO 1.0008262509460042 y=-1.000763585323458 TO 0.007099080851812687 z=0.9826918170382273 TO 0.9944024015823575); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true\n\n\nFrom this it looked plausible that the cell is off the top of the world in z, and thus did not intersect with it for that reason.',1
'But:\n\n\n   [junit4]    >   world bounds=( minX=-1.0011188539924791 maxX=1.0011188539924791 minY=-1.0011188539924791 maxY=1.0011188539924791 minZ=-0.9977622920221051 maxZ=0.9977622920221051\n\n\nSo I\'ll have to work a bit harder to see why no intersection is detected.',1
'Yay forensics!',0
'+1, we should roll our own, hopefully correctly this time',1
'It does seem buggy that even StrictMath shows this difference between versions.',1
'WTF is the point of StrictMath then?',1
'This is a simple multiplication, computed using two well-known constants: pi and 180.0.',0
'How can this possibly be inexact??',1
'See the linked openjdk issue.',0
'Looks like they changed it from division to inverse multiplication.',1
'IMO at least StrictMath should not have this optimization...',1
'IMO at least StrictMath should not have this optimization...\n+1 for that.',1
'StrictMath doesn\'t say anything about being constant in time, all it says is, basically:\nTo help ensure portability of Java programs, the definitions of some of the numeric functions in this package require that they produce the same results as certain published algorithms.',1
'[...] The Java math library is defined with respect to fdlibm version 5.3.',0
'And fdlibm doesn\'t have these conversion methods, so it\'s not violating its spec?',0
'Then why does StrictMath.java have a separate toRadians method at all with strictfp?',1
'That\'s what I\'m saying  I don\'t know!',0
'And seriously, I think it indeed misses the point: if StrictMath does have this method and the reference (fdlibm) doesn\'t have it then it should at least stick to identical implementation.',1
'I would file a bug.',0
'strictfp is another issue; in reality you won\'t be able to keep floating point computations exact (unless you declare everything as strictfp) because of processor-dependent truncations and roundings?',1
'I recall we did hit it once (a long time ago) when we tested on sparcs, amds and intels in parallel  we had reference results of matrix computations (in high precision) and there was some inaccuracies in tiny digits.',1
'I know what it does, i am saying it makes no sense to have the method there at all if its result may change.',1
'Commit a4bf526a62dbf5e2c3fed6d98112c71ed33e15d6 in lucene-solr\'s branch refs/heads/master from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a4bf526 ]\nLUCENE-7194: Roll our own toRadians() method, and also make it less likely we\'ll need to restaple the toString() tests.',0
'Commit f6be813308133de06e08717309750e1f47dd73d1 in lucene-solr\'s branch refs/heads/branch_6x from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f6be813 ]\nLUCENE-7194: Roll our own toRadians() method, and also make it less likely we\'ll need to restaple the toString() tests.',0
'+1 to rolling our own in any case.',1
'Committed changes to the spatial3d module for this purpose.',1
'Not sure where else it\'s used?',1
'Thanks Karl Wright.',0
'I think we should just add this to our forbidden API list, then see what fails (because it\'s using these APIs), and correct them...',1
'Michael McCandless How do I add these to the forbidden API list?',0
'Karl Wright Oh, I think you just add it to lucene/tools/forbiddenApis/lucene.txt?',0
'And then run ant precommit and you should see failures from places using these APIs...',0
'Here\'s what it spits out:\n\n\n[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]\n[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:94)\n[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]\n[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:95)\n[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]\n[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)\n[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]\n[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)\n[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]\n[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)\n[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]\n[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)\n[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]\n[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:151)\n[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]\n[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:169)\n[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]\n[forbidden-apis]   in org.apache.lucene.util.SloppyMath (SloppyMath.java:212)\n[forbidden-apis] Scanned 2733 (and 585 related) class file(s) for forbidden API\ninvocations (in 2.98s), 9 error(s).',0
'Robert Muir: Is this still needed?',0
'In SloppyMath.java I see the following:\n\n\n  // haversin\n  // TODO: remove these for java 9, they fixed Math.toDegrees()/toRadians() to work just like this.',1
'public static final double TO_RADIANS = Math.PI / 180D;\n  public static final double TO_DEGREES = 180D / Math.PI;\n\n\n... which leads me to wonder if Java 9 was fixed and we should instead be using Math.toDegrees()/toRadians() everywhere?',1
'Maybe Uwe Schindler knows?',0
'Here\'s the patch',1
'Commit b11e48c7553daed127b1b231641d7367a09aed1b in lucene-solr\'s branch refs/heads/master from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b11e48c ]\nLUCENE-7194: Ban Math.toRadians and Math.toDegrees',0
'Commit 40ca6f1d64ab8ec2e873c2a6c6815ca449b046a4 in lucene-solr\'s branch refs/heads/branch_6x from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=40ca6f1 ]\nLUCENE-7194: Ban Math.toRadians and Math.toDegrees',0
'Commit 388d388c990c8d9e05ec9ba9bc881fdc921e734b in lucene-solr\'s branch refs/heads/branch_6x from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=388d388 ]\nLUCENE-7194: Ban Math.toRadians, Math.toDegrees',0
'Thank you Karl Wright!',0
'Yikes, I\'ll take it.',0
Patch.,0
'I also hit and fixed a separate bug in InetAddressPoint.newSetQuery where it hit an exception if you tried to pass more than one InetAddress in the set   Sheesh.',1
's/comparsion/comparison/ but otherwise +1 to the patch',1
'nice find!',0
'Commit 770e508fd3d908e9bf7997361299af96aa437b75 in lucene-solr\'s branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=770e508 ]\nLUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal',0
'Commit 9f8fe1239afb7089b9f85432d076bdd778d3cd50 in lucene-solr\'s branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9f8fe12 ]\nLUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal',0
'Commit f0ed113bb6ebed008bc9aa5954e12de98d62c951 in lucene-solr\'s branch refs/heads/branch_6_0 from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f0ed113 ]\nLUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal',0
'What a nice catch, thanks Adrien Grand!',0
'I just want to be sure that maybe others also tested it with his/her Git installation.',0
'Commit 424a647af4d093915108221bcd4390989303b426 in lucene-solr\'s branch refs/heads/master from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=424a647 ]\nLUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change',0
'Commit 0ef36fcdd107084a2ac3156943f01eb5f7dd9c74 in lucene-solr\'s branch refs/heads/branch_5x from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0ef36fc ]\nLUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change',0
'I committed this.',1
'I think we should open another issues about the multiple build dir change.',1
'Commit b0e769c3ec598dd7398cc8df123bc4c41069e2c3 in lucene-solr\'s branch refs/heads/branch_5_4 from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b0e769c ]\nLUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change',0
Patch.,0
'this is a really good idea.',1
'otherwise we are going to get bug reports that look like data corruption:\n\n   [junit4]    > Caused by: java.lang.IllegalArgumentException: Version is too old, should be at least 2 (got 0)\n   [junit4]    >     at org.apache.lucene.util.packed.PackedInts.checkVersion(PackedInts.java:77)\n   [junit4]    >     at org.apache.lucene.util.packed.PackedInts.getDecoder(PackedInts.java:742)\n   [junit4]    >     at org.apache.lucene.codecs.lucene50.ForUtil.<clinit>(ForUtil.java:64)\n   [junit4]    >     ... 50 more',1
'This is interesting because I don\'t see this behavior.',0
'When I checkout a historical branch, it just updates timestamps on changed files in the working copy, but updates them to checkout time?',0
'$ ls -l build.xml\n-rw-r--r-- 1 dweiss dweiss 35579 Jan 26 14:55 build.xml\n\n# historical hash from Sep. 5:\n$ git checkout 8f5259b4ff2d5f0\n$ ls -l build.xml\n-rw-r--r-- 1 dweiss dweiss 25633 Jan 26 14:56 build.xml',0
'I think it\'s your build folders that are in an insane state, not the checked out files?',0
'I\'m not against this patch, I just find it odd you\'re experiencing the issue because I\'ve never had any problem with it (yes, git will not wipe out any ignored files automatically  these are ignored after all  but it\'ll switch any versioned files and update timestamps properly).',1
'Please make this an opt-out feature.',1
'I prefer to git clean the build folder myself, actually (faster than ant).',1
'It happened to me twice this morning already.',0
'Just doing things like running tests and forgetting to clean when switching branches.',0
'Ok, but this isn\'t related to timestamps on version-controlled files, only on the fact that build artefacts coexist?',0
'If so, it\'d be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder.',1
'This way you could actually switch a branch and then continue working as usual because it should work flawlessly.',1
'...have branch-dedicated build folder\nI like that idea!',1
'I guess it\'s manually possible now with -Dbuild.dir=my_branch_dir?',1
'If so, it\'d be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder.',1
'This way you could actually switch a branch and then continue working as usual because it should work flawlessly.',1
'This is an interesting idea.',1
'But if we do this, I think it should be `build/<branch>` and ant clean still removes build entirely (means it still really cleans).',1
'I guess it will force us to fix places in the build/python scripts/whatever that might have a hardcoded `build/` somewhere, and so on.',1
'Yes, absolutely.',1
'Ant clean should really clean everything.',1
'As a side note, I use this very often:\n\n\ngit clean -xfd\n\n\nThis removes everything (including all ignored files, etc.)',1
'and restores a pristine checkout state.',1
'Just in case somebody finds it useful.',0
'+1 to close this trap ASAP.',1
'The errors you hit when ant doesn\'t realize you\'ve switched git branches are awful.',1
'Can we iterate by committing this solution first, then working on the separate build directory as a followup.',1
'Again, the current situation is a real problem, because the errors you see look like corruption.',1
'I have no problem committing and pushing this soon.',0
'Patch of 11 Nov 2015.',1
'Most of the changes are to pass numDocs down to where it is actually used:\nConjunctionDISI, DisjunctionDISIApproximation, DisjunctionScorer, ConjunctionSpans, SpanOrQuery.',1
'This is incomplete, there no tests.',1
'MinShouldMatchSumScorer only has the disjunctions done.',1
'For un/ordered NearSpans there is a division by 4 (unordered) and by 8 (ordered) for zero allowed slop, something like this should also be done for the PhraseQueries.',1
'SpanContaining and SpanWithin use the conjunction estimation, these should also be smaller.',1
'The independence that is assumed is normally not there.',1
'However, the cost() results are only used to order the input DISIs/Scorers for optimization, and for that I expect this assumption to work nicely.',1
'But so would the current worst-case approach?',1
'That one is actually solved nowadays by the two phase approach.',1
'I\'ll think of a better example later.',0
'There are very likely situations where you can still decrease query runtime even further with a different order of clauses than the one based on current worst-case estimates, and I agree that the naming \'cost()\' doesn\'t really reflect the conservative estimates.',1
'However, any other non-worst-case estimate might err very badly and make queries that are currently reasonably fast extremely slow.',1
'It comes down to trading in worst-case behavior to gain average/throughput, but usually people care more about the slowest/hardest queries.',1
'However, maybe we can have worst-case and other estimates too and choose to use the latter only in cases where even making the wrong decision won\'t be too bad, so that you\'re speculative on the fast queries to gain throughput, but conservative on potentially slow queries.',1
'Another reason why I started this is that the result of cost() is also used as weights for matchCost() at LUCENE-6276, and I\'d prefer those weights to be as accurate as reasonably possible.',1
'I think we can keep this (assuming independence for conjunctions and disjunctions) as a possible alternative until the current implementation gives a bad result.',1
'For the proximity queries (Phrases, Spans) this reduces the conjunction cost() using the allowed slop.',1
'Would it be worthwhile to open a separate issue for that?',0
'Patch of 15 Nov 2015.',1
'Resolve conflicts after LUCENE-6276.',1
'I tried this patch with the wikimedium5m benchmark.',1
'This showed no significant differences to current trunk, the differences where never bigger than 2.1\\\% either way, and well within the standard deviations.',1
'This could be because the patch here should have influence for more complex queries than the one in the benchmark.',0
'I might try to add more complex queries to the benchmark later.',1
'Here is the benchmark output, it might be good for future reference:\n\n\n                    TaskQPS baseline      StdDevQPS my_modified_version      StdDev                Pct diff\n                HighTerm      178.20      (1.8\\\%)      174.50      (5.6\\\%)   -2.1\\\% (  -9\\\% -    5\\\%)\n                 MedTerm      641.36      (1.6\\\%)      630.32      (4.9\\\%)   -1.7\\\% (  -8\\\% -    4\\\%)\n              OrHighHigh       57.02      (5.5\\\%)       56.32      (6.6\\\%)   -1.2\\\% ( -12\\\% -   11\\\%)\n               OrHighMed      107.80      (5.2\\\%)      106.89      (6.2\\\%)   -0.8\\\% ( -11\\\% -   11\\\%)\n             AndHighHigh      100.02      (2.3\\\%)       99.34      (0.7\\\%)   -0.7\\\% (  -3\\\% -    2\\\%)\n                 LowTerm     2477.28      (3.0\\\%)     2463.27      (5.5\\\%)   -0.6\\\% (  -8\\\% -    8\\\%)\n              AndHighMed      627.58      (1.5\\\%)      625.22      (1.2\\\%)   -0.4\\\% (  -3\\\% -    2\\\%)\n              HighPhrase       81.21      (4.2\\\%)       80.98      (4.3\\\%)   -0.3\\\% (  -8\\\% -    8\\\%)\n               OrHighLow      136.70      (3.1\\\%)      136.35      (2.1\\\%)   -0.3\\\% (  -5\\\% -    5\\\%)\n               LowPhrase      181.55      (2.2\\\%)      181.09      (2.0\\\%)   -0.3\\\% (  -4\\\% -    4\\\%)\n         MedSloppyPhrase       56.03      (2.9\\\%)       55.93      (3.1\\\%)   -0.2\\\% (  -5\\\% -    5\\\%)\n             MedSpanNear       52.77      (1.7\\\%)       52.68      (2.6\\\%)   -0.2\\\% (  -4\\\% -    4\\\%)\n         LowSloppyPhrase      106.15      (2.9\\\%)      106.01      (3.1\\\%)   -0.1\\\% (  -6\\\% -    6\\\%)\n               MedPhrase       39.38      (3.8\\\%)       39.36      (3.3\\\%)   -0.1\\\% (  -6\\\% -    7\\\%)\n                  Fuzzy1      137.14      (2.1\\\%)      137.06      (1.5\\\%)   -0.1\\\% (  -3\\\% -    3\\\%)\n                  Fuzzy2       79.28      (1.9\\\%)       79.25      (1.5\\\%)   -0.0\\\% (  -3\\\% -    3\\\%)\n             LowSpanNear       94.38      (1.7\\\%)       94.35      (2.8\\\%)   -0.0\\\% (  -4\\\% -    4\\\%)\n            OrNotHighMed      444.12      (1.7\\\%)      444.36      (1.2\\\%)    0.1\\\% (  -2\\\% -    2\\\%)\n              AndHighLow     1878.59      (2.0\\\%)     1880.20      (1.9\\\%)    0.1\\\% (  -3\\\% -    4\\\%)\n                 Respell      106.47      (1.9\\\%)      106.62      (1.7\\\%)    0.1\\\% (  -3\\\% -    3\\\%)\n            OrNotHighLow     1831.85      (1.7\\\%)     1834.68      (1.3\\\%)    0.2\\\% (  -2\\\% -    3\\\%)\n           OrNotHighHigh       69.75      (1.6\\\%)       69.91      (1.4\\\%)    0.2\\\% (  -2\\\% -    3\\\%)\n            HighSpanNear       36.38      (2.8\\\%)       36.47      (3.8\\\%)    0.3\\\% (  -6\\\% -    7\\\%)\n        HighSloppyPhrase       45.58      (3.6\\\%)       45.70      (3.5\\\%)    0.3\\\% (  -6\\\% -    7\\\%)\n            OrHighNotLow       65.78      (7.0\\\%)       66.03      (8.4\\\%)    0.4\\\% ( -14\\\% -   16\\\%)\n                 Prefix3      448.85      (3.5\\\%)      450.67      (3.8\\\%)    0.4\\\% (  -6\\\% -    8\\\%)\n                Wildcard      114.35      (4.8\\\%)      115.02      (4.6\\\%)    0.6\\\% (  -8\\\% -   10\\\%)\n                  IntNRQ       23.48      (7.4\\\%)       23.71      (7.7\\\%)    1.0\\\% ( -13\\\% -   17\\\%)\n                PKLookup      360.70      (1.7\\\%)      364.91      (3.1\\\%)    1.2\\\% (  -3\\\% -    6\\\%)\n            OrHighNotMed      178.99      (7.2\\\%)      181.91      (8.2\\\%)    1.6\\\% ( -12\\\% -   18\\\%)\n           OrHighNotHigh       39.78      (7.1\\\%)       40.63      (7.5\\\%)    2.1\\\% ( -11\\\% -   18\\\%)',0
'Won\'t this change have the prospect of increasing the amount of GC due to all these extra objects?',1
'The patch also removes some calls to BytesRef.deepCopyOf, so I\'m not sure here.',1
'have an alternative constructor that doesn\'t clone\nThis has bitten a few people at least, including me, and I\'d rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.',1
'The Term(String field, String text) constructor makes a new BytesRef anyway.',1
'so that users like Solr can exploit the fact that their code won\'t be making any further use of the input term?',1
'There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher.',1
'There this call could be removed also.',1
'I\'m not familiar with Solr code, so in case there is another impact there, please holler.',0
'2nd patch of 3 October 2015.',1
'In addition to the previous patch, this also\n\ndeletes the Term cloning in PhraseQuery,\nadds a Term constructor from a BytesRefBuilder, and\nremoves BytesRef copying at Term construction from Solr\'s FieldType, SolrIndexSearcher, FacetField and SimpleMLTQParser.',1
'I like how the patch makes things simpler.',1
'I\'ll wait a bit before committing to give other people a chance to comment.',0
'Can you also remove the explicit cloning that we added in LUCENE-6435?',1
'Can you also remove the explicit cloning that we added in LUCENE-6435?',1
'I tried, but then a test fails.',1
'Perhaps this is because a BytesRef is passed ClassificationResult there.',1
'do you mean the BytesRef.deepCopyOfat https://github.com/apache/lucene-solr/blob/trunk/lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java#L154 ?',0
'If yes, that\'s because the reference is updated and used in the ClassificationResult.',1
'I\'ll see if I can simplify that.',1
'after a quick look it doesn\'t seem removing the deep copy in favour of creating new BytesRef would improve anything, actually it\'d be slightly worse.',1
'I would say let\'s keep that.',1
'One could also create the Term in the loop and pass that, or its Term.bytes(), around to the other methods.',1
'Term.bytes() can also be passed to the ClassificationResult.',1
'The patch here has this javadoc at Term.bytes():\n/** Returns the bytes of this term, these should not be modified.',1
*/,0
'One could also create the Term in the loop and pass that\ngood point indeed, in the end the underlying methods all create a new Term from the same BytesRefand field name, so that should be better than the current solution, so we should pass the Termcreated from within the loop to the methods to calculate prior and likelihood in SimpleNaiveBayesClassifier.',1
'attached the Paul Elschot\'s patch modified to pass the Terminstead of the BytesRefin SimpleNaiveBayesClassifier.',1
'from my perspective we can proceed committing this patch.',1
'Adrien, Paul what do you think?',0
'The patch of 14 October LGTM, and tests pass here.',1
'+1 on my end as well',1
'I will run another round of testing and inspections and commit the latest patch if no issues come up.',1
'Commit 1709576 from Tommaso Teofili in branch \'dev/trunk\'\n[ https://svn.apache.org/r1709576 ]\nLUCENE-6821 - TermQuery\'s constructors should clone the incoming term',0
'committed and resolved, thanks Paul Elschot for your patch and Adrien Grand for your help.',1
'Do we want to backport it to 5.x?',1
'I don\'t have a strong opinion but I like to get changes into the hands of our users as soon as possible, and I don\'t see how this one could break existing applications, it might just perform a few extra copies?',1
'Do we want to backport it to 5.x?',1
'+1, I consider this a bug fix, which should certainly go back to 5.x.',1
'agreed, I\'ve reopened and will backport it.',1
'Commit 1709683 from Yonik Seeley in branch \'dev/trunk\'\n[ https://svn.apache.org/r1709683 ]\nLUCENE-6821: remove unnecessary term clones',0
'Commit 1709780 from Tommaso Teofili in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1709780 ]\nLUCENE-6821 - TermQuery\'s constructors should clone the incoming term (backport branch 5.x)',0
'backported to branch 5.x',1
'See (also?)',0
LUCENE-4483.,0
'I had a look at the core code for the use of the TermQuery constructors, and I agree that it would be better to do the clone in the constructor.',1
'Shall I give this a try?',1
'Patch of 3 Oct 2015.',1
'This\n\nadds a call to BytesRef.deepCopyOf in the Term constructor,\nremoves such calls where the Term constructor is used, and\ndocuments that the result of Term.bytes() should not be modified.',1
'The patch also removes a call to the term constructor in BlendedTermQuery, which was actually making a clone.',1
'This might have gone too far, but I think it should work because the boost is in BoostQuery now.',1
'Won\'t this change have the prospect of increasing the amount of GC due to all these extra objects?',1
'Maybe might it be advisable to have an alternative constructor that doesn\'t clone so that users like Solr can exploit the fact that their code won\'t be making any further use of the input term?',1
'I don\'t think we should bother at all: executing a term query already performs I/O operations and allocates several objects per segment to create terms enums, scorers, iterators, leaf collectors, etc.',1
'so adding two extra object allocations to clone the incoming term is very unlikely to have noticeable impact on gc activity.',1
'There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.',1
'Initial patch fixing the comparator to take into account the payload when everything else is same.',1
'Commit 1691282 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1691282 ]\nLUCENE-6680: don\'t lose a suggestion that differs only in payload from another suggestion',0
'Thanks Arcadius Ahouansou, I just committed your patch with a small tweak to the if statement logic in the comparator...',1
'Commit 1691283 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1691283 ]\nLUCENE-6680: don\'t lose a suggestion that differs only in payload from another suggestion',0
'Thank you very much Michael McCandless for helping get this issue fixed.',0
'Bulk close for 5.3.0 release',1
'We should define a complete standard\nLucene uses Sun\'s java coding conventions, apparently moved here: http://www.oracle.com/technetwork/java/codeconvtoc-136057.html\nWith one exception: 2 space indent, not 4.',1
'Currently by default the Codestyle is consistent with spaces.',1
'The problem was actually with un-consistent  classes already committed that were causing the confusion.',1
'We can close this now.',1
'And thanks Mike for the explanation, actually I missed the comments !',0
'You\'re welcome Alessandro Benedetti!',0
'Thank you for raising this.',0
'Patch guards the Files.createDirectories call with a Files.exists check.',1
'Files.exists only demands read access, so it succeeds if you only have read access.',1
'And your second patch: Please don\'t do this.',1
'This is all not worth a test.',1
'It just compromises our security manager.',1
'A SecurityManager that allows to override/delete itsself just makes itsself broken.',1
'Yes, if you really want to do this in a test:\nGuideline 9-4 / ACCESS-4: Know how to restrict privileges through doPrivileged\nhttp://www.oracle.com/technetwork/java/seccodeguide-139067.html#9',1
'Good tip!',1
'I have the test working here.',0
'Will post updated patch.',1
'Here is the test.',1
'I had to f*ck with the last Permission, because to run the code with restricted permissions you need the additional permission, otherwise it runs with no permissions at all  (see javadocs)\nI also added a helper method, we may move this to LTC later',1
'Slightly improved generics in the patch to make the helper method more universal (to move to LTC).',1
'I also added a helper method, we may move this to LTC later\n+1\nThis patch looks great Uwe, generalizing that method into LTC sounds good to me.',1
'More neat helper method (Java 8).',1
'I will blow this up to ten times its size when backporting to 5.x',0
'This patch looks great Uwe, generalizing that method into LTC sounds good to me.',1
'Will look into this tomorrow.',0
'Have to sleep now, its already 4 am.',0
'The time when Mike McCandless starts coding',0
'Moved the runWithLowerPermissions to LTC.',1
'I also fixed the FSDirectory() ctor to do a mich simpler Files.isDirectory() check, because otherwise the basic TestDirectory test fails on Windows (of couse it fails...).',1
'Could somebody with Linux or MacOSX test the patch or state any other complaints with it?',0
'More tests that we cannot escape our sandbox!',0
'After discussion with Hoss Man, I added an assume to the runWithRestrictedPermissions, so it cancels test execution if no security manager is available.',1
'Running those tests without a security manager makes no sense, because they would assert nothing (because they have all permissions).',1
'Commit 1688537 from Uwe Schindler in branch \'dev/trunk\'\n[ https://svn.apache.org/r1688537 ]\nLUCENE-6542: FSDirectory\'s ctor now works with security policies or file systems that restrict write access',0
'Commit 1688541 from Uwe Schindler in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1688541 ]\nMerged revision(s) 1688537 from lucene/dev/trunk:\nLUCENE-6542: FSDirectory\'s ctor now works with security policies or file systems that restrict write access',0
'Committed and backported the lambdas to Java 7 in branch_5x.',1
'Bulk close for 5.3.0 release',1
'Here is a patch.',0
+1,1
'Commit 1683734 from Adrien Grand in branch \'dev/trunk\'\n[ https://svn.apache.org/r1683734 ]\nLUCENE-6526: Asserting(Query|Weight|Scorer) now ensure scores are not computed if they are not needed.',0
'Commit 1683744 from Adrien Grand in branch \'dev/trunk\'\n[ https://svn.apache.org/r1683744 ]\nLUCENE-6526: Revert some changes that were committed by mistake.',0
'Commit 1683745 from Adrien Grand in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1683745 ]\nLUCENE-6526: Asserting(Query|Weight|Scorer) now ensure scores are not computed if they are not needed.',0
'Bulk close for 5.3.0 release',1
'Commit 1672281 from Dawid Weiss in branch \'dev/trunk\'\n[ https://svn.apache.org/r1672281 ]\nLUCENE-6413: Test runner should report the number of suites completed/ remaining.',0
'Commit 1672282 from Dawid Weiss in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1672282 ]\nLUCENE-6413: Test runner should report the number of suites completed/ remaining.',0
Woohoo!,1
'Thanks Dawid!',0
'You\'re very welcome although I am so far behind with other changes I\'d like to make to the RR that there\'s hardly any reason to celebrate  (yet .',0
'This is awesome.',1
'It won\'t make the tests go any faster, but now I will know whether I can walk away from the running tests to work on something else.',1
'Bulk close for 5.3.0 release',1
'Here is an initial shotgun approach.',1
'I verified it finds things like file leaks by introducing them to old codecs.',1
'But it currently sometimes trips assertions during IW.commit().',1
'I\'m not sure everything is ok here.',1
'I\'m not trying to test IW so I\'m gonna neuter it and keep working.',0
'False alarm (fake IOE -> slowFileExists == false -> scary assertion).',1
'I disable it during commit (with a comment) as a workaround.',1
'But its still on during reopen which does not have this check.',1
'I think its good enough.',1
'False alarm \nPhew!',0
'Commit 1671900 from Robert Muir in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1671900 ]\nLUCENE-6405: add shotgun test for exception handling',0
'Commit 1671902 from Robert Muir in branch \'dev/trunk\'\n[ https://svn.apache.org/r1671902 ]\nLUCENE-6405: add shotgun test for exception handling',0
'I\'ll close the issue, but we should improve it more later.',1
'Really each of these tests should have unit tests (using a MDW that failOn\'s openInput/createOutput/whereever).',1
'I think it would even better.',1
'But for now this is an improvement over the IW tests and fills missing coverage for backwards-codecs/',1
'Commit 1671995 from Robert Muir in branch \'dev/trunk\'\n[ https://svn.apache.org/r1671995 ]\nLUCENE-6405: add infos exc unit tests.',0
'fix TestTransactions to handle exceptions on openInput and let MDW do it',0
'Commit 1671996 from Robert Muir in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1671996 ]\nLUCENE-6405: add infos exc unit tests.',0
'fix TestTransactions to handle exceptions on openInput and let MDW do it',0
'Bulk close for 5.2.0.',1
'\nReport after iter 10:\nChart saved to out.png... (wd: /home/rmuir/workspace/util/src/python)\n                    Task   QPS trunk      StdDev   QPS patch      StdDev                Pct diff\n             MedSpanNear       75.69      (2.0\\\%)       80.58      (3.9\\\%)    6.5\\\% (   0\\\% -   12\\\%)\n             LowSpanNear      233.30      (3.8\\\%)      259.44      (6.5\\\%)   11.2\\\% (   0\\\% -   22\\\%)\n            HighSpanNear        9.43      (3.6\\\%)       10.76      (7.5\\\%)   14.0\\\% (   2\\\% -   25\\\%)',0
'Oops, I thought by now ArrayList would be JIT-ed away, thanks.',0
'Also the UOE\'s in the NearSpansOrdered payload methods have gone in this patch, I had put these in to check the tests.',1
'I removed the UOE because now the no-payload impl is used if a segment doesn\'t happen to have any payloads.',1
'But this is valid, the documents might just not have any.',1
'+1 too bad we can\'t expect ArrayList to always perform like a plain array',1
'Commit 1671078 from Robert Muir in branch \'dev/trunk\'\n[ https://svn.apache.org/r1671078 ]\nLUCENE-6388: Optimize SpanNearQuery',0
'Commit 1671081 from Robert Muir in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1671081 ]\nLUCENE-6388: Optimize SpanNearQuery',0
'For now the check is implemented via Terms.getPayloads() until LUCENE-6390 is fixed.',1
'Bulk close for 5.3.0 release',1
'I had started seeing if we could implicitely wrap with ConstantScorer but there seem to be a couple of places that call score() when needsScores is false.',1
'Here is another alternative that still stops calling matches() after the first matching clause is found but now also pretends there is a single matching clause so that score() won\'t fail.',1
'This is the smallest viable fix I can think of for https://builds.apache.org/job/Lucene-Solr-NightlyTests-5.x/775/.',1
'But we should probably think about better fixes for the future and avoid calling score() when it\'s not needed.',1
'I opened LUCENE-6330 to tackle BooleanScorer (and maybe others).',1
'Commit 1663756 from Adrien Grand in branch \'dev/trunk\'\n[ https://svn.apache.org/r1663756 ]\nLUCENE-6329: Calling score() should be ok even if needsScores is false.',0
'Commit 1663757 from Adrien Grand in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1663757 ]\nLUCENE-6329: Calling score() should be ok even if needsScores is false.',0
'Bulk close after 5.1 release',1
'Here was my first patch (moved from LUCENE-6320).',1
'Maybe we can add some fieldinfos tests to exercise this directly, and maybe there is a way to simplify the scary logic too.',1
'+1, this looks like an easy, possibly high-impact win.',1
'I think we can be more aggressive about using the array: TreeMap has much more than 50\\\% overhead, since it also needs Integer key and pointer to that key, and object overhead holding that key pointer and value pointer.',1
'I think we can safely do this opto when it\'s > 10\\\% of the space?',1
'Mike, if you have time, can you do calculations and adjust the patch?',0
'You can take the issue too, i had basically given up on this.',0
'OK I can try to update this & commit...',1
'New patch, using dense array when > 1/16th of the numbers are used:\nEach TreeMap$Entry has object header (8 or 16 bytes), 5 pointers (4 or\n8 bytes), and a boolean (likely rounded up to 4 bytes), times 2 for\nall the inner nodes of the tree, plus the overhead of Integer (object\nheader, int), so net/net each entry in the TreeMap costs 68 - 124 bytes.',1
'The array is 4 or 8 bytes per int.',1
'I think the patch is ready...',1
'Thanks, can you change 16 to 16L?',1
'I don\'t want to think about overflowing.',1
'Thanks, can you change 16 to 16L?',1
'Oh, good catch!',1
'I\'ll fix ...',1
'+1 to commit with that modification, thanks for doing the calculations here.',1
'Commit 1687789 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1687789 ]\nLUCENE-6325: use array for number -> FieldInfo lookup, except in very sparse cases',0
'Commit 1687792 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1687792 ]\nLUCENE-6325: use array for number -> FieldInfo lookup, except in very sparse cases',0
'Thanks Robert Muir!',0
'Bulk close for 5.3.0 release',1
'Patch that adds the MatchNoDocsQuery and uses it for empty SimpleQueryParser queries as well as when a BooleanQuery is rewritten and has no clauses.',1
'I think its confusing we have MatchAll but not MatchNone.',1
'I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses?',1
'Then it wouldn\'t need a weight and scorer.',1
'I think the 0x1AA71190 of MatchAllDocsQuery is here to avoid that all Query impls that only wrap a boost end up with the same hash code.',1
'Maybe a cleaner way to do it would be to return Float.floatToIntBits(getBoost()) ^ getClass().hashCode().',1
'I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses?',1
'+1 to rewrite to an empty BooleanQuery',1
+1,1
'New patch that changes MatchNoDocsQuery to rewrite to an empty BooleanQuery.',1
'Also removes the nocommit as per Adrien\'s suggestion',1
'is the hashcode/equals stuff needed here or can the superclass impls in Query be used?',1
'They seem to already have this logic.',1
'In the tests, i would add a call to QueryUtils.check(q) to one of your matchnodocsqueries.',1
'This will do some tests on hashcode/equals.',1
'Feels wrong to me to override hashCode but not equals.',1
'I think we should move this class part of hashCode() to Query.hashCode()?',1
(ie.,0
'return Float.floatToIntBits(getBoost()) ^ getClass().hashCode()\nIf it is controversial then I\'m happy with the previous patch that overrides both equals and hashCode().',1
'+1 Adrien.',1
'Adrien: I agree about having the hashCode.',1
'Here is a new patch that doesn\'t override equals or hashCode and changes Query to use the class in the hashCode method as Adrien suggested.',1
'Thanks Lee, I like this.',1
'When i see code overriding hashcode/equals and not calling super.hashcode/super.equals, its a bad sign.',1
'We should commit this one, and remove duplicated logic and hardcoded constants in e.g.',1
'TermQuery and all other places in a followup.',1
'Robert: +1, I opened LUCENE-6333 for this, I\'ll work on a patch.',1
'Commit 1663899 from Adrien Grand in branch \'dev/trunk\'\n[ https://svn.apache.org/r1663899 ]\nLUCENE-6304: Add MatchNoDocsQuery.',0
Committed.,1
'Thanks Lee!',0
'Commit 1663901 from Adrien Grand in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1663901 ]\nLUCENE-6304: Add MatchNoDocsQuery.',0
'Bulk close after 5.1 release',1
+1,1
'I added two more timings to the patch.',1
'here is the output on one of my wiki10m segments:\n\nsize (MB)=624.591\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_40-ea, lucene.version=6.0.0, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=3.13.0-43-generic, timestamp=1423097209630}\n    has deletions [delGen=6]\n    test: open reader.........OK [took 0.075 sec]\n    test: check integrity.....OK [took 1.515 sec]\n    test: check live docs.....OK [90031 deleted docs]\n    test: field infos.........OK [8 fields] [took 0.000 sec]\n    test: field norms.........OK [2 fields] [took 0.046 sec]\n    test: terms, freq, prox...OK [6844227 terms; 170452948 terms/docs pairs; 240913350 tokens] [took 13.171 sec]\n    test (ignoring deletes): terms, freq, prox...OK [7105194 terms; 179422787 terms/docs pairs; 253586353 tokens] [took 9.632 sec]\n    test: stored fields.......OK [5135307 total field count; avg 3.0 fields per doc] [took 4.648 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.036 sec]\n    test: docvalues...........OK [2 docvalues fields; 0 BINARY; 1 NUMERIC; 1 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.206 sec]\n\n\nMaybe check index should have a integrity-check only option as a followup.',1
'It would just be sugar to the user, but this would always be pretty fast.',1
'sorry, here is the correct patch.',1
'OK I noticed one case where live docs didn\'t confess how long it took \nI\'ll fix that and commit.',1
'Commit 1658831 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1658831 ]\nLUCENE-6233: speed up CheckIndex when the index has term vectors',0
'Commit 1658832 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1658832 ]\nLUCENE-6233: speed up CheckIndex when the index has term vectors',0
'Bulk close after 5.1 release',1
'This was introduced with LUCENE-5610\nI\'ll fix the nightly Lucene benchmark to plot CheckIndex time ... we could have spotted this performance regression.',1
'I think CheckIndex should not check Terms.getMin/Max for TVs?',1
'Patch attached + a couple of unit tests for allTermsRequired=false',1
'I didn\'t see this patch yet, but as I said in SOLR-6648, I think it makes sense to set those defaults in the constructor.',1
'Boon Low, could you update the patch to a recent version of trunk?',1
'Also, could you add javadocs to the newly added constructors?',1
'That patch was based upon and tested with the v4.10.3 release on Dec 20.',0
'But I can see that have been significant changes to AnalyzingInfixSuggester in the trunk.',0
'Shall update and test the patch tomorrow.',0
'patch updated w.r.t.',1
'trunk 05/01/15',0
'The patch looks good.',1
'I added some more tests to the new code and renamed the field highlighting->highlight',1
'Minor changes to the test and made the new fields private.',1
'I\'ll commit this soon',1
'Commit 1649893 from Toms Fernndez Lbbe in branch \'dev/trunk\'\n[ https://svn.apache.org/r1649893 ]\nLUCENE-6149: Infix suggesters\' highlighting and allTermsRequired can be set at the constructor for non-contextual lookup',0
'Commit 1649955 from Toms Fernndez Lbbe in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1649955 ]\nLUCENE-6149: Infix suggesters\' highlighting and allTermsRequired can be set at the constructor for non-contextual lookup',0
'Thanks Toms, good to see the patch making into the trunk and branch_5x.',1
'I shall find some time soon to update and post the v.4.10.3 patch to include your changes.',0
'There\'s a typo the test class: testConstructorDefatuls',0
'patch for v4.10.3 release',1
'Commit 1650132 from Toms Fernndez Lbbe in branch \'dev/trunk\'\n[ https://svn.apache.org/r1650132 ]\nLUCENE-6149: Fixed typo',0
'Commit 1650134 from Toms Fernndez Lbbe in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1650134 ]\nLUCENE-6149: Fixed typo',0
Fixed.,1
'Thanks Boon!',0
'Bulk close after 5.0 release.',1
'Simple patch + test.',1
'Thinking about this more ... it may be better to do this entirely inside a FilterDirectory.',1
E.g.,0
'when IndexOutput is closed, and the IOContext is not MERGE, increment the bytes written ... and then that same directory instance could dynamically update the target merge throttling ... maybe.',1
'I ran some tests with this approach and I think it\'s no good.',1
'This creates a tricky feedback system, where both CMS (via hard stalling of incoming threads) and this directory attempt to make change to let merges catch up.',1
'When CMS\'s hard stalls kick in, this lowers the indexing byte/sec rate, which causes this directory to (over simplistically) lower the merge IO throttling, which causes the merges to take longer.',1
'I think it\'s better if all throttling efforts happen in one place, e.g.',1
CMS.,0
'I\'l think about it ...',0
'great to see progress nuking checkabort!',1
'\n\n    // Defensive: sleep for at most 250 msec; the loop above will call us again if we should keep sleeping:\n    if (curPauseNS > 250L*1000000000) {\n      curPauseNS = 250L*1000000000;\n    }\n\n\nDid you mean 250 milliseconds or 250 seconds?',0
'+1, I really like this approach.',1
'Did you mean 250 milliseconds or 250 seconds?',0
Woops!,0
'I\'ll fix, thanks.',0
'I was never sure what a good value for the rate limiter would be so I\'m very happy to see Lucene take care of it by itself.',1
'+  /** true if we should rate-limit writes for each merge; false if not.',0
'null means use dynamic default: */\n+  private boolean doAutoIOThrottle = true;\n\n\nI think the comment is outdated since doAutoIOThrottle is a boolean now (instead of a Boolean)?',1
'There is a similar leftover a couple of lines below I think: if (doAutoIOThrottle == Boolean.TRUE)\n\n\n+    /** Set by {@link IndexWriter} to rate limit writes and abort this merge.',1
'*/\n+    public final MergeRateLimiter rateLimiter;\n\n\nI think the comment is a bit confusing since this property is not actually set by the index writer?',1
'/** Returns 0 if no pause happened, 1 if pause because rate was 0.0 (merge is paused), 2 if paused with a normal rate limit.',0
'*/\n  private synchronized int maybePause(long bytes, long curNS) throws MergePolicy.MergeAbortedException\n\n\nMaybe having constants or an enum would make the code easier to read?',1
'Thanks Adrien Grand, here\'s a new patch with those fixes.',1
'Commit 1649532 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1649532 ]\nLUCENE-6119: CMS dynamically rate limits IO writes of each merge depending on incoming merge rate',0
'Commit 1649539 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1649539 ]\nLUCENE-6119: CMS dynamically rate limits IO writes of each merge depending on incoming merge rate',0
'Commit 1650025 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1650025 ]\nLUCENE-6119: fix just arrived merge to throttle correctly',0
'Commit 1650026 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1650026 ]\nLUCENE-6119: fix just arrived merge to throttle correctly',0
'Commit 1650027 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1650027 ]\nLUCENE-6119: fix just arrived merge to throttle correctly',0
'Commit 1650463 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1650463 ]\nLUCENE-6119: set initial rate for forced merge correctly',0
'Commit 1650464 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1650464 ]\nLUCENE-6119: set initial rate for forced merge correctly',0
'Commit 1650594 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1650594 ]\nLUCENE-6119: must check merge for abort even when we are not rate limiting; don\'t wrap rate limiter when doing addIndexes (it\'s not abortable); don\'t leak file handle when wrapping',0
'Commit 1650595 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1650595 ]\nLUCENE-6119: must check merge for abort even when we are not rate limiting; don\'t wrap rate limiter when doing addIndexes (it\'s not abortable); don\'t leak file handle when wrapping',0
'Commit 1651305 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1651305 ]\nLUCENE-6119: make sure minPauseCheckBytes is set on init of MergeRateLimiter',0
'Commit 1651307 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1651307 ]\nLUCENE-6119: make sure minPauseCheckBytes is set on init of MergeRateLimiter',0
'I realize this issue is closed but it\'d be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well.',1
'Let me explain.',0
'When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing).',1
'Essentially we want to maximize a function:\n\n\nf = merge_throughput + indexing_throughput\n\n\nperhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?).',1
'The underlying variables to adaptively tune are:\n\nhow many merge threads there are (for example having too many doesn\'t make sense on a spindle, with an SSD this is not a problem),\nwhen to pause/ resume existing merge threads,\nwhen to pause/ resume indexing threads.',1
'What\'s interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):\n\nreact to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),\n\n\nfind out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).',1
'Now the big question is what this algorithm should look like, of course.',1
'The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms.',1
'I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are).',1
'I\'ll report my impressions once I have it done.',0
'I think auto-tuning merge thread count would be a great addition!',1
'I know.',0
'It would take a lot of manual tuning or detection (ssd vs. non-ssd vs. hybrid vs. large mem disk buffers, etc.)',1
'off the map.',1
'And it could gracefully play with other components of the system without clogging everything (like ionice).',1
'We\'ll see.',0
'Bulk close after 5.0 release.',1
'Patch against branch_5x',1
Oops.,0
'Here is the real patch against branch_5x.',1
'+1, looks good.',1
'Commit 1636025 from Ryan Ernst in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1636025 ]\nLUCENE-6043: Fix backcompat support for UAX29URLEmailTokenizer',0
'Commit 1636074 from Ryan Ernst in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1636074 ]\nLUCENE-6043: Forgot to svn add the new file',0
'Bulk close after 5.0 release.',1
'Committed to trunk and branch_5x.',1
'Thanks again for reporting, Ilia Sretenskii!',0
'Unfortunately I can not help you with that, guys.',0
'My experience is limited with Apache Maven and I honestly have no idea about how Apache Ant and Apache Ivy work at all.',0
'Isn\'t that just too many of different dependency managers to mess with, that are causing their own conflicts troubles?',1
'+1 for backport to 4.10.x',1
'Reopening to backport to the 4.10 branch',1
'Committed to lucene_solr_4_10.',1
'Thanks Jan. - Steve',0
'updated description with work around for older sources',1
'Can you post a test case which fails with this exception?',0
'I\'m afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.',0
'Basically what I do is I\'m calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.',1
'Of 26 segments that my index have, only one is coming with NullPointerException.',1
'If I send 4 instead of 5 terms, it works properly.',1
'Also if I leave out the NOT term, it works with any number of terms.',1
'I\'m sorry I couldn\'t be of more help.',0
'If you tell me where to look and how to debug I can do it and post the results.',0
'I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception .',1
'In any way, this unchecked exception should probably be caught somewhere.',1
'If ReqExclScorer has a null \'req\' part, then something in the logic of BooleanWeight.',1
'ok I see the bug.',0
'ReqExclScorer can indeed be buggy here, because it marks \'req\' as null for \'exhausted\'.',1
'So if you call cost() after that, you will get NPE.',1
'We\'ve been seeing this issue semi-regularly in our app.',1
'We\'ve been working around it so far by simplifying our queries to remove clauses (probably a good idea anyway) but we can never be sure our users won\'t find a way to break it!',1
'this bug was fixed on trunk when reqScorer was modified to no longer be set to null\nthis patch is against 5.0, where ReqScorer can hit NPE if cost is called after nextDoc\npatch includes test and fix to call cost before nextDoc',1
'Commit 1662673 from Michael McCandless in branch \'dev/branches/lucene_solr_4_10\'\n[ https://svn.apache.org/r1662673 ]\nLUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches',0
'Commit 1662674 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1662674 ]\nLUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches',0
'Thank you Dragan and jane!',0
'Commit 1662681 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1662681 ]\nLUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches',0
'Commit 1662728 from Michael McCandless in branch \'dev/branches/lucene_solr_4_10\'\n[ https://svn.apache.org/r1662728 ]\nLUCENE-6001: null check was backwards',0
'Commit 1662732 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1662732 ]\nLUCENE-6001: null check was backwards',0
'Commit 1662733 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1662733 ]\nLUCENE-6001: null check was backwards',0
'Bulk close for 4.10.4 release',1
'GitHub user andyetitmoves opened a pull request:\n https://github.com/apache/lucene-solr/pull/96\n    Explicitly stop beast from running on top-level modules\n    Patch for LUCENE-5968\nYou can merge this pull request into a Git repository by running:\n    $ git pull https://github.com/bloomberg/lucene-solr trunk-beast-error\nAlternatively you can review and apply these changes as the patch at:\n https://github.com/apache/lucene-solr/pull/96.patch\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n    This closes #96\n\ncommit 79578f7cd4825d3d0d6700c4ec5581374216ac6f\nAuthor: Ramkumar Aiyengar <andyetitmoves@gmail.com>\nDate:   2014-09-21T07:33:59Z\n    Explicitly stop beast from running on top-level modules',0
'Uwe Schindler, Hoss Man: The pull request above has changes suggested by both, could you check and commit?',0
'Github user uschindler commented on the pull request:\n https://github.com/apache/lucene-solr/pull/96#issuecomment-56595072\n    Hi, see comments on https://issues.apache.org/jira/browse/LUCENE-5968 !',0
Done..,0
'Hey Uwe Schindler, I have addressed your comments, could this be merged in?',0
Thanks!,0
'Sorry,\nI missed this issue.',0
'I will commit in a moment.',1
Sorry.,0
Uwe,0
'Commit 1642488 from Uwe Schindler in branch \'dev/trunk\'\n[ https://svn.apache.org/r1642488 ]\nLUCENE-5968: Improve error message when \'ant beast\' is run on top-level modules\nThis closes #96',0
'Commit 1642489 from Uwe Schindler in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1642489 ]\nMerged revision(s) 1642488 from lucene/dev/trunk:\nLUCENE-5968: Improve error message when \'ant beast\' is run on top-level modules\nThis closes #96',0
'Github user asfgit closed the pull request at:\n https://github.com/apache/lucene-solr/pull/96',0
'Bulk close after 5.0 release.',1
+1,1
'Patch requiring resourceDescription (either datainput, or string).',1
'We had quite a few places missing this.',1
'+1 for this!',1
'+1, looks awesome.',1
'Commit 1626372 from Robert Muir in branch \'dev/trunk\'\n[ https://svn.apache.org/r1626372 ]\nLUCENE-5965: CorruptIndexException requires a String or DataInput resource',0
'Commit 1626375 from Robert Muir in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1626375 ]\nLUCENE-5965: CorruptIndexException requires a String or DataInput resource',0
'Bulk close after 5.0 release.',1
'Patch with suggested changes.',1
'Thanks Markus, looks great, I\'ll commit shortly.',1
'Commit 1626241 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1626241 ]\nLUCENE-5963: more efficient AnalyzingSuggester.replaceSep',0
'Commit 1626242 from Michael McCandless in branch \'dev/branches/branch_5x\'\n[ https://svn.apache.org/r1626242 ]\nLUCENE-5963: more efficient AnalyzingSuggester.replaceSep',0
'Thanks Markus!',0
'Bulk close after 5.0 release.',1
'Patch, starting from Vitaly\'s test case (thank you!)',1
'and folding into Lucene\'s tests ... it fails with this on trunk:\n\n1) testReopenReaderToOlderCommit(org.apache.lucene.index.TestDirectoryReaderReopen)\njava.lang.IllegalStateException: same segment _0 has invalid changes; likely you are re-opening a reader after illegally removing index files yourself and building a new index in their place.',1
'Use IndexWriter.deleteAll or OpenMode.CREATE instead\n\tat __randomizedtesting.SeedInfo.seed([D3F22B13D5839643:931C8A9673D003F4]:0)\n\tat org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:190)\n\tat org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:323)\n\tat org.apache.lucene.index.StandardDirectoryReader$2.doBody(StandardDirectoryReader.java:317)\n\tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)\n\tat org.apache.lucene.index.StandardDirectoryReader.doOpenFromCommit(StandardDirectoryReader.java:312)\n\tat org.apache.lucene.index.StandardDirectoryReader.doOpenNoWriter(StandardDirectoryReader.java:308)\n\tat org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:259)\n\tat org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:137)\n\tat org.apache.lucene.index.TestDirectoryReaderReopen.testReopenReaderToOlderCommit(TestDirectoryReaderReopen.java:824)',1
'Patch: I think the use-case is cool and it should be supported: its just adding an \'if\' and removing the current exception (which is geared at protecting some user who manually rm -rf\'s files from their index.)',1
'I improved the test a bit to ensure that cores are shared and also tested the dv updates case.',1
'Thanks Rob, patch looks great, except: I think we can keep the illegalDocCountChange safety?',1
'I think I could make a test case to trip that ...',1
'Can you make a test change to trip it without manually removing files from your index?',0
'Can you make a test change to trip it without manually removing files from your index?',0
'I don\' t think so ... the only way I know of this happening is if an app has a reader open, then removes the index, rebuilds it, then tries to openIfChanged the reader.',1
'We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption... it\'s nice not to have false scares even if the app is doing something it shouldn\'t...',1
'Yes, I really want to.',1
'We can\'t let such abuse prevent real features, thats just wrong.',1
'If you want safety against deleting files, maybe look at NIO.2 WatchService.',1
'This is general and would give you notification when such things happen rather than having a hack for one particular user\'s mistake.',1
'We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption\r\nIt is index corruption though.',1
'The user went and manually corrupted their index.',0
'Why hide that Mike?',1
'Michael,\nYour updated patch definitely fixes the issue.',1
'But I just wanted to understand why deletes are so special, in that - if I don\'t have any buffered deletes for the segment, but new documents only, the reused reader instance won\'t pick them up, even without the fix in place.',1
'This is because liveDocs won\'t capture unflushed doc ids?',1
'Woops, this almost dropped past the event horizon of my TODO list.',0
'I modernized the patch, and was able to improve how effective its check is, by switching to comparing segment IDs (a very good check that the segments changed on disk) vs what the patch used to do, comparing maxDoc.',1
'Commit 664e39292bd0a90ed6f20debc872ab74a1d7294f in lucene-solr\'s branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=664e392 ]\nLUCENE-5931: detect when segments were (illegally) replaced when re-opening an IndexReader',0
'Commit 6b0b119074f4cd32adc2388fbcc01f2aa70c7d5d in lucene-solr\'s branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6b0b119 ]\nLUCENE-5931: detect when segments were (illegally) replaced when re-opening an IndexReader',0
'Thanks Adrien Grand for pinging me about this almost lost issue!',0
'Bulk close resolved issues after 6.2.0 release.',1
'Here is a simple patch.',1
'Looks good.',1
'FWIW CharTermAttribute.java has some optimizations for this method.',1
'Maybe if they are really useful we should see if we can pull them out into static methods.',1
'Thanks Uwe, I\'ll fix before committing, what a crazy requirement!',0
'There are potential things to share with CharTermAttributeImpl but I\'d rather leave it to another issue as it has some pitfalls as noted.',1
'Commit 1618625 from Adrien Grand in branch \'dev/trunk\'\n[ https://svn.apache.org/r1618625 ]\nLUCENE-5892: CharsRefBuilder implements Accountable.',0
'Commit 1618627 from Adrien Grand in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1618627 ]\nLUCENE-5892: CharsRefBuilder implements Accountable.',0
'Commit 1618628 from Adrien Grand in branch \'dev/trunk\'\n[ https://svn.apache.org/r1618628 ]\nLUCENE-5892: Missed the covariant return type.',0
'Commit 1618629 from Adrien Grand in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1618629 ]\nLUCENE-5892: Missed the covariant return type.',0
Thanks!,0
'BTW: Typical example is StringBuilder itsself: It implements Appendable, but also uses covariant return type: http://docs.oracle.com/javase/7/docs/api/java/lang/StringBuilder.html#append(java.lang.CharSequence)',0
'Here is a patch.',1
'Do we have a benchmark that could be used to validate this change?',1
'I just checked out luceneutil but it only seems to have tasks for queries, not sorting?',1
'I dont understand how the MatchNoBits case is safe.',1
'If no document has values, then they will all return the missing value?',1
'Oops, I understand your question now, I didn\'t upload the latest version of my patch.',0
'i benchmarked the first version of the patch with the little benchmark in luceneutil, but saw no improvement.',1
'I think the current null check is effective?',1
'it has to be handled anyway.',1
'And personally i would be wary of overspecialization here...',1
'I dont think there is specialization needed.',1
'The null check, Robert mentions, was done like this to optimize missing values.',1
'the null check has to be done anyways by the jvm, so removing it brings nothing.',1
'see the original missing values issue for discussion.',0
'Uwe, please read the issue again: the goal was not to remove the null check, but the check for missing values.',1
'The reason why I came up with this issue is that I\'m writing a selector in order to sort based on the values of a block of documents.',1
'To make it work efficiently I need to write a NumericDocValues instance that already returns the missing value when there are either no child documents in the block or if none of them have a value.',1
'So there is no need to check the missing values in the comparator.',1
'I\'m surprised that you think of it as a specialization as this is actually making things simpler?',1
'The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance.',1
'And it makes it easier (and potentially more efficient) to write selectors.',1
'I\'m surprised that you think of it as a specialization as this is actually making things simpler?',1
'The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance.',1
'And it makes it easier (and potentially more efficient) to write selectors.',1
'It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues.',1
'and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.',1
'the goal was not to remove the null check, but the check for missing values.',1
'In fact you are removing the null check, which is the extra branch to check for missing values - just look at the old code (this was my trick).',1
'It was done exactly like this to not slow down - hotspot can optimize that away, if it finds out that it is null - it does this very fast.',1
'We checked this at the time I added this to Lucene 3.5 or like that.',0
'We compared the two implementations - without missing values and the new one with missing values - and they were exactly the same speed.',1
'The same that Robert discovered here, too.',0
'In fact your patch would only work in Lucene trunk, in 4.x this cannot be done like that.',1
'It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues.',1
'and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.',1
'Doesn\'t it happen already if you have two fields that have different compression?',1
'I didn\'t see it happening with the currently assembly generated.',1
'I\'m ok with the issue if we see a performance increase, just not seeing it.',1
'Fair enough',1
'I don\'t think we should remove the default implementation for FilterScorer, as the scorer is not really changed when using this abstract class, its just wrapped?',1
'For the same reason, i think the boostingscorer (since its just an implementation detail of how the current BS2 stuff solves this case) should be transparent.',1
'Thanks for taking the time to review my patch and comment on the approach.',0
'The reason that I advocated changing FilterScorer and BoostedScorer is to allow some of my custom Query implementations to use a regular BooleanQuery for recall and optionally scoring while taking advantage of the actual Scorers used on a per document, per clause basis.',1
'This has been working great across quite a few Lucene releases but failed when I upgraded to 4.9 due to the two regressions in behavior for Scorer.getChildren() as described in this ticket.',1
'In this scenario, a BooleanQuery containing two TermQueries (one a miss and the other a hit) returns the following from BooleanWeight.scorer():\n\nBoostedScorer\n\t\nTermScorer (hit)\n\n\n\nCalling getChildren() on this returns an empty list because the BoostedScorer just returns in.getChildren() and thus you are unable to navigate to the actual TermScorer in play.',1
'This would impact any classes that extend FilterScorer and don\'t override getChildren().',1
'In other words, the current wiring does make the BoostedScorer transparent but with the disadvantage of hiding the actual scorer that performs the work.',1
'If this is an unsupported workflow, I\'m happy to move the discussion over to the user mailing list.',0
'I see: this makes sense.',1
'If you have a custom scorer you may need access to the raw one, so this makes sense to remove the transparency...',1
'I\'ll look at the patch again and reply back if I have more questions.',0
'Commit 1608454 from Robert Muir in branch \'dev/trunk\'\n[ https://svn.apache.org/r1608454 ]\nLUCENE-5796: Fix Scorer getChildren for two combinations of BooleanQuery',0
'Commit 1608457 from Robert Muir in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1608457 ]\nLUCENE-5796: Fix Scorer getChildren for two combinations of BooleanQuery',0
'Thanks Terry!',0
'Commit 1603521 from Michael McCandless in branch \'dev/branches/lucene_solr_4_9\'\n[ https://svn.apache.org/r1603521 ]\nLUCENE-5775: Deprecate JaspellLookup; fix its ramBytesUsed to not StackOverflow',0
'Commit 1603523 from Michael McCandless in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1603523 ]\nLUCENE-5775: Deprecate JaspellLookup; fix its ramBytesUsed to not StackOverflow',0
'Hi,\nshould we maybe also deprecate the Solr factories?',1
'Because then Solr prints a warning on startup if they are used.',1
'OK will do; just add @deprecated / @Deprecated to JaspellLookupFactory right?',1
'I am trying to check that out, too',0
'With Eclipse it looks like this should be enough to deprecate the factory you mentioned.',1
'But the default lookup for file-based stuff in Solr is still Jaspell:\n\n./core/src/java/org/apache/solr/spelling/suggest/jaspell/JaspellLookupFactory.java:public class JaspellLookupFactory extends LookupFactory {\n./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;\n./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:  public static String DEFAULT_FILE_BASED_DICT = JaspellLookupFactory.class.getName();\n./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;\n./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:      lookupImpl = JaspellLookupFactory.class.getName();\n\n\nSo maybe we should change defaults.',1
'Should we open SOLR issue?',0
'So maybe we should change defaults.',1
'Should we open SOLR issue?',0
'OK I\'ll open a Solr issue.',0
'I opened SOLR-6178',0
'Commit 1603542 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1603542 ]\nLUCENE-5775: deprecate JaspellLookup',0
'OK I committed the deprecation to trunk; I\'d really like to just remove it, but we can\'t do that until we address SOLR-6178 ... so I\'ll leave this issue open to remove Jaspell in trunk.',1
'Commit 1604122 from Uwe Schindler in branch \'dev/trunk\'\n[ https://svn.apache.org/r1604122 ]\nSOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory',0
'Commit 1604124 from Uwe Schindler in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1604124 ]\nMerged revision(s) 1604122 from lucene/dev/trunk:\nSOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory',0
'Commit 1604125 from Uwe Schindler in branch \'dev/branches/lucene_solr_4_9\'\n[ https://svn.apache.org/r1604125 ]\nMerged revision(s) 1604122 from lucene/dev/trunk:\nSOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory',0
'Commit 1604707 from Uwe Schindler in branch \'dev/branches/lucene_solr_4_9\'\n[ https://svn.apache.org/r1604707 ]\nRevert:\nMerged revision(s) 1604122 from lucene/dev/trunk:\nSOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory',0
'Commit 1604710 from Uwe Schindler in branch \'dev/trunk\'\n[ https://svn.apache.org/r1604710 ]\nMove changes of:\nSOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory',0
'Commit 1604711 from Uwe Schindler in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1604711 ]\nMerged revision(s) 1604710 from lucene/dev/trunk:\nMove changes of:\nSOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory',0
+1,1
'patch, I also cleaned up all remnants/conditionals about not supporting offsets.',1
'+1, nice that all codecs support offsets now!',1
'Commit 1584140 from Robert Muir in branch \'dev/trunk\'\n[ https://svn.apache.org/r1584140 ]\nLUCENE-5563: remove sep layout',0
'I can backport if we want to, but I\'m not sure its worth the trouble here.',1
'We still have 3.x codec in 4.x, as well as the fact the blockterms readers/indexes have changed in trunk and have better testing: so backporting poses some risks.',1
'Bulk close after 5.0 release.',1
'Here is a simple test.',1
'Seems to be related to the use of index-time synonyms.',0
'Here\'s a patch I\'m testing (all tests seem to pass, but I will see if i can add some better ones).',1
'FieldTermStack is ordered by position, but the current code doesn\'t really handle position increments of 0 (synonyms).',1
'In this case when we reach a dead-end (nextMap == null), we keep looking as long as the incoming position is the same until we find a match.',1
+1!,1
'Commit 1579255 from Robert Muir in branch \'dev/trunk\'\n[ https://svn.apache.org/r1579255 ]\nLUCENE-5538: FastVectorHighlighter fails with booleans of phrases',0
'Commit 1579264 from Robert Muir in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1579264 ]\nLUCENE-5538: FastVectorHighlighter fails with booleans of phrases',0
'Commit 1579269 from Robert Muir in branch \'dev/branches/lucene_solr_4_7\'\n[ https://svn.apache.org/r1579269 ]\nLUCENE-5538: FastVectorHighlighter fails with booleans of phrases',0
'Bulk close 4.7.1 issues',1
'here is a patch',1
'The first ensureOpen() in incRef() now seems redundant after this patch?',1
'The first ensureOpen() in incRef() now seems redundant after this patch?',1
'agreed - uploaded a new patch',1
'+1, looks correct.',1
'SegemntCoreReaders atomic increment is also correct - @BrianGoetzSays',1
+1,1
'Commit 1549012 from Simon Willnauer in branch \'dev/trunk\'\n[ https://svn.apache.org/r1549012 ]\nLUCENE-5362: IndexReader and SegmentCoreReaders now throw AlreadyClosedException if the refCount in incremented but is less that 1.',0
'Commit 1549013 from Simon Willnauer in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1549013 ]\nLUCENE-5362: IndexReader and SegmentCoreReaders now throw AlreadyClosedException if the refCount in incremented but is less that 1.',0
'Fix the issue by pushing boosts from parent queries to child queries when the parent queries are flattened.',1
'I clone the child queries before setting their boost so I don\'t break anything that expects them unchanged.',1
'I\'m not super happy that I have to clone the queries but it seemed like the simplest solution.',1
'Thanks Nik, your fix looks good!',1
'I don\'t think cloning the queries is an issue, it happens all the time when doing rewrites, and it\'s definitely better than modifying those queries in-place.',1
'I\'ll commit it tomorrow if there is no objection.',1
'Commit 1556483 from Adrien Grand in branch \'dev/trunk\'\n[ https://svn.apache.org/r1556483 ]\nLUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.',0
'Commit 1556484 from Adrien Grand in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1556484 ]\nLUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.',0
'Commit 1556485 from Adrien Grand in branch \'dev/branches/lucene_solr_4_6\'\n[ https://svn.apache.org/r1556485 ]\nLUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.',0
'While doing a final review, I noticed that you mistakenly modified the boost of the original query instead of the clone.',1
'I took the liberty to fix it before committing but please let me know if this looks wrong to you.',1
'Committed, thanks!',1
Wonderful!,1
Thanks.,0
'First patch, 17 Oct 2013, quite rough, one nocommit.',1
'The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html\nThe patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,\nwhich is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.',1
'Otherwise it uses WAH8DocIdSet, the current behaviour.',1
'Does this choice make good use of the benchmark results?',1
'To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality.',1
'(Perhaps a similar method should be added to EliasFanoDocIdSet.)',1
'In other cases, the patch falls back to WAH8DocIdSet.',1
'I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.',1
'The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.',1
'I like the idea of using the Elias-Fano doc id set given how it behaves in the benchmarks but it is tricky that it needs to know the size of the set in advance.',1
'In practice, the cache impls that you are going to have in CWF.cacheImpl are most likely QueryWrapperFilters, not FixedBitSets or OpenBitSets, so there is no way to know the exact size in advance.',1
'We could use DocIdSetIterator.cost but although it is recommended to implement this method by returning an upper bound on the number of documents in the set, it could return any number.',1
'Do you think there would be a way to relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)?',1
'relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)\nThe approximation would have to be a lower bound, i.e.',1
'it might be higher than the actual number of documents.',1
'The EliasFanoEncoder reserves all the memory it needs at construction time, so the loss in compression would be roughly as noticable as the accuracy of the bound.',1
'DocIdSetIterator.cost has another purpose, so it\'s not worthwhile to use it here I think.',1
'Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?',0
'That is not immediately clear from the benchmark results, but it could be so.',0
'DocIdSetIterator.cost has another purpose, so it\'s not worthwhile to use it here I think.',1
'I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.',1
'Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?',0
'That is not immediately clear from the benchmark results, but it could be so.',0
'My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg.',1
'QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS.',1
'Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set.',1
'This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve...',1
'I\'m currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.',1
'... the closest thing we have to a cardinality() which is available for every DocIdSet.',0
'For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.',1
'(Ideally in this case the posting list could be copied from the index into the cache, but we\'re not there yet.)',1
'Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?',1
'There is also a TermFilter in the queries module.',1
'I have not looked at the code yet.',0
'Is it necessary to move that to core so a check for that can be used here, too?',1
'... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???',1
'For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.',1
'I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set.',1
'Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.',1
'In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.',1
'Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?',1
'I don\'t like much having support for specific filters based on instanceof calls.',1
'I really think the only two options are to consume the filter to cache twice, or to first load into memory in another filter impl and then load it again in an EF doc id set.',1
'And I would go for option 2 since option 1 is likely going to be slower?',1
'I don\'t like much having support for specific filters based on instanceof calls.',1
'Me neither, but that can be fixed by adding a docFreq() method to DocIdSet, as another hint to be used by CachingWrapperFilter.',1
'This method should return the actual number of doc ids in the set when its return value >= 0.',1
'The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.',1
'I also prefer option 2, but I\'d like to avoid using the other filter impl when possible by using the docFreq hint.',1
'I\'ll try and make another patch for this.',1
'The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.',1
'Good point, I fixed it.',1
'I also prefer option 2, but I\'d like to avoid using the other filter impl when possible by using the docFreq hint.',1
'To me this feels like a big change compared to what it gives us.',1
'I would prefer having another copy rather than adding this method.',1
'It felt like a big change when I started, but it was easier than I thought, have a look at the patch of 18 Oct.',1
'There are about 35 other places that directly extend DocIdSet or create a new one from an inline subclass, I have not checked these yet.',0
'This passes the current TestCachingWrapperFilter,  but there are no tests for this change yet.',1
'For small segments, maxDoc() <= 256,  this will use WAH8, would FBS better for those cases?',1
'The last choice for using the EF after the WAH8 was built is done using sufficientlySmallerThanBitSet because that was available, but I\'m not really sure whether a smaller load factor should be used there.',1
'Looking again at the benchmark on how to solve building an EF docidset without knowing the number of values in advance, one solution would be to use a PFD docidset for that because it builds quickly and it has good next() performance.',1
'The next() will be used once through the set to build the final docidset to be cached.',1
'However an even better way might be to use one or more temporary long arrays to store the incoming doc ids directly in FOR format, (without forming deltas and without an index).',1
'This can be done because the maximum doc id value is known.',1
'While storing the doc ids, one can switch to an FBS on the fly when the total number of doc ids becomes too high.',1
'The existing PackedInts code should be a nice fit for this.',1
'Since allocating the long arrays takes time, one can start with one array of say 1/512 of the maximum needed size, and continue into another (bigger) array as long as necessary or until an FBS is preferable.',1
'After some more thought on this I think using the WA8 docidset as in the patch is the best solution for now, because I think that gives the best building time for the expected cases.',1
'I might add an EliasFanoEncoder constructor with only an upperBound argument for this case.',1
'This would leave some room for adding more values (as in ArrayUtil.grow) and it would reorganize the encoded sequence to always use the latest number of values.',1
'Reorganizing the encoded sequence would be needed when the number of bits for encoding the lower values changes, and this is floor(log2(upperBound/numValues)) but never negative.',1
'(In a docidset for filtering the upperBound is normally the segment size, and the values are the doc ids.)',1
'About the patch of 20 Oct 2013:\nThere is an EliasFanoEncoder2 with a constructor with only an upperBound.',1
'This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.',1
'It works by growing by a factor of 2 as necessary, and reencoding completely.',1
'This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.',1
'The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.',1
'I ran some tests on this, it appears to work fine here.',1
'A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.',1
'CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,\nand it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.',1
'For the rest see my comments on the patch of 18 Oct.',0
'I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.',1
'The advance/next performance is only slightly less, again as expected.',1
'I could not measure build times, I expect it to just about double for the upperbound only constructor.',1
'I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.',1
'Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding?',1
'This is what the other (wah8/pfor) sets do.',1
'and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS\nMaybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?',1
'Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding?',1
'This is what the other (wah8/pfor) sets do.',1
'I suppose you mean by recomputing the actually needed sizes for the long arrays of the encoder after all encoding is done, and then reallocating them?',1
'That would certainly be possible.',1
'I\'ll have a look at wah8/pfor for this.',0
'Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?',1
'The problem is that none of the compressing sets is as good as FBS at advancing far in almost full sets.',1
'I\'m working on this now, another patch is slowly on its way.',0
'About the patch of 21 Oct:\nAdds EliasFanoEncoderUpperBound (instead of EliasFanoEncoder2) with only an upperBound argument to the constructor.',1
'Adds a freeze() method to EliasFanoEncoder to reallocate to actual size.',1
'Both are used in EliasFanoDocIdSet, which now also reverts to using an FBS as needed.',1
'Adapted TestEliasFanoDocIdSet to use EliasFanoDocIdSet randomly half of the time with a -1  numValues so it may use EliasFanoEncoderUpperBound instead of EliasFanoEncoder at first.',1
'For the rest see my remarks about the patch of 20 Oct.',0
'On the patch of 22 Oct:\nThe previous patch contains a bug in the freeze() code, it allocates more than an FBS size to one of the encoding arrays.',1
'This should fix it.',1
'I\'ve done some more performance measurements of this EliasFanoDocIdSet that allows only an upperBound to its constructor.',1
'No surprising results but I\'d like add an APL2 to  the benchmark program that was derived from the (Test)DocIdSetBenchmark at LUCENE-5236 and LUCENE-5101 and post it.',1
'Adrien, can I assume an APL 2 on the first DocIdSetBenchmark at LUCENE-5101?',1
'The patch here is getting a little overloaded, so shall I open a separate issue for the EliasFanoDocIdSet that allows only an upperBound to its constructor?',1
'I opened a github pull request for the latest patch:\nhttps://github.com/apache/lucene-solr/pull/19',0
'Since LUCENE-5983 CachingWrapperFilter uses RoaringDocIdSet.',0
'Are there any advantages for EliasFanoDocIdSet left for use there?',1
'Although the elias-fano set is smaller in the sparse case, it\'s true that RoaringDocIdSet tends to be faster to build and to iterate on.',1
'So overall I think the RoaringDocIdSet has a better trade-off indeed.',1
'Github user PaulElschot closed the pull request at:\n https://github.com/apache/lucene-solr/pull/19',0
'Patch; it turned out to be easier than I expected: I just tapped into the existing logic that ShingleFilter has for handling holes between tokens.',1
'+1, patch looks good.',1
'+1 to your suggestion about ShingleFilterTest.TestTokenStream:\n// TODO: merge w/ CannedTokenStream?',1
'Thanks Steve!',0
'Here\'s a new patch w/ that TODO done ...',1
'I think it\'s ready.',1
'Commit 1524117 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1524117 ]\nLUCENE-5180: ShingleFilter creates shingles from trailing holes',0
'Commit 1524120 from Michael McCandless in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1524120 ]\nLUCENE-5180: ShingleFilter creates shingles from trailing holes',0
'Commit 1524122 from Michael McCandless in branch \'dev/trunk\'\n[ https://svn.apache.org/r1524122 ]\nLUCENE-5180: move CHANGES entry',0
'java.lang.IndexOutOfBoundsException: start 9999, end 10004, s.length() 10000\n\tat java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:453)\n\tat java.lang.StringBuilder.append(StringBuilder.java:179)\n\tat org.apache.lucene.search.postingshighlight.DefaultPassageFormatter.append(DefaultPassageFormatter.java:135)\n\tat org.apache.lucene.search.postingshighlight.DefaultPassageFormatter.format(DefaultPassageFormatter.java:79)\n\tat org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightField(PostingsHighlighter.java:435)\n\tat org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightFields(PostingsHighlighter.java:353)\n\tat org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightFields(PostingsHighlighter.java:268)',0
'Can you show a real usecase for a document matching beyond content.length()?',1
'Your patch artificially creates an out-of-bound Passage, but I think it\'s better if we can see a real usecase, e.g.',1
'maybe a combination of TokenFilters may cause that?',1
'But if e.g.',0
'the app indexed content1 but then tries to highlight content2, I don\'t think that\'s a supported usecase...',1
'Please find attached another test case.',1
'It is sort of bad luck to run into this in a real use case but it actually happened to me.',1
Interesting.,1
'FYI, I will not be available in the next 2 weeks, and haven\'t reproduced it yet.',0
'If no one assigns himself to the issue, I will when I\'m back.',0
'I have reproduced it with Manuel\'s test',0
'Attached is just a combined patch of Manuel\'s 2 patches.',0
'There is definitely bug(s) here.',1
'As far as the fix, to me the big question (I put it in a nocommit to his test case), is if formatter classes should really have to deal with these cases.',1
'OK here\'s a patch.',1
'the cause of the bug is that we only know startOffsets are always increasing (the algorithm relies on this, and merges them across terms).',1
'So we cannot safely terminate when end >= limit (only start >= limit), but we don\'t have to confuse the formatter with the cases of terms that \'span\' the limit.',1
'Hmm so this means we may pick a truncated passage to present?',1
'I suppose it\'s unlikely to score well ... just seems bad though.',1
'Wait, couldn\'t we fix passageQueue.offer(current) to not offer it if current.endOffset == contentLength?',1
'Improved patch, thank you Mike',1
'OK let\'s not try to address that on this issue ...',1
'I\'m not even sure it needs fixing.',1
'It ought to be rare-ish that a truncated passage is selected.',0
'There was a bug in my patch: I added another unit test for this!',1
'I think its ready.',1
'+1\nTricky!',1
'Commit 1513207 from Robert Muir in branch \'dev/trunk\'\n[ https://svn.apache.org/r1513207 ]\nLUCENE-5166: PostingsHighlighter fails with IndexOutOfBoundsException',0
'Commit 1513231 from Robert Muir in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1513231 ]\nLUCENE-5166: PostingsHighlighter fails with IndexOutOfBoundsException',0
'Thank you Manuel!',0
'Thank you for the quick help!',0
'I just found another problem here: If we have both, matches that do and matches that don\'t span the content boundary the formatter is asked to highlight the spanning match.',1
'Please find attached additional tests and a possible fix for this.',1
'Hi Manuel: thank you!',0
'Another bug, or a bug in my fix to the other bug \nI\'ll investigate deeper in a bit.',0
'Manuel: your fix is correct, thank you.',1
'To explain: I had totally forgotten about this little loop on tf within the passage (i had removed this optimization in LUCENE-4909, which didnt turn out to work that great, so wasn\'t committed).',1
'We might at some point want to still just remove the optimization just based on the reason that it makes this thing more complicated, it was just intended to speed up the worst case (where someone has very common stopwords and stuff like that).',1
'But for now to complete the bugfix, we should commit your patch (LUCENE-5166-revisited.patch).',1
'Commit 1514367 from Robert Muir in branch \'dev/trunk\'\n[ https://svn.apache.org/r1514367 ]\nLUCENE-5166: also fix and test this case where tf > 1 within the passage for a term',0
'Commit 1514379 from Robert Muir in branch \'dev/branches/branch_4x\'\n[ https://svn.apache.org/r1514379 ]\nLUCENE-5166: also fix and test this case where tf > 1 within the passage for a term',0
'Thank you again!',0
'4.5 release -> bulk close',1
'Commit 1594464 from Robert Muir in branch \'dev/branches/lucene5666\'\n[ https://svn.apache.org/r1594464 ]\nLUCENE-5166: clear most nocommits, move ord/rord to solr (and speed them up), nuke old purging stuff',0
'First draft of patch attached.',1
'Let me know how this looks.',0
'Thank you.',0
'Thanks Tim!',0
'This looks like a great improvement: I like factoring out calcDistance from calcSimilarity.',1
'And I like that we now take raw into account when figuring out which comparison to make to accept the term or not.',1
'Maybe we could improve it a bit: if raw is true we don\'t need to calcSimilarity right?',1
'For my sanity ... where exactly was the bug in the original code?',0
'Thank you for your quick response!',0
'I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.',1
'Let me know if I\'m missing something.',0
'The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.',1
'So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.',1
'In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn\'t the determining factor in whether SlowFuzzyTerms accepted a term.',1
'Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!',1
'I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.',1
'Let me know if I\'m missing something.',0
'Ahh, you\'re right ...',1
'I missed that.',0
OK.,0
'The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.',1
'So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.',1
'In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn\'t the determining factor in whether SlowFuzzyTerms accepted a term.',1
'Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.',0
'Now I understand the bug ... thanks.',0
'Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!',1
'But this fix only applies in cases (edit distance > 2) where automaton\'s don\'t, I think?',1
'(The fixes are to LinearFuzzyTermsEnum).',0
'Robert,\nI agree that it appears to, but if you want a distance > 2, the current levenshtein automaton doesn\'t allow that (http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html?showComment=1303598602291#c3051732466052117784).',1
'The classic QueryParser silently converts distances > 2 to 2.',1
'If I understand SlowFuzzyQuery correctly, it uses the levenshtein automaton for distances <= 2, but it runs brute force if the distance is > 2.',1
'My personal preference would be to undeprecate SlowFuzzyQuery (certainly leave it in the sandbox) because it offers a capability that the current levenshtein automaton doesn\'t.',1
'In cases where the indices are very large, it wouldn\'t make sense to expose distance > 2 capability; but on small to medium indices, there are use cases that require it.',1
'Updated patch.',1
'Added short circuits to avoid calculating similarity when not necessary.',1
'Corrected term.length to text.length in calcSimilarity call.',1
'Activated old tests that test for edit distance matches where the edit distances are greater than the query term length.',1
'Thanks Tim, new patch looks great!',1
'Thanks Tim!',0
'Mike,\n  Thank you for your feedback and quick response!',0
'Bulk close resolved 4.4 issues',1
patch.,1
'looks good!',1
'New patch, added a test case, and fixed PSDP to detect if you try to snapshot/release when it\'s not being used by an IW ...',1
'I think it\'s ready.',1
+1!,1
'[trunk commit] mikemccand\nhttp://svn.apache.org/viewvc?view=revision&revision=1478726\nLUCENE-4976: use single file to hold PersistentSnapshotDeletionPolicy state on disk',0
'[branch_4x commit] mikemccand\nhttp://svn.apache.org/viewvc?view=revision&revision=1478730\nLUCENE-4976: use single file to hold PersistentSnapshotDeletionPolicy state on disk',0
'[branch_4x commit] mikemccand\nhttp://svn.apache.org/viewvc?view=revision&revision=1478854\nLUCENE-4976: fix Solr IndexDeletionPolicy impls to handle empty commits onInit',0
'[trunk commit] mikemccand\nhttp://svn.apache.org/viewvc?view=revision&revision=1478855\nLUCENE-4976: fix Solr IndexDeletionPolicy impls to handle empty commits onInit',0
'[trunk commit] mikemccand\nhttp://svn.apache.org/viewvc?view=revision&revision=1479394\nLUCENE-4976: add missing sync / delete old save files',0
'[branch_4x commit] mikemccand\nhttp://svn.apache.org/viewvc?view=revision&revision=1479395\nLUCENE-4976: add missing sync / delete old save files',0
'Bulk close resolved 4.4 issues',1
'facet collections have IntIterator interface.',1
'And so the method should be something like IntIterator getChildren(int ordinal)?',1
'I don\'t know much about IntIterator, but I surmise it\'ll do good enough.',1
'Added TaxoReader.getChildren(int ordinal) and corresponding test.',1
'I also migrated PrintTaxonomyStats to use getChildren, which removed all mentions of ParallelTaxonomyArrays from it.',1
'Shouldn\'t next() throw NoSuchElementException if child is already INVALID_ORDINAL?',1
'It shouldn\'t ever return INVALID_ORDINAL, right?',1
'Ie, caller screwed up and called next w/o calling hasNext first.',1
'I looked at other IntIterator impls and none throw NoSuchElementException, so I thought it\'s best to follow.',1
'Also, since it returns ordinals, it\'s kind of ok to return INVALID_ORDINAL.',1
'I wished that we had a simple IntIterator interface with only next() for this case...',1
'I don\'t mind throwing it though.',1
'What do you think?',0
'Or we could return a not-Java-iterator, that just has int next() that returns INVALID_ORDINAL when it\'s done...',1
'Woops, our comments crossed...',0
'I wished that we had a simple IntIterator interface with only next() for this case...\n+1, I think that\'s best.',1
'I wanted to avoid introducing another class (facet collections already use this primitive IntIterator), but maybe a ChildrenIterator with next() is simplest.',1
'I\'ll look into it.',0
'Patch with ChildrenIterator',1
'+1, thanks Shai!',1
'[trunk commit] shaie\nhttp://svn.apache.org/viewvc?view=revision&revision=1464730\nLUCENE-4897: add a sugar API for traversing categories',0
'[branch_4x commit] shaie\nhttp://svn.apache.org/viewvc?view=revision&revision=1464743\nLUCENE-4897: add a sugar API for traversing categories',0
'Committed to trunk and 4x.',1
'Closed after release.',1
'I think that we should use a primitive iterator, e.g.',1
'Here\'s a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.',1
'Some questions:\n\nwhich package should this stuff be in?',1
'FieldCache is in o.a.l.search, and the reader is in o.a.l.index.',1
'there are a bunch of FieldCache-specific queries and filters.',1
'Can these just be reworked to be DV-specific instead?',1
'can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?',1
'this still uses the global FieldCache.',0
'Should the caches be moved to the readers instead?',1
'But I assume the plan is to remove aol.search.FieldCache entirely\nI wasn\'t going to initially - just make it package private.',1
'But thinking about it, can we just put a map of fieldnames->XXXDocValues in a threadlocal on UninvertingFilterReader (like on SegmentCoreReaders)?',1
'That makes things a lot cleaner.',1
'We\'d need a way to purge the map, though.',1
'Maybe we should make a branch for this\nHaving just spent an hour or so trying to cut things over, yes, that\'s probably a good idea   It touches a lot of the codebase.',1
'Should simplify things like SortField a lot though.',1
'\nI was wondering about how to do this.',1
'We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it\'s absent, then you can uninvert any field you like, at the risk of fieldcache-insanity.',1
'Why allow this?',0
'I don\'t think we should do this.',1
'it also prevents it from working with anything that checks fieldinfos.',1
'Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue?',1
'Unless you think that we\'ll end up having to make major changes if we don\'t build this in from the beginning.',1
'I\'m new to a lot of this part of the codebase, so all advice is very welcome here\nI think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way.',1
'there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.',1
'I\'m willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.',1
'New patch, copying the uninvert logic from FieldCacheImpl into a new Uninverter class (I haven\'t worked out how to implement the NumericDocValues uninverter yet - pointers welcome), adding a cache to the UninvertingFilterReader itself and checking uninversions against a map of fieldnames to DocValuesTypes passed in as a constructor argument.',1
'New patch, with a NumericDocValues uninverter.',1
'Also a basic test.',1
'NumericDocValues doesn\'t work with 32-bit values yet (the test for IntField fails); I need to somehow detect at uninversion-time how wide the indexed values are.',1
'Something that will uninvert a LongField or DoubleField into a NumericDocValues representation.',1
'OK, here\'s a patch that attempts to solve the problem of 32-bit vs 64-bit numeric values.',1
'I don\'t really like it, but it seems to work, so it\'s a start.',1
'Users of UninvertedFilterReader have to specify up-front the width of numeric fields they wish to uninvert.',1
'This is hacky for any number of reasons, not the least of which is I have to define a whole new set of DocValuesTypes on Uninverter, which is a lot less elegant than just using the existing FieldInfo ones.',1
'There doesn\'t seem to be a good way of detecting this at uninvert-time from the FieldInfo data, though.',1
'We could assume 64-bit values and fall back to 32-bit if we encounter an exception, but I worry that we\'re getting a performance hit here for every 32-bit field.',1
'Alan i didnt look closely, but could GrowableWriter be used to avoid the hack?',1
'that way you only take up space relevant to the bits you are actually using...',1
'Actually I\'ve convinced myself that checking for a NumberFormatException is a better way of doing this...',1
'could GrowableWriter be used to avoid the hack?',1
'I don\'t think so, because this is to do with reading the already-indexed bytes and converting them back to longs or ints.',1
'Unless I\'ve got the wrong end of the stick here.',0
'Attached a patch that adds an inner class IntIterator that is consistent with the contract of a java.util.Iterator but next() returns a primitive int.',1
'I tested basic expected usage.',0
'I figured being close to a standard iterator would be more familiar/friendly, although if it worked similar to DocIdSetIterator it would be faster since the caller would check for the sentinal value instead of calling hasNext() (which wouldn\'t exist).',1
'I forgot to make the methods public, which I\'ll fix when it gets committed.',1
'I don\'t think we need to mimic java\'s Iterator?',0
'Ie, it\'s fine to have only next()...\nAlso, the iterator is wrong if a rehash happens right?',1
'Can you add to the javadocs that the set should not be modified while the iterator is in use, except using the iterator\'s remove method (like Java\'s HashMap)?',1
'Mike: sounds good.',1
'But you know what?',0
'Iteration by the client code on this data structure is actually so darned easy; maybe this set iterator isn\'t really needed after all, or could be supplied with a comment so it\'s clear how to do it.',1
'SentinalIntSet set = ...\nfor (int v : set.keys) {\n  if (v == set.emptyVal)\n    continue;\n  //use v...\n}\n\n\npiece-o-cake',1
'maybe this set iterator isn\'t really needed after all\n+1',1
'So instead of adding to the API I decided to enhance the documentation to make it more clear how to use this class.',1
'(attached)\nOf note I added a warning of the potential unsuitability of the lack of hashing of the key.',1
'I committed better javadocs, including sample code to iterate the values.',1
'Closing issue as won\'t fix (for now).',1
'If at some point we want an iterator, the patch is here and it can be re-considered.',1
'Closed after release.',1
'\nShould we create a branch.',0
'I also have some ideas to fix...\nWe can, or maybe just use trunk?',0
'I don\'t think the patch makes these factories any worse.',1
'+1 to commit Robert\'s patch to trunk and iterate there.',1
'Or you can assign the issue.',0
'I just have family coming into town today and won\'t have any time to do any of the work to help out with this one.',0
'I thought i knew what was involved with this stuff pretty well but I grossly underestimated the amount of work to even throw the exception',0
'I have no preference on how to proceed.',0
'I just dont want to download such a large patch, modify the sources and upload it again.',0
'especially as TortoiseSVN and other clients depend on order of files in filesystem, the order of created patches is different, too.',0
'So its impossible to see any change in comparison to earlier patches.',0
'As we don\'t intend to release trunk soon: If all tests pass, can you simply commit as a first step, Robert?',0
'+1 on the current patch.',1
'We can open further issues to unfuck ResourceLoaderAware (it should be removed, too and the ResourceLoader should be passed to the ctor, too).',1
'I committed this to trunk.',1
'Can we be extra careful to use LUCENE-4877: in all commit messages so when its time to backport its easy to find the revisions?',0
'Thanks in advance for any improvements!',0
'getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?',1
'I just remembered.',0
'I think the reversewildcardfactory in solr has one of these too.',0
'Patch fixing these minor nits:\n\nTestMappingCharFilterFactory\'s factory could switch to being instantiated using the charFilterFactory() method\nEdgeNgramTokenizerFactory\'s gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer\nLimitTokenCountFilterFactory\'s maxTokenCount param should be required; this is a pre-existing problem though\nPatternTokenizerFactory\'s group param should use the getInt() method with a default of -1.',1
'Actually, I was wrong about LimitTokenCountFilterFactory - it already has a test in place to insure reporting of missing required maxTokenCount param.',1
'Committing shortly.',0
'Thanks Steve!',0
'This patch adds more param parsing methods to AbstractAnalysisFactory, including get(), require(), getFloat(), getChar(), and getSet(), and changed all analysis factories to use them where appropriate.',1
'I don\'t like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required???',1
'- maybe these could be converted to getRequiredXXX() ?',1
'I implemented these as require(), requireXXX(), etc.',1
'Tests all pass, and precommit\'s happy.',1
'Committing shortly.',1
'Thanks for cleaning this up Steve, much nicer.',1
'I\'d like to merge these commits back to 4.x if there are no objections.',1
'Closed after release.',1
'here\'s one way we could improve it.',1
'There are probably other alternatives that might be better, too.',0
'I only fixed one factory as its better to decide this before going thru all of them.',1
'+1, patch looks good.',1
+1,1
'I have fixed all the factories, but currently I just started fixing tests and adding tests for bogus parameters to all factories.',1
'I added some helpers to BaseTokenStreamTestCase and fixed TestArabicFilters.java as a prototype...\nslow moving',1
'updated patch: tests in analysis/common are done.',1
'I\'ll look at the other analysis modules and solr tomorrow.',0
+1,1
'whew, made it thru the rest.',0
'all tests pass.',1
'Really nice de-cluttering in the tests.',1
'I don\'t like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required???',1
'- maybe these could be converted to getRequiredXXX() ?',1
'I think AbstractAnalysisFactory could use additional param parsing methods:\n\nget(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.',1
'Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?',1
'getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?',1
'getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.',1
'getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?',1
'A few nits:\n\nTestMappingCharFilterFactory\'s factory could switch to being instantiated using the charFilterFactory() method\nEdgeNgramTokenizerFactory\'s gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer\nLimitTokenCountFilterFactory\'s maxTokenCount param should be required; this is a pre-existing problem though\nPatternTokenizerFactory\'s group param should use the getInt() method with a default of -1.',1
'I can do the work if you agree with these.',0
'Should we create a branch.',0
'I also have some ideas to fix...',0
'\nI don\'t like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required???',1
'- maybe these could be converted to getRequiredXXX() ?',1
'I think AbstractAnalysisFactory could use additional param parsing methods:\n    get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.',1
'Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?',1
'getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?',1
'getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.',1
'getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?',1
'A few nits:\n    TestMappingCharFilterFactory\'s factory could switch to being instantiated using the charFilterFactory() method\n    EdgeNgramTokenizerFactory\'s gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer\n    LimitTokenCountFilterFactory\'s maxTokenCount param should be required; this is a pre-existing problem though\n    PatternTokenizerFactory\'s group param should use the getInt() method with a default of -1.',1
'+1 to all of this!',1
'\nShould we create a branch.',0
'I also have some ideas to fix...\nWe can, or maybe just use trunk?',0
'I don\'t think the patch makes these factories any worse.',1
'here\'s just where i am so far, just scavenging stored tests from where i can find.',0
'I still havent ported all the ones from .compressing package yet.',0
'updated patch.',1
'I think this is close... adrien do you know of any other tests we should steal and put in here?',0
'Patch looks good!',1
+1,1
'[trunk commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1457926\nLUCENE-4852: BaseStoredFieldsFormatTestCase',0
'I committed this as a start.',1
'Hopefully we add more test methods to this base class!',1
'[branch_4x commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1457931\nLUCENE-4852: BaseStoredFieldsFormatTestCase',0
'Closed after release.',1
'[branch_4x commit] Tommaso Teofili\nhttp://svn.apache.org/viewvc?view=revision&revision=1448210\nLUCENE-4782 - backported fix to branch_4x',0
'[trunk commit] Tommaso Teofili\nhttp://svn.apache.org/viewvc?view=revision&revision=1448207\nLUCENE-4782 - removed wrong line in build.xml',0
'[trunk commit] Tommaso Teofili\nhttp://svn.apache.org/viewvc?view=revision&revision=1448204\nLUCENE-4782 - fixed SNBC docsWithClassSize initialization in case of codec doesn\'t support Terms#getDocCount',0
'thanks Robert, that may make sense but I\'m not really sure as in 4.x we still support 3x codecs in general.',1
'Also I noticed that other methods used in the classification module that depend on the underlying codec may return -1 (unsupported).',1
'I\'ll have a deeper look and see if it really make sense to \'fallback\' such calls or either it\'d be safer and reasonable to follow your suggestion.',1
'[trunk commit] Tommaso Teofili\nhttp://svn.apache.org/viewvc?view=revision&revision=1448932\nLUCENE-4782 - suppressing SNBC test for Lucene3x codec for now',0
'[branch_4x commit] Tommaso Teofili\nhttp://svn.apache.org/viewvc?view=revision&revision=1448933\nLUCENE-4782 - suppressing SNBC test for Lucene3x codec for now',0
'Closed after release.',1
'I am fine with such a system property and also some code to warn user of several incompatible combinations, but I want to be able to run tests to find the problems behind the issue.',1
'In my opinion, we should really warn users also on Solr startup, if they have jRockit (this JVM only works with Lucene if you pass -XnoOpt) or J9 (fails with Lucene 4.0+), so they don\'t corrumpt their index.',1
'Please note: Policeman Jenkins (before it was shot by some Generics Drug Dealer) was running JRockit with this JVM option.',0
'Why does it need to be a system property, Hoss?',0
'The test group annotations can be enabled/disabled via system properties and they also do display messages on assumption-ignored tests  wouldn\'t this be enough to cover your use case?',1
'@SuspiciousJ9Shit\n@SuspiciousJRockitShit\n\n\nThe only problem I see is that these need to be provided statically  if you need to detect them at runtime then I\'d either need to change the code of the runner or we\'d need to switch to assumptions inside a rule, for example.',1
'[trunk commit] Uwe Schindler\nhttp://svn.apache.org/viewvc?view=revision&revision=1421818\nRevert the revert of the revert.',0
'I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do.',0
'I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx.',0
'Thanks in advance!',0
'[branch_4x commit] Uwe Schindler\nhttp://svn.apache.org/viewvc?view=revision&revision=1421819\nMerged revision(s) 1421818 from lucene/dev/trunk:\nRevert the revert of the revert.',0
'I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do.',0
'I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx.',0
'Thanks in advance!',0
'Bulk move 4.4 issues to 4.5 and 5.0',1
'Move issue to Lucene 4.9.',1
'<<Non-IBMers please ignore this message>>\nDont see this as a bug against IBM JDK.',0
'Thanks and Regards\nBrijesh Nekkare\nIBM Java team',0
'here is a patch that adds #estimateDocCount to DISI.',1
'It still has some nocommits mainly related to BitSets and carnality but I this its fine as a start.',1
'I removed the TermConjunction specialization and changed the heuristic in FilteredQuery to use the estimated cost.',1
'Nice idea!',1
'At least the FilteredQuery code looks fine to me.',1
'I agree, the Bits interface and FixedBitSet has a costly cardinality (Bits does not have it at all...), so we should think about that.',1
'As far as I see it returns maxDoc as cost.',0
'One thing: estimatedDocCount is long, but docIds in Lucene are still int - this makes no sense, because the current scorer/disi interface can never return anything > Integer.MAX_VALUE, so the estimatedDocCount can never be 64 bits.',1
'We should maybe rethink in the future to make docIds in Lucene longs, but until this has happened, we should not mix both datatypes in public APIs, this would cause confusion.',1
'When i did the cost estimate patch on LUCENE-4236, i chose a long too.',1
'but there it was trying to estimate the number of documents visited,\ne.g.',0
'the number of postings.',0
'so the formula for a conjunction would be min(subscorer cost) * #subscorers, and for a disjunction its just the sum of all the subscorer costs, and so on.',0
'I felt like for scoring purposes this is more useful than the number of documents, but thats just my opinion.',1
'I tend to agree with robert that using longs makes things a lot easier here too.',1
'We don\'t need to deal with int overflows.',1
'Maybe we should rename to estimateDocsVisited?',1
'Uwe the current discussion is about not measuring count of documents, but instead i/o operations.',1
'So advance(), docId(), fixedBitset and so on are totally unrelated to that.',1
'Uwe the current discussion is about not measuring count of documents, but instead i/o operations.',1
'Man, I just wanted to make clear that the current patch and the current issue summary have this problem.',1
'ok: I agree if its a count of documents, the type should be consistent.',1
'But as i suggested i dont think a count of documents is that great for how we will use this (picking conjunction leader, filtering heuristics, maybe minimum-should-match disjunction scoring, maybe cleanup exact phrase scorer / add its optimizations to sloppy phrase scorer, maybe more BS versus BS2 heuristics in BooleanWeight, etc etc)',1
'Patch updated to trunk (as Stefan needs this for LUCENE-4571).',1
'I carefully reviewed all these costs and also integrated it into spans.',1
'Though these queries (e.g.',1
'nearquery) dont yet use it, they should.',1
'The only query i changed was to make ConjunctionTermScorer algorithm our ConjunctionScorer, sorting subscorers on cost (which is docFreq in the all terms case).',1
+1,1
Thanks!,0
'I will come up with a new prototype impl for LUCENE-4571 soon which will include and build upon this patch.',1
'The patch might be hard to apply due to the svn replace Stefan.',1
'I plan on doing final checks on it and committing it today.',1
'Really its silly lucene doesnt have this for our scorers already, since e.g.',1
'textbook IR formulas make use of this stuff.',1
'We can open followup issues to improve the other scorers logic (FilteredQuery, SpanNearQuery, etc).',1
'I only did the obvious \nconjunction one so we get the logic of ConjunctionTermScorer across any arbitrary scorers.',1
'Just gimme a few hours, sorry for the delay',0
'Thanks Robert, I was thinking about this the other day!',0
'Cool that you brought this back to life!',1
'+1 to the patch.',1
'I think you need to add a CHANGES.TXT entry still.',1
simon,0
'I\'ll put it in when committing...I always wait until then, otherwise its just asking for conflicts if you ever merge up the patch',1
'[trunk commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1456670\nLUCENE-4607: add DISI/Spans.cost',0
'[branch_4x commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1456686\nLUCENE-4607: add DISI/Spans.cost',0
'Closed after release.',1
'I tried this on that geonames database since my default indexing (just shoving everything in as a TextField)\ncreates a huge .nrm file today (150MB: 8M docs * 19 fields).',1
'Just as a test I tried a simple similarity\nimplementation that uses \n\n@Override\npublic void computeNorm(FieldInvertState state, Norm norm) {\n  norm.setPackedLong(state.getLength());\n}\n\n\n\n-rw-rw-r--  1 rmuir rmuir  49339454 Nov  5 22:30 _7e_nrm.cfs\n\n\nIf you want to use boosts too, you would have to be careful how you encode, but I think this can be useful.',1
'In this case its 1/3 of the RAM, even though documents lengths are exact vs. lossy (though most fields are \nshortish, some are huge, like alternate names fields for major countries and cities, which have basically every \nlanguage imaginable shoved in the field: thats why it doesnt save more I think)',0
'+1 - should we also document that we don\'t have similarities that can make use of it at this point?',1
'+1, very cool!',1
'I don\'t understand the question Simon: all the ones we provide happen to use Norm.setByte\nI don\'t think we need to add documentation to Norm.setFloat,Norm.setDouble saying that we don\'t\nprovide any similarities that call these methods: thats not important to anybody.',1
'I don\'t understand the question Simon: all the ones we provide happen to use Norm.setByte\nJust to clarify.',0
'Currently if we write packed ints and a similarity calls Source#getArray you get an UOE.',1
'I think we should document that our current impls won\'t handle this.',1
'I don\'t see how its relevant.',1
'Issues will happen if you use Norm.setFloat (as they expect a byte).',1
'I\'m not going to confuse the documentation.',1
'The built-in Similarities at query-time\ndepend upon their index-time norm implementation: this is documented extensively everywhere!',0
'fair enough.',0
'I just wanted to mention it..',0
'If someone changes their similarity to use a different norm type at index-time than at query-time,\nthen he or she is an idiot!',0
'I plan to revert this for 4.1 to contain the amount of backwards compatibility code we need to implement for LUCENE-4547.',1
'If someone uses this functionality in its current form, they will easily hit the LUCENE-4547 bug.',1
'I implemented this more efficiently with the new APIs in the lucene4547 branch anyway: when it would save RAM, and the # of values is small, it dereferences the unique values and packs ords.',1
'This is typically the case with our smallfloat encoding.',0
'[trunk commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1432096\nLUCENE-4540: revert',0
'I backed this out of 4.1.',1
'When LUCENE-4547 lands, we can resolve it with that implementation.',0
'[branch_4x commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1432100\nLUCENE-4540: revert',0
'[branch_4x commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1406433\nLUCENE-4540: allow packed ints norms',0
'Closed after release.',1
'+1 That\'s pretty damn cool',1
Awesome!,1
+1,1
'Almost got this working, two bugs to resolve:\n\na bug in eclipse compiler (imo), i tell it to create no class files, but its creating some for spatial (package-info.class processing) because it uses package-info.java instead of package.html.',1
'I\'ll make the macro use a throwaway directory and delete it.',1
'a bug in solrj javadocs: it links to the lucene queryparser syntax incorrectly.',1
'I don\'t know why this is working with \'ant javadocs\', but it really shouldnt, since lucene queryparser should not be in its compile classpath.',1
'I\'ll fix it to use docRoot.',1
'updated patch: everything is passing.',1
'Ill run precommit and get this thing in (trunk/4x only): i spent a lot of time cleaning up docs and want to keep the bar high.',1
'we can adjust the properties as needed later as more cleanup happens, but i dont want to let them get any worse.',1
'The test has a whitespace in pathname problem.',1
'Also its not nice to permgen.',1
'I have a patch fixing those 2 problems.',1
'It also prints a nice taskname instead of [javac] on ANT output.',1
'+1, thanks for cleaning up.',1
'taskname is cool, i didn\'t like the log output but didnt know about this.',1
'Committed trunk revision: 1389491\nCommitted 4.x revision: 1389492',1
'[branch_4x commit] Uwe Schindler\nhttp://svn.apache.org/viewvc?view=revision&revision=1389492\nMerged revision(s) 1389491 from lucene/dev/trunk:\nLUCENE-4409: Improve ECJ-Linter (permgen, taskname) + fix whitespace bug',0
'[branch_4x commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1389192\nLUCENE-4409: implement javadocs linting with eclipse ecj compiler',0
'Closed after release.',1
'My latest test for branch_4x finished this test in 2771.02 seconds on Windows.',0
'No errors were reported.',1
'CPU was relatively idle whenever I checked (25\\\%).',1
'In Eclipse it took 316 seconds (while ant test was still running the rest of the tests.)',1
'On trunk, TestIndexWriterWithThreads took 90 seconds.',1
'Did you run all these with the same seed?',0
'It\'s weird, the variance on this test is indeed very high.',1
'I think it may have something to do with the fact that it spins many threads (that do i/o) so if you\'re running on a multicore and there are other parallel jvms running tests you\'re putting a load on the hardware.',1
'If ran in isolation things get much faster (for me).',1
'I\'ve replaced some of the random() calls with the non-asserting random; I see some difference but not that much.',1
'here\'s a patch: there are two things,\n\nthe test is too slow in general (too many iterations)\nthe test is super-slow on windows because of syncd i/o: i wired it to use mmapdirectory.',1
'updated patch: I\'d rather just use newDirectory actually.',1
'This means the test will be occasionally slow on windows, but I think thats ok.',1
'I dont want to start the path of losing test coverage because of certain os brokenness.',1
'[branch_4x commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1387549\nLUCENE-4397: speed up test',0
Patch.,1
'New patch, fixing previous nocommits / downgrading to TODOs.',1
'I also removed the specialized scorers since they seem not to help much.',1
'All tests pass, but I still need to fix all tests that now avoid MemoryPF to also avoid DirectPF.',1
'Otherwise I think it\'s ready...',1
'Would it really be that much slower if it was slightly more reasonable, e.g.',1
'storing freqs\nin packed ints (with huper-duper fast options) instead of wasting so much on them?',1
'Would it really be that much slower if it was slightly more reasonable, e.g.',1
'storing freqs\n in packed ints (with huper-duper fast options) instead of wasting so much on them?',1
'Probably not that much slower?',1
'I think that\'s a good idea!',1
'But I think we can explore this after committing?',0
'There are other things we can try too (eg collapse skip list into shared int[]: I think this one may give a perf gain, collapse positions, etc.',1
).,0
'New patch, adding scary warning & MIGRATE.txt entry, fixing javadoc errors, and adding lucene.experimental ... still haven\'t thought of another name yet ...',1
'I dont have better name either.',0
'Lets just commit it with this one and think about it for later!',1
'Escape URI paths for XSL',1
'I don\'t understand the problem, can you provide your build.xml?',0
'If you changed the Lucene-provided build files, this is not an issue, because those always build out of the box.',1
'If XSL does not like your own and modified build files, they are invalid XML, so fix those.',1
'The attached patch seems to work only around your invalid XML.',1
'My build.xml is the same as upstream, the problem is my checkout path looks like this\n/home/buildserver/workspace/builds/\n{search-engineering}-solr-lucene-{trunk}\n\nThis means that the prepare-webpages target gets its paths in the buildpaths variable as a pipe separated list like so\n\n/home/buildserver/workspace/builds/{search-engineering}\nsolr-lucene\n{trunk}/lucene/analysis/common/build.xml|/home/buildserver/workspace/builds/{search-engineering}-solr-lucene-{trunk}\n/lucene/analysis/icu/build.xml|...(and so on)\nXSLT picks this up later and tries to load these paths, however XSLT assumes that they are URLS which makes the { character invalid and causes\ncom.sun.org.apache.xalan.internal.xsltc.TransletException: javax.xml.transform.TransformerException: com.sun.org.apache.xml.internal.utils.URI$MalformedURIException: Path contains invalid character: {\nThis pattern is infrastructural to where I work and is not likely to change (I would like it too)\nNot sure if that makes sense',1
'Hi,\nthat makes sense, thanks for reporting this!',1
'I will fix this, thanks for the nice workaround patch.',1
'Committed rev 1339097.',1
'Thanks Greg!',0
'Draft patch.',1
'Added ScoreMode as parameter to JoinUtil#createJoinQuery(...).',1
'Maybe ScoreMode should be a public enum inside the join package.',1
'Updated patch.',1
'Started adding randomizing score mode in TestJoinUtil test class.',1
'Made ScoreMode a public enum in join package.',1
'Updated patch.',1
'Fixed random tests.',1
'Added support for explain.',1
'Added ScoreMode support for documents that relate to more than one document.',1
'I think it is ready to be committed.',1
'I still see one omitted   Otherwise this looks great: +1 to commit!',1
Oops...,0
'I see.',0
'I\'ll commit soon!',1
'Committed to trunk and branch4x.',1
'Hi, Is this possible to use in solr I tried setting \n{!scoreMode=Avg}\n, but it doesn\'t seem to have any effect.',1
'Solr uses a different joining implementation.',0
'Which doesn\'t support mapping the scores from the `from` side to the `to` side.',1
'If you want to use the Lucene joining implementation you could wrap this in a Solr QParserPlugin extension.',1
'Same problem if I use:\nIndexSearcher searcher = new IndexSearcher(req.getSearcher().getIndexReader());\nI can\'t override the class since the constructor is private.',1
'This is probably to only use the static methods.',0
'I\'ve used an output to see the hashcode and it stays the same.',0
'Can I change this behaviour somehow?',1
'I got suggested to extend the Query class and return a Hash myself.',1
'My query class contains:\n        @Override\n        public int hashCode() \n{\n            return q.toString().hashCode();\n        }\nIt seems to work now.',1
'Hi David, I just committed LUCENE-4704.',1
'Query instances returned from JoinUtil will implement equals and hashcode in future versions.',1
'Thanks alot!',0
'I\'ll try the patch when 4.2 get\'s released.',0
'The problem is this guy is configurable via IndexWriterConfig too.',1
'Maybe the IWConfig getter/setter here\nshould also be pkg-private?',1
'This seems pretty expert to expose publicly?',1
'We could then also make ThreadAffinityDWPTpool pkg-private too.',1
'I made getAndLock pkg-private (since its effectively is anyway, does no harm),\nin r1329024 to fix the broken javadocs links... but we should figure out what we want to do here\nfor a real fix.',1
'How about simply removing this from IWC?',1
'It\'s such an insanely expert thing that anyone wanting to experiment with it can modify Lucene\'s sources (and submit a patch back if the results are good)...',0
'Patch, just making the IWC set/get package private, and fixing LTC to use reflection to gain access...',1
'New patch, making a few more classes package private...',1
+1,1
'My first question is, what about backward compatibility requirements?',1
'The problem\nis if people start using such a structure (\'remote api\') that depends upon\nthe structure of our java source code, then they will be upset if we break it.',1
'If we totally change a Query\'s API, does that push all the responsibility of the\nAPI designer to deal with serialization backwards compat?',1
'APIs are difficult enough\nas-is just for java consumers.',1
'This seems similar to the java serialization issue (which we removed for this reason).',1
'Can\'t the serialization be totally independent?',1
'Now you\'ve got me thinking.',0
'Several points:\nFirst, Jackson is far, far, less implementation-sensitive than the built-in Java serialization.',1
'It looks at the public API of constructors, getters, and setters.',1
'How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?',1
'Second, this is not about long-term persistence.',1
'I\'d be happy to document it as \'do not expect to move one of these across versions.\'',1
'It\'s most obvious application is to move arbitrary queries around in SolrCloud.',1
'However, thirdly, you want it independent?',1
'I can make it independent.',1
'Here\'s how.',0
'Let\'s assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support.',1
'Once you make a parser for something, you\'ve promised that it has (at least) a particular set of inputs until the next major version, yes?',1
'So, for each Query object class, make a Jackson mixin class, that maps the \'official inputs\' to the current collection of setters/constructor arguments.',1
'Want to re-wrangle the query classes?',1
'If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.',1
'If it were up to me, I\'d start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I\'m not really opinionated.',1
'Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign \'add a few getters and setters\' classes to the queries?',1
'\nFirst, Jackson is far, far, less implementation-sensitive than the built-in Java serialization.',1
'It looks at the public API of constructors, getters, and setters.',1
'How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?',1
'TermQuery currently has a bogus constructor today:\n\n\n/** Expert: constructs a TermQuery that will use the\n  *  provided docFreq instead of looking up the docFreq\n  *  against the searcher.',1
'*/\npublic TermQuery(Term t, int docFreq) {\n\n\nIts bogus because some scoring models may not use docFreq at all (e.g.',1
'language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things \nBut thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, \nso its fair game.',1
'I\'m just bringing this up as a counterexample...',1
'I thought serialization was harmless before myself\nuntil I tried to make useful changes (like yanking vector-space model out of the scoring system) and was\ntotally blocked by serialization.',1
'So I\'m gonna be suspicious of the word no matter what \n\nIf it were up to me, I\'d start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I\'m not really opinionated.',1
'Are these annotations in java 6 API?',0
'Annotations are no different than other pieces of code, they are an additional\nresponsibility.',1
'As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as\n@Override, well nothing automated yet, but we should be somehow enforcing that as well.',1
'Besides the fact we don\'t\nwant to add any dependencies to the lucene core, how would we know such annotations were correct?',1
'Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign \'add a few getters and setters\' classes to the queries?',1
'Don\'t let me get in your way: I\'m just the devil\'s advocate when i hear serialization.',1
'As far as adding getters and setters\nto queries I\'m not sure if they are benign or not without us looking at them.',1
'For example, AutomatonQuery and all of its\nsubclasses: Wildcard, Regexp, etc are totally immutable once created.',1
'This is for good reasons so I don\'t think we should\nadd any setters to these classes, and the stuff it stores internally shouldn\'t be accessible via a getter.',1
'Rob,\nThese are not Java annotations, they are specific to Jackson.',0
'(Though Jackson will pay attention to the JAX-B annotations in Java 6, I fear that this is a cure worse than the disease.)',1
'So hanging them on the real classes makes for a hard dependency on Jackson to compile, and I\'m not sure myself that this is a good idea in the middle of Lucene.',1
'If I stick to the mixin approach, no annotation are needed at all.',1
'As for the rest, I think that we agree.',1
'The point of the annotations is to only serialize the stuff that makes up the plausible, stable, sort of attributes that you would support in a parser syntax.',1
'(though I suppose someone might want to SolrCloudify some really expert complexity), and the selective annotation avoids the rest.',1
'So I\'m going to sketch something out and we\'ll all see what it looks like.',0
'Yeah a sketch (maybe just Term and Boolean or something ?)',1
'would be cool, maybe my paranoia\nis unjustified... then we could see what it actually would need to look like and think about\nthe backwards-compatibility/API costs etc would be.',1
'Add accessor to BooleanQuery.java to allow serializing \'disable-coord\'.',1
'My only comment on that: it would help perpetuate this inverted \'disable/omit\' stuff\nthat i just cant stand.',1
'I really wish BQ had \'enableCoord\' in the ctor with true as\nthe default: of course a horrible thing to try to fix but we were able to fix this\nfor omitTF etc so I\'m not yet losing hope.',1
'git://github.com/bimargulies/lucene-json-qp.git\nRob, here you will find the promised look at how this works.',1
'No QP yet, that\'s the easy part.',1
'Apologies for the Maven project, but that\'s what I do quickly.',0
'No patches to core required at all yet.',0
'if you run the unit test, you will see what the json looks like.',0
'Concise, no surprise, it\'s not.',1
'The query integration at a glance looks very non-invasive from my perspective!',1
'I think we should pursue this?',1
'We might run into some tricky parts but we could\nhave this component as a separate pluggable module with probably minimal changes\nto the core Query apis right?',1
Right.,1
'The tiny patch is an example of the sort of change, and I think it\'s always good to have \'getters\'.',1
'In fact, I don\'t even need that patch for the code to work, but i thought it was a good idea.',1
'I\'ll continue to set this up.',0
'Let me know when/how/if you want it shaped to go into the tree somewhere.',0
'I think if it works for you, just iterate in your github and ping the issue when you make progress?',0
'Otherwise we worry too much about where/how the code sits when that isn\'t really so important\nat this stage.',0
'As far as final integration, I think there are a number of ways to do it but its really\nunrelated to making progress here.',0
'One suggestion might be to split queryparser/ module\nlike analyzers/ so we have:\n\nclassic/ [including things based on it: complexPhrase, ext, analyzing]\nflexible/ [including precedence which is based on it]\nxml/\njson/\n\nThis could probably make things less confusing, as currently queryparser/ is a mix of\ndifferent frameworks with different dependencies (e.g.',1
'xml depends on queries/ and sandbox/,\nbut the others dont, and json will depend on jackson and maybe other stuff, etc, etc).',0
'And then finally, probably a followup issue to do solr integration (i\'m more fuzzy on that).',1
'Lots of tests write to std streams currently (solr).',1
'We could do that fairly easily (in a number of ways) but it\'ll break a number of tests.',1
'I think it\'s fine if tests write to the std streams, but not core Lucene code (lucene/core/src/java/*)?',1
'This is possible by verifying where System.out takes place at runtime via stack analysis.',1
'Alternatively a bytecode woven aspect.',1
'But at the same time it could also be a find-and-grep over sources?',1
'Comments would have to be removed in the pipeline prior to grepping for sysouts.',1
'Checking all sysouts/syserrs and verifying stack traces there seems like an overkill compared to the above.',1
findbugs?,0
checkstyle?,0
'Don\'t we now fail if there is a noncommit in the src?',1
'Can\'t we use the same mechanism?',1
'Some core code legitimately uses System.out.println, e.g.',1
CheckIndex.,1
'However on review, some of its use cases should actually be System.err...',1
'I removed the std prints in lucene/core/src/java that I could find on quick grepping.',1
'I\'ll leave this open so we can somehow automatically catch this...',1
'I took a guess that one way would be to replace the System IO streams with ones that will throw exceptions.',1
'It requires cglib, and might (due to the abuse of sun.reflect.Reflection to get the callee efficiently) be sun hotspot specific.',1
'I guess this is not perfect, it would only error out if the code is called\nIf there is thought to this being a good thing I could look into how to wire it up to the unit-tests',1
'Helps if I upload the right version',1
'You can just as well substitute your own implementation of PrintStream using System.setOut/setErr and check stacks on printlns...',1
'But I agree with Benson that a static analysis approach is much cleaner.',1
'Don\'t know if there\'s anything out of the box in findbugs/ pmd, but even if not then this can be done as a 10-liner by applying an aspect to classes via aspectj and parsing the output logs detecting if an aspect has been applied (it shouldn\'t match anywhere).',1
'Thats a good point and started bugging me last night when I was thinking about it.',1
'As as result attached is a static analysis version that will hate any GETSTATIC java/lang/System::(err|out) (I briefly looked at findbugs and did not find a working version for this idea) \nThis one uses ASM 3.3\nIts dumb output looks like \n/opt/sun-jdk-1.7.0/bin/java -classpath /tmp/test.jar:/tmp/out/production:/tmp/asm-all-3.2.jar SystemPrintCheck /tmp/test.jar\nSystemPrintCheck$SystemOutMethodVisitor.visitMethodInsn @ SystemPrintCheck.java +42\nSystemPrintCheck.main @ SystemPrintCheck.java +102\nProcess finished with exit code 1',1
'ASM static test version that dislikes System.out / System.err',1
fyi.,0
'PMD has a rule for this  SystemPrintln.',1
'http://pmd.sourceforge.net/rules/index.html\nDidn\'t check the details though.',0
'Interesting I didnt look at PMD although that is more down to my personal dislike of code lint tools that do source code analysis\nshrugs It was a fun distraction anyhow',1
'Well would it sway the argument if I said that the ASM code near directly translates into a findbugs rule (I have done this before)\nI was also not wanting to suggest findbug custom rules at the start because thats a bigger change in including an entire code lint tool (unless its included with lucene already, in which case forgive my stupidity I am still finding my way around)\nSomething doesn\'t fit right in my mind with the AspectJ approach, I have seen it not work in the past for obscure reasons and it feels that running the weaving in pretend to check the verbose output is not much further on from checking the source code in the first place.',1
'Oh, btw.',0
'I think a FindBugs rule for detecting sysouts/syserrs would be a great addition to FindBugs  you should definitely file it as an improvement there.',1
'In reality at least class-level exclusions will be needed to avoid legitimate matches like the ones shown above (main methods, exception handlers), but these can be lived with.',1
'Issue is marked 3.6 and actively being discussed but has no assignee - assigning to most active committer contributing patches/discussion so far to triage wether this can/should be pushed to 4.0 or not.',1
'I\'d push it to 4.0 (automation in whatever form).',1
'Yeah sorry I have been busy at work for a spell, I was going to craft it into a findbugs rule and see if I can get accepted into the findbugs project,\nJust been a bit tied up thats all',0
'No worries Greg, really.',0
'For 3.x I think manual check will do (or what I\'ve done above with AspectJ).',1
'For 4.x it\'d be nice to have findbugs lint anyway (for this and other issues).',1
'It\'ll most likely require some rules tuning too, so it can be a separate issue.',1
'I think if we fix LUCENE-4202, we could just have a separate task for this that only runs on a fileset of non-test code?',1
'We could also exclude some tools like CheckIndex from it.',1
'We could add things like:\nSystem#in\nSystem#out\nSystem#err\nThrowable#printStackTrace() <-- eclipse stupid-stubs\n...',1
'Here\'s a patch implementing this check with LUCENE-4202.',1
'It excludes any test code, but I didnt add any exceptions for legitimate command-line tools.',1
'Current list looks like:\n\ncheck-system-out:\n[forbidden-apis] Reading inline API signatures...\n[forbidden-apis] Reading API signatures: /home/rmuir/workspace/lucene-trunk/lucene/tools/forbiddenApis/system-out.txt\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.HyphenationTree (HyphenationTree.java:467)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:408)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:412)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:416)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:637)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:638)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:640)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:658)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:659)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:660)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:292)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:302)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:312)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:326)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:336)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:346)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:356)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:366)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:376)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:386)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:395)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:404)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:413)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:529)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:534)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:542)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:314)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:320)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:336)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:346)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:382)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSetExceptions (RSLPStemmerBase.java:135)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSuffixExceptions (RSLPStemmerBase.java:159)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.tartarus.snowball.SnowballProgram (SnowballProgram.java:438)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:69)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:71)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:73)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.egothor.stemmer.Compile (Compile.java:126)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:107)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:111)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:301)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:303)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.egothor.stemmer.Trie (Trie.java:379)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:95)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:102)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:106)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:116)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:117)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:123)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:127)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:128)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:129)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:134)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:135)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ContentItemsSource (ContentItemsSource.java:178)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.DemoHTMLParser (DemoHTMLParser.java:58)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.EnwikiQueryMaker (EnwikiQueryMaker.java:110)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:84)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:93)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker (ReutersQueryMaker.java:90)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:195)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:204)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:227)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:242)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:243)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.HTMLParserTokenManager (HTMLParserTokenManager.java:12)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:35)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:37)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.programmatic.Sample (Sample.java:72)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseReaderTask (CloseReaderTask.java:41)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseTaxonomyReaderTask (CloseTaxonomyReaderTask.java:40)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:181)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:183)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:108)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:110)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:112)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask (NewAnalyzerTask.java:81)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewCollationAnalyzerTask (NewCollationAnalyzerTask.java:88)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewLocaleTask (NewLocaleTask.java:66)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewShingleAnalyzerTask (NewShingleAnalyzerTask.java:83)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:138)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:271)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PrintReaderTask (PrintReaderTask.java:54)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:140)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:141)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:142)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:146)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:40)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:41)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:42)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:43)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:40)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:41)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:43)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:44)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:41)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:42)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:44)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:45)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:41)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:42)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:44)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:45)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:41)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:42)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:44)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:45)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:43)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:44)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:46)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:47)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:117)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:121)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:123)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:307)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:308)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:309)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:44)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:45)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:46)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:47)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:48)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:49)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:50)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:65)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:54)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:60)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:112)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:46)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:63)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:152)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:50)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:100)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:111)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:145)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:155)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:157)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1963)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1965)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1973)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1975)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1980)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1985)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1992)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:116)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:128)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:150)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:154)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:180)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:210)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:223)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:255)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:271)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:344)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:378)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:387)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:398)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:402)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:405)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:408)\n[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:354)\n[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:388)\n[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:398)\n[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:600)\n[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:660)\n[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1208)\n[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1249)\n[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1368)\n[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1602)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1691)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1700)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1707)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1714)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1723)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1724)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1753)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1758)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1762)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1771)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)\n[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1778)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1787)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1789)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1790)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1793)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1795)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1797)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1798)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1801)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:56)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:57)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:58)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:59)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:60)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:61)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:62)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:63)\n[forbidden-apis] Forbidden field access: java.lang.System#err\n[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:66)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:82)\n[forbidden-apis] Forbidden field access: java.lang.System#out\n[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:87)\n[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java',0
'BUILD FAILED\n/home/rmuir/workspace/lucene-trunk/lucene/build.xml:190: Check for forbidden API calls failed, see log.',0
'Here\'s a patch: including fixes.',1
'I think its ready to commit',1
'+1, thanks Robert!',1
'Hmm I think we need a separate check in FreqProxTermsWriterPerField?',1
'yeah I agree.',1
'I just made this patch up to show the problem though!',1
'it shoudl check exactly at the point before shifts any bits: and the exception should be UOE',1
'By the way, the reason this doesnt fail always but only for certain codecs:\nsome codecs have assertions that get tripped, so they fail the test.',1
'other codecs don\'t have these asserts, so they pass the test, and checkindex happens to pass.',1
'but this is only because checkindex ignores deleted docs in testPostings, the index really is corrumpt in those cases!',1
'attached is a new test demonstrating this: for some codecs it triggers an assert, for others it makes a corrumpt index.',1
'I havent tested this yet on 3.x but i suspect it fails!',1
'See my comment and test (which produces a corrumpt index on 3.x)\nThe fact this test doesnt fail on 3.x is a bad thing',1
'attached is a fix.',1
'I want to commit soon.',1
'We just used the wrong shift.',1
'Our sign bit is free here to steal for payloads.',1
'So we don\'t need to limit positions to Integer.MAX_VALUE/2',1
+1,1
'thanks everyone!',0
'Hi Benson,\nThis looks like a duplicate of SOLR-2634.',0
'On that issue, you said you would research using Jenkins credentials to upload to the Nexus snapshot repo, but you never did.',0
'On that issue, you said you would research using Jenkins credentials to upload to the Nexus snapshot repo, but you never did.',0
'Or rather, you never reported on that issue that you did.',0
'FYI, on lucene.zones.apache.org, where all of the Lucene/Solr Jenkins jobs run, /home/hudson/.m2/settings.xml does not exist (Jenkins jobs run under the hudson user account), so we can\'t depend on pre-existing Jenkins credentials.',1
'This time for sure.',0
https://issues.apache.org/jira/browse/INFRA-4496.,0
'This time for sure.',0
https://issues.apache.org/jira/browse/INFRA-4496.,0
'Righteous to the max.',1
'See INFRA-4497 for request to enable Nexus access for Lucene and Solr.',0
'According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.',0
'I had previously thought that a single SCM section in the lucene-solr-grandparent POM would do the trick, but upon inspection of the output from mvn help:effective-pom, I can see that the grandparent POM\'s section doesn\'t properly interpolate ${module-directory} (since the property isn\'t defined in that POM), and the values inherited in other POMs are all wrong, because path steps are added with artifact names instead of directory names, in addition to ${module-directory} interpolation...',1
'Anyway, this patch fixes the SCM definition problem for trunk POMs.',1
'I\'ll do the same for branch_3x too.',1
'I didn\'t include SCM sections in the few aggregation-only POMs, since these are never deployed/released.',1
'Committing shortly.',1
'According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.',0
'Committed to trunk and branch_3x.',1
'Is the result of this blocked on the jail break?',0
'Is the result of this blocked on the jail break?',0
'I don\'t know what you mean by jail break?',0
'I\'m waiting for someone to enable Nexus access for Lucene: INFRA-4497 - I had planned to hassle them after a week had gone by with no action (it\'s only been a day or two so far).',1
'I was referring to Uwe\'s email about stopping all builds due to 1.6 versus 1.6 issues in the jail.',1
'Not that it\'s especially my business, but how did the snapshots get pushed historically to nexus if you didn\'t have access to nexus?',0
'I don\'t know - I was not involved with Lucene\'s Maven stuff, or with Hudson, at that time.',0
'Maybe others know?',0
'I deployed all of Lucene\'s trunk artifacts to the Apache snapshot repository using my personal account.',0
'However, I can\'t do any further deploys, apparently because Nexus doesn\'t like me.',0
'Here\'s what I asked on #asfinfra 15 minutes ago:\n\nHi, I\'m working on publishing Lucene\'s and Solr\'s Maven snapshots to the Apache Snapshot repo.',0
'Brian Demers enabled o.a.lucene and o.a.solr in the Nexus repo (https://issues.apache.org/jira/browse/INFRA-4497), and I successfully uploaded Lucene snapshot artifacts for 4.0.0-SNAPSHOT using Maven Ant Tasks.',1
'I attempted to use an encrypted password in ~/.m2/settings.xml, but that failed, apparently because Maven Ant Tasks doesn\'t (yet) grok encrypted passwords (http://jira.codehaus.org/browse/MANTTASKS-177).',1
'So I switched back to a plaintext password, but now I\'m getting 401 errors from Nexus when I try to deploy Solr snapshots, or to re-deploy the Lucene snapshots.',1
'I suspect Nexus has locked my account out (sarowe), but I\'m not sure - can anybody here help?',1
'No response yet.',0
'the nexus log says, \'unable to authenticate sarowe\'.',1
'the nexus log says, \'unable to authenticate sarowe\'.',1
'Yeah, the same credentials worked earlier, and now don\'t work.',1
'I can log into lucene.zones.apache.org using these same credentials, and I assume Nexus and FreeBSD are pulling from the Apache LDAP DB.',0
'So as I wrote to #asfinfra (see above), I\'m thinking I\'ve been locked out.',0
'Lucene.Zones login is local only (i added you to passwd file).',0
'You should better check people.ao or SVN for credentials.',1
'I am now in the process of deploying Solr trunk snapshot artifacts.',1
'I\'ll do the same for branch_3x Lucene&Solr.',1
'These are done.',0
'I\'ll work on switching the Jenkins jobs now.',1
'Lucene.Zones login is local only (i added you to passwd file).',0
'You should better check people.ao or SVN for credentials.',1
'Okay, that makes sense.',1
'I knew it was the Apache LDAP server, and not specifically Nexus, when SVN logins failed when I tried to commit the patch listed above.',1
'The Jenkins Maven trunk and branch_3x builds are now configured to deploy snapshot artifacts to the Apache snapshot repository, and both have successfully done so.',1
'Not that it\'s especially my business, but how did the snapshots get pushed historically to nexus if you didn\'t have access to nexus?',1
'I found the following comment on SOLR-586 that described the process by which Maven snapshots made it into the Apache snapshot repository: https://issues.apache.org/jira/browse/SOLR-586?focusedCommentId=12623985&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12623985.',1
'Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?',1
'I would imagine a proper repo manager is a more suitable official location than Jenkins.',1
'As an aside, do you know why SSL is mandatory to access nexus?',0
'I\'ve found https repos to be very problematic for maven when the client needs to use an HTTP proxy such as is common in corporate environments.',1
'I\'m not saying its impossible, but just tricky and took trying different things.',1
'If you don\'t know the answer, who/how should I inquire further?',0
'Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?',1
'Done - see r1296722 and r1296723.',1
'As an aside, do you know why SSL is mandatory to access nexus?',0
'[...] If you don\'t know the answer, who/how should I inquire further?',0
'Sorry, I don\'t know the answer to either question.',0
'I would start asking either on #asfinfra or infrastructure@apache.org, and they should be able to point you to the right person.',0
'Removing 3.6 Fix version.',1
'If I\'ll make it by the release, I\'ll put it back.',1
'Patch w/ test.',1
'Here\'s the patch I ended up with when working on this on top of 3.x (don\'t remember if it was 3.5 or 3.6).',1
'This is not intended for commit, but you many want to look at the manager and its validation logic.',1
'Not sure how much of it is still relevant, except the recreate scenario.',1
'I think it\'s OK to add IOE to the signature?',1
'Ok.\nthat decRef could have closed the reader\nHmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?',1
'So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right?',1
'Therefore there\'s no point to even tryIncRef?',1
'Just because it\'s using the LineFileDocs\nAhh ok. As I said, I didn\'t read the test through.',0
'I will review the patch after you post a new version.',0
'New patch, just handling the NRT case.',1
'that decRef could have closed the reader\r\nHmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?',1
'So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right?',1
'Therefore there\'s no point to even tryIncRef?',1
'You\'re right ... so I just left the two decRefs in the patch ...',1
'Few comments:\n\nThis assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty.',1
'Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?',1
'Maybe change the end of the test to a single-line IOUtils.close()?',1
'You wrote previously that the test uses LineFileDocs, but I don\'t see it.',1
'It seems it only adds facets to documents?',0
'If so, can it go back to newDirectory()?',0
'It\'s good that you identify replaceTaxonomy, makes the code safer.',1
'TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)?',1
'It\'s odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there\'s no reason why it shouldn\'t always return it.',1
'Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much.',1
'Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo.',1
'But I think that\'s ok since it means the check will fail on the next refresh attempt.',1
'Really, if ever DTW.epoch changes, we should fail.',1
'I don\'t know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it\'s the best we can do?',1
'In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed).',1
'But after I received a question yesterday from a someone who did not understand why we don\'t call close(), perhaps we should, for clarity?',1
'Otherwise this looks great!',1
'When I worked on it in the past, DTR wasn\'t NRT and the sync was a nightmare.',1
'Making it NRT really simplified this manager!',1
'Thanks for all the feedback Shai, I incorporated it all except for\nthis one:\nTR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)?',1
'It\'s odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there\'s no reason why it shouldn\'t always return it.',1
'Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much.',1
'Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo.',1
'But I think that\'s ok since it means the check will fail on the next refresh attempt.',1
'Really, if ever DTW.epoch changes, we should fail.',1
'I don\'t like that cutting over to DTW would open up the thread hazard\nthat we fail to catch the replace ... admittedly it\'d be rare but why\nopen it up?',1
'The added Expert/@lucene.internal method seems minor ...',1
'When I worked on it in the past, DTR wasn\'t NRT and the sync was a nightmare.',1
'Making it NRT really simplified this manager!',1
'Thank you for doing all the hard work first (making DTR NRT)',0
'I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?',1
'Good, I\'ll fix that.',1
'Also test() also has these 5 close() statements which can be folded into one IOUtils.',1
'But that\'s just style.',0
'Woops, I missed that one ...',0
'I\'ll fix.',1
'What I meant is that if instead of checking epoch on TR you check on DTW, you won\'t (I think!)',1
'get into that hazard.',1
'Ahh, right, as long as I check taxoWriter after the reopen: good!',1
'I\'ll fix to just use DTW...',1
'New patch w/ last round of changes ... thanks Shai!',1
'Looks good, +1.',1
'Thanks for doing the work Mike!',0
'Closed after release.',1
'Forgot to mention that Doron actually discovered the bug, I just had the time to provide the fix .',0
'Patch fixes the bug by moving to track reference count by DTR.',1
'Also, added a test which covers that bug.',1
'On the go, fixed close() to synchronize on this if the instance is not already closed.',1
'Otherwise, two threads that call close() concurrently might fail (one of them) in decRef().',1
'I think it\'s ready to commit, will wait until tomorrow for review.',1
'Patch looks good, builds and passes for me, thanks for fixing this Shai.',1
'Few comments:\n\nCHANGES: rephrase the e.g.',1
'part like this: (e.g.',1
'if application called incRef/decRef).',1
'New test:\n\t\nLTC.newDirectory() instead of new RAMDirectory().',1
'text messages in the asserts.',1
'DTR:\n\t\nWould it be simpler to make close() synchronized (just like IR.close())\nWould it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one?',1
'This way we continue to delegate that logic to IR, and do not duplicate it.',1
'Current patch removes the ensureOpen() check from getRefCount().',1
'I think this is correct - in fact I needed that when debugging this.',1
'Perhaps should document about it in CHANGES entry.',1
'Patch addresses Doron\'s comments.',1
'Missed that test comment about no need for random directory.',0
'About the decRef dup code, yeah, that\'s what I meant, but okay.',0
'I think this is ready to commit.',1
'Committed revision 1234450 (3x), 1234451 (trunk).',1
'Thanks Doron !',0
'In the LUCENE-3453 branch I\'ve removed DocValuesField.setXXX methods: they are all inherited from Field.setValue instead.',1
'Likewise for NumericField (which also had its own setters for numeric values, with different names)...',1
'Fixed with LUCENE-3453.',1
'seems like this issue still exists.',1
'Now we overload Field#setValue(int|long|short|...)',1
'Wait  DocValuesField.java doesn\'t overload any setters now right?',0
'I\'m confused.',0
'I am talking about Field.java\n\n\n  public void setValue(BytesRef value) {\n     //....\n  }\n\n  public void setValue(int value) {\n      //....\n  }\n\n  public void setValue(long value) {\n    //....\n  }\n\n  public void setValue(float value) {\n      //....\n  }\n\n  public void setValue(double value) {\n   //....\n  }',1
'Right, Field.java has setters.',0
'DocValuesField no longer does (nor does NumericField), ie we fixed this issue (that these classes were overloading the setters from Field.java).',1
'Or... are you saying this is naming issue?',0
'Ie we can work out the naming (do we use setValue(T value) or setT(T value) for Field.java and for the new Norm class) here...',1
Patch.,1
'I now pass DataInput down to IndexFormatTooNew/OldExc, and\n.toString() it, and impl\'d .toString in the all the IndexInput impls I\ncould find.',1
'I dont think i like all the duplication/extra tracking in every indexinput impl...',1
'In my opinion its not worth it!',1
'can this be done in a cleaner way?',1
'I agree with Robert, adding this to IndexInput is stupid.',1
'I think, only adding the file name as done before should be fine.',1
'There are only few places where we pass null as file name to the exception (whoch may be fixed).',1
'Passing the whole directory name is in my opinion useless.',1
'Its up to the implementation using lucene to keep track of its directory name (when it opens a IndexReader it already knows its dir name).',1
'I would close this as won\'t fix and maybe only fix the remaining places that misses the file name (e.g.',1
SegmentTermsEnumReader).,1
'you might be able to work me down to a partial path rather than a full path...\nlike if IndexInput takes String name in its ctor (the same one passed to Directory.openInput NOT the full path, keeps String as a private variable), and implements toString itself.',1
'then we wouldnt have to track additional variables in each indexinput impl, only change openinput and the ctors to pass this information.',1
'But i\'m still not sure how useful this is.',1
'It really seems like an implementation detail that we check the stored fields to determine if an indexformat is too old.',1
'who cares what the file name is?',1
'+1, I like that solution, Robert.',1
'I\'ll rework the patch...',1
'New patch folding in Robert\'s idea....',1
'I added final String resourceDescription to II, returned from\ntoString, made it required arg to the ctor, and fix all II subclasses\nto pass something reasonable.',1
'When our II impls originate an exception (eg from EOF), I also include\nII.toString(); if a method they call throws IOE (eg file.read(...)\ninside SimpleFSII), then I catch & rethrow w/ II.toString() included.',1
'I also include the sub-file name when inside a sliced II (CFS/CFX);\nI added a required arg (sliceDescription) to the sliceInput method\nfor this.',1
'OK, new patch; I changed to EOFE, and I just use in.toString() instead of special casing DI vs II.',1
'I think it\'s ready!',1
'mike can we close LUCENE-3138 too?',0
'Bulk close after release of 3.5',1
'+1\nWe now use Lucene Hunspell for a few customer deployments, and it would be great to have it the analysis module, since it supports some 70-80 languages out of the box, and gives great flexibility since you can edit - or augment - the dictionaries to change behaviour and fix stemming bugs.',1
'As a side benefit I also expect that when the Ooo dictionaries get more use in Lucene, users will over time be able to extend and improve the dictionaries, and contribute their changes back, benefiting also Ooo users.',1
'Patch with a port of the code.',1
'Because most of the dictionaries are L/GPL, I\'ve written my own dumb stupid dictionary for test purposes.',1
'During testing I discovered a long standing bug to do with recursive application of rules This has now been fixed.',1
'Code now is also version aware, as required by the CharArray* data structures.',1
'Thanks Chris for adding this to Lucene Analysis module.',0
'We did lots of work on Google Code, so it should really be in Lucene, except the dictionaries.',1
'We should only add links to web pages where to get them.',1
'...so it should really be in Lucene, except the dictionaries.',1
'how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project?',1
'Maybe the dictionaries are under ASL eventually?',1
'how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project?',1
'Maybe the dictionaries are under ASL eventually?',1
'Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license.',1
'I guess thats something they will have to sort out during incubation.',1
'I don\'t see the licenses changing since the dictionaries tend to be developed by national language organisations, but maybe the ASF will negotiate.',0
'Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license.',1
'I don\'t think we should read too much into that text file: its not even obvious which of the many dictionaries in that folder it applies to!',0
'I know for a fact that some of the files in there are NOT GPL, for example the en_US dictionary: http://svn.apache.org/viewvc/incubator/ooo/trunk/main/dictionaries/en/README_en_US.txt?revision=1162288&view=markup',0
'Okay good spotting.',0
'so how do we want to proceed?',1
'Do we want to bring some of the dictionaries in?',1
'Should we address that in a later issue once its become clearer in OO what they\'re doing?',1
'Patch now includes a package.html linking to a PDF about hunspell and suggesting dictionaries are sourced from the OpenOffice wiki.',1
'Committing tomorrow.',1
'Committed revision 1167467.',1
'Reopening for 3x backport.',1
'3x back port:\nCommitted revision 1167505.',1
'Is there a JIRA for adding HunspellStemFilterFactory to Solr?',1
'Nope, its on my mental TODO but go for it.',0
SOLR-2769,0
'Bulk close after release of 3.5',1
'as a start, i installed the two freebsd ports for java doc on hudson into /usr/local/share/doc/jdk1.5 and jdk1.6\nI\'ll see if i can add the hooks to the build scripts now',1
'As a partial solution, I setup the 30 minute builds to just directly override javadoc.link (and javadoc.link.java for Solr) for our 30 minute builds... we don\'t care about the actual javadoc artifacts or where the links actually point to, only that there are no warnings.',1
'This is in r1138418',0
'+1\nI think we should allow you optionally set a sysprop using linkoffline\nhell, why bother with the sysprop?',1
'.. lets just commit the package-list files for all third party libs we use into dev-tools and completely eliminate the need for net when building javadocs.',1
'lets just commit the package-list files for all third party libs we use into dev-tools and completely eliminate the need for net when building javadocs.',1
'+1\nHitting build failures because we can\'t download these package lists is silly.',1
'I agree with hossman too.',1
'I\'m just a javadocs dummy and was doing what I could to stop the 30minute builds.',0
'I cant figure out this linkoffline (at least with my experiments its confusing)... but this sounds great.',1
'I propose switching to the oracle.com link suggested by Chris Male:\nhttp://download.oracle.com/javase/6/docs/api/package-list apparently works reliably.',1
'This would be lots simpler than trying to figure out dev-tools etc., assuming that this link is indeed reliable.',1
+1,1
'+1 let\'s try this and see if it is indeed reliable.',1
'Just one other idea:\n\nWe already have JAVA_HOME set (direct or implicitely set by ANT)\nThe Javadocs are always at same location in $JAVA_HOME\n\nCould we not use this to point to the package list (at least fpr the JDK part).',1
'I don\'t like the hardcoded package list.',1
'The Javadocs are always at same location in $JAVA_HOME\nI looked at JAVA_HOME on Windows for 1.5.0_22 and 1.6.0_23 (both 64 bit JDKs), and neither included Javadocs.',1
'Maybe they\'re separately downloadable?',0
'Sorry, you are right.',0
'I have it here, but the README.html in JDK\'s root folder says:\n\nJDK(TM) Documentation\nThe on-line JavaTM Platform, Standard Edition (Java SE) Documentation contains API specifications, feature descriptions, developer guides, reference pages for JDKTM tools and utilities, demos, and links to related information.',0
'This documentation is also available in a download bundle which you can install on your machine.',0
'To obtain the documentation bundle, see the download page.',0
'For API documentation, refer to the The JavaTM Platform, Standard Edition API Specification This provides brief descriptions of the API with an emphasis on specifications, not on code examples.',0
'sarowe asked about this issue in LUCENE-3587.',0
'FWIW, i thought this issue (LUCENE-3228) had been resolved a long time ago based on some comments i remember people making, but evidently those comments where on irc/mail and folks didn\'t post them in Jira.',1
'Problems with the patch i attached (that i know of):\n1) we don\'t distribute dev-tools in our releases, so at a minimum we\'d need to find a new home for any package-list files we wanted to ship.',1
'2) the Java documentation from Oracle has some licensing/restrictions  that affect redistribution which don\'t seem to be compatible with ASF 3rd party licensing policy so we can\'t include the java package-list files in our releases\n...we could still use the ideas in this patch to deal with package-list files for non java/oracle distrobutions, but at the time this patch was written the only other extneral javadocs we linked to where that might be useful was junit, and since this patch was created, that link has just been removed outright from our build.xml files.',1
'I don\'t think there\'s anything left here but to resolve as Won\'t Fix',1
'I don\'t think there\'s anything left here but to resolve as Won\'t Fix\nI disagree.',1
'1) we don\'t distribute dev-tools in our releases, so at a minimum we\'d need to find a new home for any package-list files we wanted to ship.',1
'How does lucene/src/tools/javadoc/ grab you?',0
'2) the Java documentation from Oracle has some licensing/restrictions that affect redistribution which don\'t seem to be compatible with ASF 3rd party licensing policy so we can\'t include the java package-list files in our releases\nHere is a direct link to the Sun/Oracle documentation redistribution restrictions: http://java.sun.com/docs/redist.html.',1
'We can include the Java javadoc package-list file in Subversion but exclude it from our source releases, and include a mechanism to download it when it\'s absent (i.e., in the source release).',1
'There is an ASF precedent for redistributing Java javadoc package-list files in the Maven project\'s javadoc plugin: http://jira.codehaus.org/browse/MJAVADOC-315 and https://jira.codehaus.org/browse/MJAVADOC-327 - in Subversion at http://svn.apache.org/viewvc/maven/plugins/trunk/maven-javadoc-plugin/src/main/resources/org/apache/maven/plugin/javadoc/.',1
'I can\'t find any associated discussion of legal ramifications, though.',1
'I\'ll put up a patch shortly implementing these ideas.',1
'Patch for branch_3x.',1
'Features:\n\nAdds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.',1
'When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.',1
'When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.',1
'When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.',1
'When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.',1
'Links from Solr javadocs to Lucene\'s javadocs are enabled.',1
'When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss\'s patch on this issue.',1
'When building a release version, links are to the same-versioned Lucene release javadocs.',1
'Trunk for patch with the same changes.',1
'If there are no objections I will commit this tomorrow.',1
'Committed:\n\nr1210020: trunk\nr1210022: branch_3x',1
'\n    [junit] Testsuite: org.apache.lucene.index.TestNRTThreads\n    [junit] Testcase: testNRTThreads(org.apache.lucene.index.TestNRTThreads):\tFAILED\n    [junit] expected:<8> but was:<18>\n    [junit] junit.framework.AssertionFailedError: expected:<8> but was:<18>\n    [junit] \tat org.apache.lucene.index.TestNRTThreads.testNRTThreads(TestNRTThreads.java:515)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)\n    [junit] \n    [junit] \n    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 19.812 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] doc id=157 is supposed to be deleted, but got docID=119\n    [junit] doc id=82 is supposed to be deleted, but got docID=68\n    [junit] doc id=83 is supposed to be deleted, but got docID=38\n    [junit] doc id=80 is supposed to be deleted, but got docID=36\n    [junit] doc id=81 is supposed to be deleted, but got docID=37\n    [junit] doc id=67 is supposed to be deleted, but got docID=24\n    [junit] doc id=69 is supposed to be deleted, but got docID=26\n    [junit] doc id=68 is supposed to be deleted, but got docID=25\n    [junit] doc id=672 is supposed to be deleted, but got docID=430\n    [junit] doc id=444 is supposed to be deleted, but got docID=344\n    [junit] doc id=441 is supposed to be deleted, but got docID=766\n    [junit] doc id=442 is supposed to be deleted, but got docID=343\n    [junit] doc id=443 is supposed to be deleted, but got docID=767\n    [junit] doc id=70 is supposed to be deleted, but got docID=67\n    [junit] doc id=71 is supposed to be deleted, but got docID=27\n    [junit] doc id=72 is supposed to be deleted, but got docID=28\n    [junit] doc id=73 is supposed to be deleted, but got docID=29\n    [junit] doc id=74 is supposed to be deleted, but got docID=30\n    [junit] doc id=75 is supposed to be deleted, but got docID=31\n    [junit] doc id=76 is supposed to be deleted, but got docID=32\n    [junit] doc id=219 is supposed to be deleted, but got docID=175\n    [junit] doc id=662 is supposed to be deleted, but got docID=425\n    [junit] doc id=663 is supposed to be deleted, but got docID=426\n    [junit] doc id=218 is supposed to be deleted, but got docID=174\n    [junit] doc id=361 is supposed to be deleted, but got docID=286\n    [junit] doc id=362 is supposed to be deleted, but got docID=287\n    [junit] doc id=360 is supposed to be deleted, but got docID=285\n    [junit] doc id=366 is supposed to be deleted, but got docID=291\n    [junit] doc id=365 is supposed to be deleted, but got docID=290\n    [junit] doc id=364 is supposed to be deleted, but got docID=289\n    [junit] doc id=363 is supposed to be deleted, but got docID=288\n    [junit] doc id=368 is supposed to be deleted, but got docID=293\n    [junit] doc id=367 is supposed to be deleted, but got docID=292\n    [junit] doc id=518 is supposed to be deleted, but got docID=361\n    [junit] doc id=517 is supposed to be deleted, but got docID=805\n    [junit] doc id=220 is supposed to be deleted, but got docID=176\n    [junit] doc id=324 is supposed to be deleted, but got docID=269\n    [junit] doc id=322 is supposed to be deleted, but got docID=268\n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestNRTThreads -Dtestmethod=testNRTThreads -Dtests.seed=0:0\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {extra8=MockFixedIntBlock(blockSize=1054), extra9=MockVariableIntBlock(baseBlockSize=87), body=MockSep, extra0=MockVariableIntBlock(baseBlockSize=87), packID=Pulsing(freqCutoff=16), extra1=MockRandom, extra2=Standard, extra3=SimpleText, date=MockVariableIntBlock(baseBlockSize=87), extra4=MockSep, extra5=Pulsing(freqCutoff=16), extra6=MockFixedIntBlock(blockSize=1054), extra7=MockVariableIntBlock(baseBlockSize=87), docid=MockVariableIntBlock(baseBlockSize=87), title=SimpleText, titleTokenized=Standard}, locale=ar_JO, timezone=Europe/Oslo\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestSearchForDuplicates, TestMockAnalyzer, TestCheckIndex, TestDoc, TestFlex, TestIndexReaderCloneNorms, TestIndexWriterExceptions, TestIndexWriterUnicode, TestMultiLevelSkipList, TestNRTThreads]\n    [junit] NOTE: Mac OS X 10.6.7 x86_64/Apple Inc. 1.6.0_24 (64-bit)/cpus=4,threads=1,free=41147720,total=85000192\n    [junit] ------------- ---------------- ---------------\n    [junit] TEST org.apache.lucene.index.TestNRTThreads FAILED',0
'It\'s probably the new DWPT code.',1
'There was a specific issue to fix this problem LUCENE-2956.',1
phew!,0
'This seems like a delete issue.',1
'I only looked at the output robert posted so far but it seems that a FrozenDelPackage gets lost somewhere here....',1
'I will look after buzzwords',0
'I can reproduce this easily and even if I set search threads to 0 and index threads to 1.',1
'I forced the IW to use OpenMode.CREATE and suddenly the tests are not failing anymore.',1
'It seems that the tempdir is not cleaned up since always the second test fails for me but never the first run.',1
'this is not a DWPT issue, phew!',1
'the test cleans itself up in afterClass(), so there is in fact an issue.',1
'taking a look at this, I don\'t like the way _TestUtil.getTempDir(String desc) was working before... it was basically desc + LTC.random.nextInt(xxx), so if you wired the seed like I did, and somehow stuff doesnt totally clean up, then its easy to see how it could return an already-created dir.',1
'I changed this method to use _TestUtil.createTempFile...',1
'I think this is much safer.',1
'robert can you still reproduce or can we close this issue here?',0
'i could never really reproduce... but sometimes if i ran all tests with -Dtests.seed=0:0 it would happen.',1
'the reason this test is not reproducible is that this test uses \'n seconds\' as a limit.',1
'so whether it passes or fails depends upon what your computer is doing at the moment.',0
'I think we must change it to limit itself by number of docs instead.',1
'I think we must change it to limit itself by number of docs instead.',1
'I agree: let\'s fix that.',1
'this was a temp file issue - fixed',1
Patch.,1
'The basic impl is working, I think (the random test passes),\nbut I have alot of nocommits still!',1
'New patch, I think it\'s ready to commit!',1
'BlockJoinQuery still needs hashCode/equals, and a javadoc note (as I remarked earlier at 2454) about the possible inefficiency of the use of OpenBitSet for larger group sizes.',1
'When the typical group size gets a lot bigger than the number of bits in a long, another implementation might be faster.',1
'This remark the in javadocs would allow us to wait for someone to come along with bigger group sizes and a real performance problem here.',1
'I would prefer to use single pass and for now I only need the parent docs.',1
'That means that I have no preference for 2454 or this one.',0
'Patch, adding equals and hashCode and clone to BlockJoinQuery.',1
'Also, I now throw UOE from get/setBoost, stating that you should do so against the child query instead.',1
'The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.',1
'Another implementation (should be another issue, but since you asked...) could be a set of increasing integers, based on a balanced tree structure with a moderate fanout (e.g.',1
'32), and all integer values relative to the minimum determined by the data for the pointer from the parent.',1
'The whole thing could be stored in one int[], the pointers would be (forward) indexes into this one array, and each internal node would consist of two rows of integers (one data, one pointers), and each row would be compressed as a frame of reference into the array.',1
'This thing can implement \n\nint next(int x)\n\n and \n\nint previous(int x)\n\n easily, and an iterator over this can implement \n\nadvance(target)\n\n for a DocIdSetIterator, and because of the symmetry it can also do that in the reverse direction as needed here.',1
'Compression at higher levels might not be necessary.',1
'For now, there is no code for this, except for the frame of reference.',0
'Occasionaly the need for a more space efficient filter shows up on the mailing lists, so if anyone wants to give this a try...',1
'The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.',1
'Ahh, OK.',0
'Though, I suspect this (the linear scan OBS does for next/prevSetBit) is a minor cost overall, if indeed the app has so many child docs per parent that a sparse bit set would be warranted?',1
'Ie, the Query/Collector would still be visiting these many child docs per parent, I guess?',1
'(Unless the query hits few results).',0
'I don\'t think a jdoc warning is really required for this... but I\'m fine if you want to add one?',1
'I\'ll commit this soon and resolve LUCENE-2454 as duplicate!',1
'Is there a wiki page on how to use this?',0
'I need to implement an index with nested docs and an example scheme and query would be awesome.',1
Thanks!,0
'I wrote this blog post giving a quick overview: http://blog.mikemccandless.com/2012/01/searching-relational-content-with.html',0
+1,1
'Perhaps we should also fail the test if that happens?',1
'Was there reason why only the stacktrace printed, but tests were considered successful?',1
'some tests are still problematic, at least on windows...',1
'I think perhaps some of the crazier ones like DiskFull, TestCrash, anything that has to disable MockDirectoryWrappers\'s checks because they must create corrupt indexes or other scary things.',1
'Patch adds registerTempFile to LTC plus prints stack information if rmDir fails.',1
'I think we should also fail the test if that happens?',1
'some tests are still problematic, at least on windows... \nOk, I didn\'t notice your comment when posted the patch.',0
'So let\'s keep it as-is.',0
'I think it\'s ready to commit',1
'Patch applies the same changes to backwards\' LTC.',1
'I think I\'ve found the problem - MockIndexOutputWrapper did not close delegate if dir.maybeThrowEx actually threw an exception.',1
'Here\'s a patch that fixes it:\n\n\nIndex: lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java\n===================================================================\n--- lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java       (revision 1127062)\n+++ lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java       (working copy)\n@@ -45,20 +45,23 @@\n\n   @Override\n   public void close() throws IOException {\n-    dir.maybeThrowDeterministicException();\n-    delegate.close();\n-    if (dir.trackDiskUsage) {\n-      // Now compute actual disk usage & track the maxUsedSize\n-      // in the MockDirectoryWrapper:\n-      long size = dir.getRecomputedActualSizeInBytes();\n-      if (size > dir.maxUsedSize) {\n-        dir.maxUsedSize = size;\n+    try {\n+      dir.maybeThrowDeterministicException();\n+    } finally {\n+      delegate.close();\n+      if (dir.trackDiskUsage) {\n+        // Now compute actual disk usage & track the maxUsedSize\n+        // in the MockDirectoryWrapper:\n+        long size = dir.getRecomputedActualSizeInBytes();\n+        if (size > dir.maxUsedSize) {\n+          dir.maxUsedSize = size;\n+        }\n       }\n+      synchronized(dir) {\n+        dir.openFileHandles.remove(this);\n+        dir.openFilesForWrite.remove(name);\n+      }\n     }\n-    synchronized(dir) {\n-      dir.openFileHandles.remove(this);\n-      dir.openFilesForWrite.remove(name);\n-    }\n   }\n\n   @Override\n\n\nMaybe we solve it by moving delegate.close() before dir.maybeThrow, instead of the try-finally?',1
'Committed revision 1127470 (trunk).',1
'Committed revision 1127471 (3x).',1
'LTC now verbose whatever we need to pursue rmDir failures.',1
'Bulk closing for 3.2',1
'This is on Ubuntu btw.',0
'Run log:\n\nNOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240\nNOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240\nThe following exceptions were thrown by threads:\n*** Thread: Lucene Merge Thread #0 ***\norg.apache.lucene.index.MergePolicy$MergeException: java.io.FileNotFoundException: /tmp/test4907593285402510583tmp/_51_0.sd (Too many open files)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)\nCaused by: java.io.FileNotFoundException: /tmp/test4907593285402510583tmp/_51_0.sd (Too many open files)\n\tat java.io.RandomAccessFile.open(Native Method)\n\tat java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)\n\tat org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.<init>(SimpleFSDirectory.java:69)\n\tat org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.<init>(SimpleFSDirectory.java:90)\n\tat org.apache.lucene.store.SimpleFSDirectory.openInput(SimpleFSDirectory.java:56)\n\tat org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:337)\n\tat org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:402)\n\tat org.apache.lucene.index.codecs.mockrandom.MockRandomCodec.fieldsProducer(MockRandomCodec.java:236)\n\tat org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.<init>(PerFieldCodecWrapper.java:113)\n\tat org.apache.lucene.index.PerFieldCodecWrapper.fieldsProducer(PerFieldCodecWrapper.java:210)\n\tat org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:131)\n\tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:495)\n\tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:635)\n\tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3260)\n\tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2930)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)\nNOTE: test params are: codec=RandomCodecProvider: {field=MockRandom}, locale=nl_NL, timezone=Turkey\nNOTE: all tests run in this JVM:\n[TestIndexWriter]\nNOTE: Linux 2.6.32-31-generic i386/Sun Microsystems Inc. 1.6.0_20 (32-bit)/cpus=1,threads=2,free=26480072,total=33468416',0
'Does that repro line reproduce the failure for you Doron?',0
'It\'s odd because that test doesn\'t make that many fields... oh I see it makes a 100 segment index.',1
'I\'ll drop that to 50...',1
'The nightly build also hits too-many-open-files every so often, I suspect because our random-per-field-codec is making too many codecs...',1
'I wonder if we should throttle it?',1
'Ie if it accumulates too many codecs, to start sharing them b/w fields?',1
'I dropped it from 100 to 50 segs.',1
'Can you test if that works in your env Doron?',0
'Yes, thanks, now it passes (trunk) - with this seed as well quite a few times without specifying a seed.',1
'I\'ll now verify on 3x.',0
'I fact in 3x this is not reproducible with same seed (expected as Robert once explained) and I was not able to reproduce it with no seed, tried with -Dtest.iter=100 as well (though I am not sure, would a new seed be created in each iteration?',1
'Need to verify this...)\nAnyhow in 3x the test passes also after svn up with this fix.',0
'So I think this can be resolved...',1
'Fixed by Mike, thanks Mike!',1
'Thanks for raising it Doron!',0
'Bulk closing for 3.2',1
'bulk move 3.2 -> 3.3',1
'its easy to add the sleep, but we dont even have good multithreaded tests with rollback() [except testing how exceptions are handled and not really asserting anything?]',1
'Can we push this out to 4.0?',1
'I think we can push to 4.0...',1
'Bulk move 4.4 issues to 4.5 and 5.0',1
'Move issue to Lucene 4.9.',1
'Hi Mark, could you add an example algorithm with this behavior?',0
'Also, this is from the package javadocs:\n\n\n# multi val params are iterated by NewRound\'s, added to reports, start with column name.',0
'merge.factor=mrg:10:20\nmax.buffered=buf:100:1000\n\n\nIs it possible to workaround the problem by specifying a sufficiently long column name as the first value, that is, replacing e.g.',1
'\'mrg\' or \'buf\' in the above?',1
'Hey Doron - I have a patch for this, I\'ve just been too lazy to extract it.',1
'I\'m not sure if there is anything built-in that is long enough to matter - it comes into play if, for example, if you want to alternate fully qualified class names per round.',1
'My original workaround was to simply pad the column name - but it was ugly and had it\'s limitations, so I instead made some modifications to the formatting classes.',1
'My original workaround was to simply pad the column name\nYeah that\'s what I meant, so ok, better formatting will help.',1
'Mark, are you planning on working on this one?',0
'Is it ok to defer until 4.0?',0
'yeah, this is def not that important.',1
'Lets move to 4.',1
'Bulk move 4.4 issues to 4.5 and 5.0',1
'Move issue to Lucene 4.9.',1
'The patch.',1
'It fixes the problem when usePhraseHighlighter=true.',1
'When the flag is false and FVH works on N-gram field, not a few terms may be created in tree, then it causes uncontrollable.',1
'But I think the case of using usePhraseHighlighter=false with N-gram field is rare, the attached patch will be enough.',1
'bulk move 3.2 -> 3.3',1
'I\'ll commit soon.',1
'trunk: Committed revision 1170908.',1
'3x: Committed revision 1170913.',1
'Bulk close after release of 3.5',1
'The change from HashSet to ArrayList for flatQueries resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0.',1
'Our queries sometime contain tens of thousands of terms.',1
'As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls.',1
'Uh, that is not good news.',0
'Please don\'t hesitate to open a ticket.',0
'Patches would be welcome as I don\'t have time to look into it for a few weeks...',0
'Created LUCENE-3719 with a patch.',1
'Robert: In your patch: Why use exactly that float (looks arbitrary) and not e.g.',1
'Float.MIN_VALUE (of course not NEGATIVE_INFINITY!)?',1
'Doesn\'t matter, just want to understand.',0
'Hi Uwe: you are right!',1
'obviously this one needs a comment, but this is the idea:\n\nnorm[0]=4.656613E-10\nnorm[1]=5.820766E-10\nnorm[2]=6.9849193E-10\nnorm[3]=8.1490725E-10',1
'Here\'s a new patch...\napparently there was code in SmallFloat to specifically do this.',1
'This simply removes the bug.',1
'I would like to commit soon.',1
'\n\n  public boolean isMatch() {\n    return (0.0f < getValue());\n  }\n\n\nIsn\'t that a bug?',1
'We fixed our search code to not discard negative scores, so explain should also handle that?',1
'This line of code was also one of the first things I was thinking about.',0
'I also think this is a bug in explain!',1
'Isn\'t that a bug?',1
'We fixed our search code to not discard negative scores, so explain should also handle that?',1
'I agree this is a bad assumption really (although subclasses can override).',1
'I am just concerned what explains will \'break\' if we fix this.',1
'But still i think the float quantization issue (the root cause of this problem really) should be fixed... ideally we fix both!',1
'I\'m not sure why this is a quantization issue... SmallFloat handles underflow by mapping to the smallest number greater than 0, so the only way to get a 0 field norm is an explicit boost of 0.',1
'Anyway, if we want to discard explicit representation for 0, some of the code that handled these edge cases should also be modified:\n\n\n    if (smallfloat < fzero) {\n      return (bits<=0) ?',1
'(byte)0   // negative numbers and zero both map to 0 byte\n       :(byte)1;  // underflow is mapped to smallest non-zero number.',1
'OK, although underflow is generally handled, Robert found at least one case where it doesn\'t work.',1
'System.out.println(SmallFloat.floatToByte315(5.8123817E-10f));\nThat prints 0 for some reason.',1
'I\'ll see if I can figure out why.',1
'Thanks, so the bug is really my imagination (as SmallFloat is designed to generally handle this, its just some corner case i produced in a test).',1
'So, if we can fix smallfloat my gross hack is not necessary, as we would then only produce byte 0 norms in the case of a zero boost... we can discuss if anything needs to be done there (in my opinion, its not serious, i am only concerned about non-zero floats quantizing to a zero byte).',1
'and then separately we gotta figure out about explains: in my opinion whether or not a document was matched by a query is unrelated to the score...',1
'I don\'t understand 95\\\% of what yonik & robert have been saying in this issue  but fortunately i don\'t think that matters  it sounds like they both agree htat what they are talking about is unrelated to this bug.',0
'Sorry for the confusion, my issues were actually a separate (corner-case) bug, which yonik fixed already in LUCENE-2937.',1
'That bug would cause you to have a field boost of 0 in some very very rare/likely-to-have-never-happened-with-our-default-sim cases when you shouldn\'t.',1
'So for this issue we can address the explains, for when you explicitly set boost of zero.',1
'would appreciate a review so we can get this into 3.1\n+1 to the patch.',1
'Patch looks great.',1
'So then i expanded the patch to also include BooleanQueries containing phrase queries on multiple fields, and then i was finally able to make TestSimpleExplanations in similar ways to what Koji is seeing.',1
'Hoss\'s assumption is correct because the problem was found at customer site when they used a BooleanQuery containing PhraseQueries (term query on 1-gram fields).',1
'fixed in trunk and 3x...',1
'Committed revision 1074357.',1
'Committed revision 1074363.',1
'Bulk close for 3.1',1
'here\'s a patch along the lines i suggested.',1
'sorry, here is correct patch (I had issue dyslexia)',0
'I think Steven addressed this issue already...',0
'I did some minor cleanups (typos, don\'t link to unversioned (trunk) resources since this is a versioned document, etc).',1
'I think this one is still left to do?',0
':\n\nDemo sources are no longer shipped in a binary release',1
'Demo sources are no longer shipped in a binary release\nHmm i missed this: Can we simplify this and just tell users to download the source release?',1
'reopening to address the issue of demo source code being in the binary checkout... we should either include it, or change the demo instructions to tell people to get the demo sources from the source release',1
'I added to the demo-part-2 instructions that you should get a source checkout.',1
'Bulk close for 3.1',1
'here\'s a patch with what Mike suggested, moving the skipping stuff private to the codecs, and separating the interval from the minimum df necessary to index skip data.',1
'i also added the \'dont skip when close\' opto to Sep and Preflex codecs (since i neglected to do this and only did Standard).',1
'I kept all parameters the same (I think we should benchmark etc before changing) but I did some experiments with a fairly large skipMinimum and it looked promising.',1
'I think we later (after we have good benchmarks) set this reasonable, and maybe bump skipInterval for Sep codec too, especially if we improve its \'pendingPositions\' consuming to balance it out.',1
'So, I think we should apply this little patch as-is as a small step.',1
'here\'s a patch solving a lot of the issue for the skiplists and doc/freq/prox etc pointers for Simple64.',1
'as discussed above, because its size on disk is fixed, we encode blockID and blockID deltas instead of file pointers.',1
'with the saved bits, we steal one for the case where the delta is within-block, in this case this delta is really the upto delta.',1
'this puts my simple64 indexes smaller than standardcodec (and speeds up the queries too)',1
'I edited my comment above, because the new version of jira works differently somehow with newlines/pasting and it screwed up the quoting.',0
'To add insult to injury, when you edit there is no longer a way to comment on what you changed...',0
'Both patches look great!',1
'Bit by bit...',0
'here\'s a patch for the regular FixedIntBlock and VariableIntBlock cases: they write upto first, if a bit is set then its within block and upto is the upto delta.',1
'otherwise they then read the vlong for the fp delta.',1
'this saves about 5\\\% total bulkvint index size.',1
'OK I committed this for now to the branch (r1070580), we can always revert it.',1
'I cutover FOR, PFOR, and PFOR2 to use this, and all tests pass with all these codecs.',1
'tarball containing a maven project with source code and unit tests for:\n\nAFOR1\nAFOR2\nFOR\nPFOR Non Compulsive\nSimple64\na basic tool for debugging IntBlock codecs.',1
'It includes also the lucene-1458 snapshot dependencies that are necessary to compile the code and run the tests.',1
'I pulled out the simple64 implementation here and adapted it to the bulkpostings branch.',1
'Thanks for uploading this code Renaud, its great and the code is easy to work with.',1
'I hope to get some more of the codecs you wrote into the branch for testing.',0
'I changed a few things that helped in benchmarking:\n\nthe decoder uses relative gets instead of absolute\nwe write #longs in the block header instead of #bytes (as its always long aligned, but smaller numbers)\n\nBut mainly, for this one I think we should change it to be a VariableIntBlock codec... right now it packs 128 integers into as few longs as possible, but this is wasteful for two reasons: it has to write a per-block byte header, and also wastes bits (e.g.',1
'in the case of a block of 128 1\'s).',1
'With variableintblock, we could do this differently, e.g.',1
'read a fixed number of longs per-block (say 4 longs), and our block would then be variable between 4 and 240 integers depending upon data.',1
'New patch, including Robert\'s patch, and also adding Simple64 as a VarInt codec.',1
'We badly need more testing of the VarInt cases, so it\'s great Simple64 came along (thanks Renaud!).',1
'All tests pass w/ Simple64VarInt codec.',1
'OK I committed the two new Simple64 codecs (to bulk branch).',1
'\nIn the case of 240 1\'s, i was surprised to see this selector was used over 2\\\% of the time\nfor the gov collection\'s doc file?',0
'our results were performed on the wikipedia dataset and blogs dataset.',1
'I don;t know what was our selection rate, I was just referring to the gain in overall compression rate.',0
'But still, for the all 1\'s case I\'m not actually thinking about unstructured text so much...\nin this case I am thinking about metadata fields and more structured data?',1
'Yes, this makes sense.',1
'In the context of SIREn (kind of simple xml node based inverted index) which is meant for indexing semi-structured data, the difference was more observable (mainly on the frequency and position files, as well as other structure node files).',1
'This might be also useful on the document id file for very common terms (maybe for certain type of facets, with a very few number of values covering a large portion of the document collection).',1
'Just an additional comment on semi-structured data indexing.',0
'AFOR-2 and AFOR-3 (AFOR-3 refers to AFOR-2 with special code for allOnes frames), was able to beat Rice on two datasets, and S-64 on one (but it was very close to Rice on the others):\nDBpedia dataset: (structured version of wikipedia)\n\n\nMethod\nEnt\nFrq\nAtt\nVal\nPos\nTotal\n\n\nAFOR-1\n0.246\n0.043\n0.141\n0.065\n0.180\n0.816\n\n\nAFOR-2\n0.229\n0.039\n0.132\n0.059\n0.167\n0.758\n\n\nAFOR-3\n0.229\n0.031\n0.131\n0.054\n0.159\n0.736\n\n\nFOR\n0.315\n0.061\n0.170\n0.117\n0.216\n1.049\n\n\nPFOR\n0.317\n0.044\n0.155\n0.070\n0.205\n0.946\n\n\nRice\n0.240\n0.029\n0.115\n0.057\n0.152\n0.708\n\n\nS-64\n0.249\n0.041\n0.133\n0.062\n0.171\n0.791\n\n\nVByte\n0.264\n0.162\n0.222\n0.222\n0.245\n1.335\n\n\nGeonames Dataset: \n\n\nMethod\nEnt\nFrq\nAtt\nVal\nPos\nTotal\n\n\nAFOR-1\n0.129\n0.023\n0.058\n0.025\n0.025\n0.318\n\n\nAFOR-2\n0.123\n0.023\n0.057\n0.024\n0.024\n0.307\n\n\nAFOR-3\n0.114\n0.006\n0.056\n0.016\n0.008\n0.256\n\n\nFOR\n0.150\n0.021\n0.065\n0.025\n0.023\n0.349\n\n\nPFOR\n0.154\n0.019\n0.057\n0.022\n0.023\n0.332\n\n\nRice\n0.133\n0.019\n0.063\n0.029\n0.021\n0.327\n\n\nS-64\n0.147\n0.021\n0.058\n0.023\n0.023\n0.329\n\n\nVByte\n0.216\n0.142\n0.143\n0.143\n0.143\n0.929\n\n\nSindice Dataset: Very heterogeneous dataset containing hundred of thousands of web dataset\n\n\nMethod\nEnt\nFrq\nAtt\nVal\nPos\nTotal\n\n\nAFOR-1\n2.578\n0.395\n0.942\n0.665\n1.014\n6.537\n\n\nAFOR-2\n2.361\n0.380\n0.908\n0.619\n0.906\n6.082\n\n\nAFOR-3\n2.297\n0.176\n0.876\n0.530\n0.722\n5.475\n\n\nFOR\n3.506\n0.506\n1.121\n0.916\n1.440\n8.611\n\n\nPFOR\n3.221\n0.374\n1.153\n0.795\n1.227\n7.924\n\n\nRice\n2.721\n0.314\n0.958\n0.714\n0.941\n6.605\n\n\nS-64\n2.581\n0.370\n0.917\n0.621\n0.908\n6.313\n\n\nVByte\n3.287\n2.106\n2.411\n2.430\n2.488\n15.132\n\n\nHere, Ent refers to entity id (similar to doc id), Att and Val are structural node ids.',1
'Hi Michael, \nthe first results are not that impressive.',1
'Could you tell me what is BulkVInt ?',0
'Is it the simple VInt codec implemented on top of the Bulk branch ?',0
'What the difference between \'+united +states\' and \'+nebraska +states\' ?',0
'Is nebraska a low frequency term ?',0
'\nThe BulkVInt codec is VInt implemented as a FixedIntBlock codec.',1
'Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.',0
'previously various codecs\nlooked much faster than Vint but a lot of the reason for this is due to the way Vint\nwas implemented...',1
'This is odd, because we observed the contrary (on the lucene-1458 branch).',1
'The standard codec was by an order of magnitude faster than any other codec.',1
'We discovered that this was due to the IntBlock interface implementation that:\n\nwas copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).',1
'had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).',1
'But this might have been improved since then.',1
'Michael told me he worked on a new version of the IntBlock interface which was more performant.',1
'So, if we \'group\' the long values so we are e.g.',1
'reading say N long values\nat once in a single internal \'block\', I think we might get more efficiency\nvia the I/O system, and also less overhead from the bulkpostings apis.',1
'If I understand, this is similar to increasing the boundaries of the variable block size.',1
'Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.',1
'On the facility for allOnes in AFOR-3: one of the reasons why this appears to be of rather little use is that current analyzers do not index stop words.',1
'They do this for two reasons, index size and query time, both based on VByte.',1
'With an allOnes facility the first reason disappears almost completely, and query times can be also be guarded in other ways, for example by checking for document frequency and then trying to fall back on digrams.',1
'So the message is: please keep this in.',1
'Attached patch, adding block multiplier to Simple64VarInt.',1
'I haven\'t tested perf impact yet...',0
'New patch, cuts over to bulk-reading the byte[] and then pulling a LongBuffer from that.',1
'Bulk move 4.4 issues to 4.5 and 5.0',1
'Move issue to Lucene 4.9.',1
'I added a SegmentListener class which is set on IWC.',1
'I still need to write unit tests and add an event for aborted merges.',1
'Perhaps we want to enable a collection of segment listeners instead of only one?',1
'A CompositeSegmentListener niftily removes the need for collection.',1
'A CompositeSegmentListener niftily removes the need for collection\nHow would it look?',0
'A SegmentListener that has a number of children SLs and delegates eventHappened() calls to them.',1
'I think start/endTime can be long and not Long?',1
'Maybe instead of adding init/start/endTime to OneMerge, you can pass a \'time\' parameter to MergeEvent.',1
'So its signature will be MergeEvent(Type type, long time, OneMerge merge).',1
'The \'time\' parameter can then be interpreted according to Type.',1
'Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?',1
'About this:\n\n\npublic long getMergeSegmentSize(boolean includeDocStores) throws IOException {\n  // nocommit: cache this?',1
'return mergeInfo.sizeInBytes(includeDocStores);\n}\n\n\nSegmentInfo caches sizeInBytes, so I think the \'nocommit\' can go away.',1
'However, I did notice that SegmentInfo\'s cache is potentially buggy  it doesn\'t take into account \'includeDocStores\'.',1
'I.e., if you call it once w/ \'false\' and then with \'true\' (or vice versa), you won\'t get the right sizeInBytes.',1
'I\'ll open a separate issue to fix this.',0
'I think start/endTime can be long and not Long?',1
'Long\'s used because it allows null.',1
'Maybe instead of adding init/start/endTime to OneMerge, you can pass a \'time\' parameter to MergeEvent.',1
'So its signature will be MergeEvent(Type type, long time, OneMerge merge).',1
'The \'time\' parameter can then be interpreted according to Type.',1
'Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?',1
'I think it\'s generally useful to keep track of the time(s) in the OneMerge object.',1
'It\'s implemented this way so that the, likely a user interface does not need to store the various times itself.',1
'Actually, logging applications also need to print the durations.',1
'I did notice that SegmentInfo\'s cache is potentially buggy - it doesn\'t take into account \'includeDocStores\'.',1
'Yes, that\'d be good to fix.',1
'Long\'s used because it allows null.',1
'I see.',0
'We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .',1
'I think it\'s generally useful to keep track of the time(s) in the OneMerge object.',1
'I\'m not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it\'d be nice if we can add more to it.',1
'As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes.',1
'But as I said, I don\'t mind about it too much.',0
'BTW, I think SegmentListener is not the proper name?',1
'I.e., it does not listen on Segments, right?',1
'Maybe a SegmentMergeListener, or simple a MergeListener?',1
'It anyway accepts only a MergeEvent ....',0
'We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that\nThat\'s a style thing.',1
'I prefer null as it\'s conclusive and doesn\'t lead to incorrect calculations, just NPEs.',1
'I think SegmentListener is not the proper name?',1
'I.e., it does not listen on Segments, right?',1
'Maybe a SegmentMergeListener, or simple a MergeListener?',1
'I had MergeListener, then changed it to be more generic.',1
'I think we should add other segment events such as flush, open, clone, close, etc?',1
'I see.',0
'I\'m ok with both then.',1
'I\'ll added events for flush, open, clone, close and the CompositeSegmentsListener.',1
'Here\'s a first cut including workarounds to avoid NPEs and file not found exceptions in SegmentInfo (when calling size in bytes).',1
'There\'s a test case for merge init, start, and complete.',0
'I need to add one for abort.',1
'The aborted merge event is now generated and tested for.',1
'I separated out a ReaderListener because it\'s tied to the ReaderPool which eventually will exist external to IW.',1
'Here\'s another iteration.',1
'Changed the name to IndexEventListener.',1
'Added experimental to the Javadocs, and I probably need to add more.',1
'There are some nocommits still, eg, for the reason a flush kicked off.',1
'Reader events should be in a different issue as reader pool is moving out of IW soon?',1
'All tests pass.',1
'Here\'s an update, there\'s one nocommit as I\'m not sure how we want to capture and exception and rethrow (a Throwable).',1
'Adding the reason a flush occurred requires quite a bit of refactoring that we can probably leave for later if it\'s needed.',1
'Updated to trunk, and all tests pass.',1
'but constitutes an API backcompat break\nCan abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe?',1
'If so, then it won\'t break anything.',1
'Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?',1
'What do you think?',0
'Would abort() on Directory fit better?',1
'E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput?',1
'Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code?',1
'If so, would rollback() be a better name?',1
'I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it.',1
'The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that\'s what you\'re trying to remove ).',1
'So on one hand, I\'d like to see IndexWriter\'s code simplified (this class has become a monster), but on the other, it doesn\'t feel right to me to add this logic in IndexOutput.',1
'Maybe I don\'t understand the use case for it well though.',1
'I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?).',1
'And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.',1
'All in all, I\'m +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.',1
'+1 I think this\'d be a good simplification of IW/IR code.',1
'I don\'t mind that IO would know how to delete the partial file it had created; that seems fair.',1
'So eg CompoundFileWriter would abort its output file on hitting any exception.',1
'I think we can make a default impl that simply closes & suppresses exceptions?',1
'(We can\'t .deleteFile since an abstract IO doesn\'t know its Dir).',1
'Our concrete impls can override w/ versions that do delete the file...',1
'Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe?',1
'If so, then it won\'t break anything.',1
'It can\'t.',1
'To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to.',1
'Abstract IO class has neither.',1
'If we add them, they have to be passed to a new constructor, and that\'s an API break \nWould abort() on Directory fit better?',1
'E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput?',1
'Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code?',1
'If so, would rollback() be a better name?',1
'Oh, no, no.',0
'No way.',1
'I don\'t want to push someone else\'s responsibility on Directory.',1
'This abort() is merely a shortcut.',1
'Let\'s go with a usage example:\nHere\'s FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358\nNow, the same, with abort() - https://gist.github.com/746367',0
'I think we can make a default impl that simply closes & suppresses exceptions?',1
'(We can\'t .deleteFile since an abstract IO doesn\'t know its Dir).',1
'Our concrete impls can override w/ versions that do delete the file...',1
'I don\'t think we need a default impl?',1
'For some directory impls close() is a noop + what is more important, having abstract method forces you to implement it, you can\'t forget this, so we\'re not gonna see broken directories that don\'t do abort() properly.',1
'This change is really minor, but I think, convinient.',1
'You don\'t have to lug reference to Directory along, and recalculate the file name, if the only thing you want to say is that write was a failure and you no longer need this file.',1
'I offered a default impl just to not break the API.',1
'I don\'t think a default impl is a good option.',1
'If we\'re ok making an exception for 3x as well (I know I am), then I don\'t think we should have a default impl.',1
'I think a bw compat exception is fine too!',1
'Patch against 3x:\n\nChanges listCommits() signature to return a List<IndexCommit>\nDirReader.listCommits() sorts the list in the end.',1
'Added a test case to TestIndexReader.',1
'IndexCommit implements Comparable.',1
'Removed impl from CommitPoint (which also removed a redundant duplicate \'gen\' member).',1
'I did not implement ReaderCommit to support deletes.',1
'Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use.',1
'If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions.',1
'Not sure it\'s worth the efforts.',1
'I believe it is ready to commit.',1
'I\'ll wait a day or two until I commit it.',0
'Your comments are welcome.',0
'Shai, looks good to me!',1
'+1 to commit',1
'Committed revision 1034080 + 1034144 (3x).',1
'Due to backwards tests failure, I kept the method signature as returning Collection, and only documented the new behavior.',1
'Committed revision 1034140 (trunk).',1
'Java 5 allows covariant return types.',0
'Could we not declare both methods in 3.x and deprecate the old one?',1
'In trunk we can remove it and only provide List.',1
'We cannot declare both methods  But backwards does not fail now',1
'Bulk close for 3.1',1
'Clearing 3.1 fix version... it\'s not clear how we can fix this w/o drastic API changes...',1
'It is actually possible to force this, today, by having your collector return false from acceptDocsOutOfOrder...\r\nWell you are using a custom collector anyway if you are doing this, so can\'t we just add a sentence to that\r\nmethod\'s javadocs indicating that you should return false if you want to use the scorer navigation apis?',1
'I think this issue is fixed already?',0
'VisitSubScorers works in 3.6.2 (if it gets released, Robert backported) and in 4.0 its working, too?',0
'As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?',1
'As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?',1
'+1, i think for freq() and getChildren() we should throw UOE with text like this.',1
'But we can also do the javadocs too.',1
'Then i think there would be a lot less surprises.',1
'An idea (separate issue!)',1
'would be:\nBS1 completely violates the scorer interface, the only method you can call is the one taking a Collector.',1
'In my opinion, BS1 should not implement the Scorer interface, that the whole bug!',1
'It should maybe some separate class like OutOfOrderDocIdReporter (name is just an example) that only implements collect(Collector).',1
'And the navigation api (advance, next) should be separated from score() and freq() - a simple java interface Scorer.',1
'So the current in-order scorer would be a simple DocIdSetIterator that additionally implements the Scorer interface (to provide score() and freq()) and current out-of-order scorers would implement only the OutOfOrderDocIdReporter API and pass a inlined Scorer interface (without advance and next) to the setScorer() method (like BucketScorer currently).',1
'Collectible... (not serious)',0
'\nIf we don\'t think any other future scorer would want to score docs NOT in order ... then maybe we should simple rename scoreDocsInOrder to needsNavigation?',1
'(Or scoreDocAtOnce, scoreDocAtATime, something else...).',1
'I actually just remembered the query-time join i think does this too?',0
'But yeah, if we are going to have booleans, i would prefer something more along the lines of document-at-a-time since its less confusing than\nscoreDocsInOrder (its standard IR terminology and less confusing).',1
'Bulk move 4.4 issues to 4.5 and 5.0',1
'Move issue to Lucene 4.9.',1
'I tried to start on this however, nothing can be deleted without the terms dictionary and the terms docs working in order to obtain the doc ids to delete.',1
'Resolving deleted terms -> doc IDs doesn\'t require a sorted terms dict right?',1
'Ie a simple hash lookup suffices?',1
'Resolving deleted terms -> doc IDs doesn\'t require a\nsorted terms dict right?',1
'Ie a simple hash lookup suffices?',1
'True, however I figured it\'d be best to try our own dog food, or\nAPIs.',1
'I think the main issue right now is the concurrency of the\n*BlockPools from LUCENE-2575.',1
'Then we should be able to\nimplement deleting, which doesn\'t require skip lists.',1
'I guess if\nwe really wanted to, we could simply buffer terms and only apply\nthem in getReader.',1
'getReader would block any writes that could\nbe altering the *BlockPools.',1
'Maybe this is a good first step?',1
'Is there\nany reason we need to apply deletes in the actual updateDoc and\ndeleteDoc methods?',0
'I\'m implementing a basic doc id iterator per DWPT which will allow us to implement delete by term, and the deleted docs sequence ids.',1
'This is for merging of segments?',0
'However we\'re using readers to do the merging so this really won\'t be useful?',1
'For the deleted docs sequence id array, perhaps I\'m a little bit\nconfused, but how will we signify in the sequence id array if a\ndocument is deleted?',1
'I believe we need a secondary sequence id\narray for deleted docs that is init\'d to -1.',1
'When a document is\ndeleted, the sequence id is set for that doc in the\ndel-docs-seq-arr.',1
'When the deleted docs Bits is being accessed,\nfor a given doc, we\'ll compare the IRs seq-id-up-to with the\ndel-docs-seq-id, and if the IR seq-id is greater than or equal\nto, the Bits.get method will return true, meaning the document\nis deleted.',1
'I am forgetting how concurrency will work in this case, ie,\ninsuring multi-threaded visibility due to the JMM.',1
'Actually,\nbecause we\'re pausing the writes/deletes when get reader is\ncalled on the DWPT, JMM concurrency should be OK.',1
'If we implement deletes via sequence id across all segments, then the .del file should probably remain the same (a set of bits)?',1
'Also, when we load up the BV on IW start, then I guess we\'ll need to init the array appropriately.',1
'In regards to the deltas, when they\'re in RAM (ie, for norm and DF updates), I\'m guessing we\'d need to place the updates into a hash map (that hopefully uses primitives instead of objects to save RAM)?',1
'We could instantiate a new array when the map reached a certain size?',1
'Does anybody know where to checkout the realtime branch?',0
'I am very interested in it!',0
Thanks!,0
'Does anybody know where to checkout the realtime branch?',0
'I am very interested in it!',0
Thanks!,0
'there is no realtime branch open right now.',0
'We had to delete it since we re-integrated it for DocumentsWriterPerThread.',1
'(SVN requires that once you have re-integrated) However, there is no development happening along those lines right now and we didn\'t decide if we move forward since for general purpose the NRT features we have is reasonably fast.',0
'Anyway, I think there is still a need for this if we can provide it as a non-default option?',1
'Can you post here the full stacktrace?',0
'I agree, we should fix this.',1
'I\'ll change to a try/finally w/ a success boolean.',1
'You can use IndexWriter#unlock to forcefully remove the lock, as a workaround.',1
Patch.,1
'I tried both IndexWriter#unlock and Directory#cleanLock(IndexWriter.WRITE_LOCK_NAME) but non of those removed the entry from LOCK_HELD HashSet.',1
'It was unchanged.',1
'Ahh, sorry, I think you are hitting LUCENE-2104.',1
'Just to confirm this patch as fix.',1
'The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem.',1
'The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.',1
'Yes, I do hit LUCENE-2104 at the same time... nice.',1
'Out of curiosity - would you mind posting here the exception?',0
'This is an UT, that 1st copies a known (broken) Index files to a place, and than tries to use it.',1
'Naturally, it fails (since the index files are corrupted), and then it tries to recreate the index files and recreate the index content, but it fails to obtain the write lock again.',1
'After patch above applied to 3.0.1, the UT does pass okay.',1
'This is the stack trace I have with vanilla 3.0.1:\n\norg.sonatype.timeline.TimelineException: Fail to configure timeline index!',0
'at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:106)\n\tat org.sonatype.timeline.DefaultTimeline.repairTimelineIndexer(DefaultTimeline.java:79)\n\tat org.sonatype.timeline.DefaultTimeline.configure(DefaultTimeline.java:60)\n\tat org.sonatype.timeline.TimelineTest.testRepairIndexCouldNotRead(TimelineTest.java:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat junit.framework.TestCase.runTest(TestCase.java:164)\n\tat junit.framework.TestCase.runBare(TestCase.java:130)\n\tat junit.framework.TestResult$1.protect(TestResult.java:106)\n\tat junit.framework.TestResult.runProtected(TestResult.java:124)\n\tat junit.framework.TestResult.run(TestResult.java:109)\n\tat junit.framework.TestCase.run(TestCase.java:120)\n\tat org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)\n\tat org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/cstamas/worx/sonatype/spice/trunk/spice-timeline/target/index/write.lock\n\tat org.apache.lucene.store.Lock.obtain(Lock.java:84)\n\tat org.apache.lucene.index.IndexWriter.init(IndexWriter.java:1045)\n\tat org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:868)\n\tat org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:99)\n\t... 19 more',0
'The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem.',1
'The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.',1
'OK thanks for confirming  I\'ll backport to 3.0.x as well.',1
'(Yes patch is against trunk).',0
'The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem.',1
'The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.',1
'OK thanks for confirming  I\'ll backport to 3.0.x as well.',1
'(Yes patch is against trunk).',0
'This exception shows a LockObtainFailed exception - can you post the one that resulted in NegativeArraySize  curious to know where you hit it, and what sort of corruption yields to that .',1
'Merged to 2.9 revision: 949507',1
'The bounding box and my search point location (center) are shown as red dots\nThe blue dot is the location of the hit I am expecting to get but don\'t - In my real data there are many others around it.',0
'The yellow dot is the location I added that is outside the box but inside the search circle.',0
'This lead me to conclude that the Bounding Box is not the issue.',1
'OK, so ignoring the whole Boundary Box story and looking deeper into the code with my test case I noticed that a different bestFit value was being determined by the CartesianTierPlotter.',1
'I get a value of 13 for the test that passes (radius 32miles) and 14 when the test fails with a search radius of 31.',1
'This means to me that we end up searching in the wrong tier.',1
'Looking at CartesianTierPlotter.bestFit() I see on the  line below the passed in value of miles is divided by 2.',0
'>>> double r = miles / 2.0; \nI\'m guessing r is meant to be a radius - but the miles parameter is already a radius  - of my search circle.',1
'This has an effect on the calculation of the best fix box width - aka corner in the code - and the resulting bestFit or tierId.',1
'If I change this to not divide by 2 - my issue test case passes - as do all my other tests.',1
'Again I\'d appreciate if someone who knows the code could comment and confirm my finding or tell my I\'m crazy!',0
Thx,0
'I got pen and paper out and worked out  the calculation being done in  CartesianTierPlotter.bestFit().',0
'>>>  double corner = r - Math.sqrt(Math.pow(r, 2) / 2.0d);\nI ended up with the same formula and it is definitely expecting the radius of the search circle as param.',1
'There is therefore no need to divide miles param by 2.',1
'BTW the formula can be simplified to \n//corner is the width/height of the box that fits between the arc of the search circle \n//and a corner of the boundary box containing the search circle\ndouble corner = r - r/Math.sqrt(2);',1
'Hi Julian\nYour problem should be solved by work discussed here https://issues.apache.org/jira/browse/LUCENE-2359',1
'Thanks Nicolas.',0
'It took me a while but I finally got round to verifying your patch - I was using my fix in the meantime but your patch addresses other issues as well.',1
'I\'ve backported and all my tests pass.',1
'I\'d mark this as resolved but your patch has been reverted I see.',1
'I confirm that Nicolas\' patch in LUCENE-2359 resolves this issue.',1
'This implementation of geo in Lucene has been deprecated and will not be fixed any further nor backported.',1
'see LUCENE-1747',0
'Closing because the old spatial contrib module referenced here no longer exists as of Solr 4.',1
'By the way, Spatial4j correctly computes the bounding lat-lon box of a circle no matter where the circle is.',1
'The original code from the old Lucene spatial project (and old Solr code too) was incorrect, and is no longer used.',1
'So this issue is fixed, in a sense.',1
'But the relevant code is gone.',0
'Attachment with a test case and proposed fix',1
'Adding a Google Map to help visualise the problem.',0
'Here are the instructions:\n\nmkdir -p modules/analysis\nsvn add modules\nsvn move lucene/contrib/analyzers/* modules/analysis\npatch -p0 < ../LUCENE-2444.patch\n\n\nAll tests pass.',1
'+1, looks good!',1
'If no one objects, i\'d like to commit this first patch today to move the code in SVN.',1
'This is just a step.',0
'Then we can keep the issue open and discuss what/if-any additional\nthings should be moved to the module:\n\nLICENSE/NOTICE?',1
'I know i have been polluting the lucene one heavily from analyzer poaching.',0
CHANGES?,0
'I think a module having its own would make sense\n... other things?',1
'This looks great Robert!',1
'i applied this patch to a checkout, then removed contrib/analyzers completely.',1
'there was a problem, the contrib-uptodate macro assumes contrib/*\nSo this patch fixes the problem by adding a module-uptodate macro, you can test it\nby following the same instructions, but additionally doing \'rm -rf contrib/analyzers\'.',1
'Committed revision 941308.',1
'ok if no one objects i\'ll commit this boilerplate stuff soon.',1
'we can try to improve the language etc later but its a start.',1
'Committed LUCENE-2444_boilerplate.patch revision 941369.',1
'attached is a patch, its a little ugly since CharTermAttribute doesn\'t implement Replaceable',1
'Go for it, its a private impl class, what should we do else.',1
'Speed, speed, speed.',0
'Its better than coping into a StringBuilder before and after.',1
'Even Java 6 has no Replaceable interface!',0
'attached is an updated patch, with examples in the overview etc.',1
'I would like to commit at the end of the day if no one objects.',1
'Committed revision 937039.',1
'backported to 3x, revision 941698',1
'Bulk close for 3.1',1
'Lets reuse IW.deleteUnusedFiles() ?',1
'No need to multiply confusion )',1
'Lets reuse IW.deleteUnusedFiles() ?',1
+1,1
'Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.',1
'Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:\n\n\npublic synchronized void deleteUnusedFiles() throws IOException {\n  deleter.checkpoint(segmentInfos, true);\n}\n\n\nThe failure happens in CommitPoint\'s ctor in the assert statement which verifies the SegmentInos does not have external Directory.',1
'When I debug-traced the test, it passed and so I concluded it\'s a concurrency issue (and indeed testDuringAddIndexes spawns several threads.',1
'addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it \'fixes\' their Directory reference.',1
'I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn\'t find any synchronization block where one blocks the other.',1
'Eventually, I\'ve changed deleteUnusedFiles to this:\n\n\npublic synchronized void deleteUnusedFiles() throws IOException {\n  synchronized (commitLock) {\n    deleter.checkpoint(rollbackSegmentInfos, true);\n    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);\n  }\n}\n\n\nI\'ve tried to sync on commitLock (which seems good anyway), but the test kept failing.',1
'Even cloning SI did not work because it might have changed just before the clone.',1
'Only when passing rollbackSI to checkpoint does the test pass.',1
'But I\'m not sure if that\'s the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...\nBTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles.',1
'So I guess it\'s a NRT problem only.',1
'In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that\'s undocumented and unprotected - e.g.',1
'if someone extends IW and adds some logic which requires reading SI ...',1
'I\'m not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).',1
'Ok I understand.',0
'About the name, revisitPolicy is not exactly accurate (I think?)',1
'because it also deletes the pending files (and not just revisit the policy).',1
'Unless IW.deleteUnusedFiles will invoke both deletePendingFiles and revisitPolicy ... the latter will just do\n\n\nif (commits.size() > 0) {\n  policy.onCommit(commits);\n  deleteCommits();\n}\n\n\nWhat do you think?',1
'+1, I think that\'s a good approach.',1
'Adds revisitPolicy to IFD (package-private) and also calls it from IW.deleteUnusedFiles.',1
'All tests pass',1
'ok I\'ll remove them before commit.',1
'Will commit this later - giving chance for more people to review.',1
'Committed revision 936605.',1
'Backport to 3.1',1
'Committed revision 941417.',1
'Bulk close for 3.1',1
'attached is a patch that makes CollationTestBase, BaseTestLRU, and BenchmarkTestCase abstract, as a start.',1
'Attached is an updated patch, with a new test for MemoryIndex.',1
'Instead of looking for stuff on your hard drive, it creates some randomish documents using a selection of strings that will match the test queries, combined with some random unicode strings ala TestStressIndexing2.',1
'This removes the use of lucene.common.dir here, so now moving on to the benchmark tests.',1
'the attached patch refactors the benchmark tests:\n\nlogic to run a benchmark test is moved to BenchmarkTestCase\nthis forces them all to respect LuceneTestCase.TEMP_DIR for all file operations\nlucene.common.dir is removed',1
'All tests pass with the latest patch, additionally I tested that the benchmark tests work from Eclipse.',1
'If no one objects I would like to commit in a bit.',1
'This is a great cleanup Robert!',1
'I love the copyToWorkDir...',1
'Committed revision 935014.',1
'Committed an additional fix: 935048, this allows you to run the contrib/ant tests from eclipse too.',1
'backported to 3.x: revision 941669',1
'I am reopening (not setting as blocker since its just a test issue, but it did cause tests to fail when reviewing the release).',1
'worst case, after the release, i think it would be good to backport the new MemoryIndexTest to the 2.9.x and 3.0.x branches.',1
'Here the patch that fixes the MemoryIndexTest bug in 3.0 / 2.9',1
'Backported the MemoryIndexTest fixes to 2.9.4 and 3.0.3',1
'Attached patch nulls out the Fieldable reference.',1
'As Tokenizers are reused, the analyzer holds also a reference to the last used Reader.',1
'The easy fix would be to unset the Reader in Tokenizer.close().',1
'If this is the case for you, that may be easy to do.',1
'So Tokenizer.close() looks like this:\n\n\n/** By default, closes the input Reader.',1
'*/\n@Override\npublic void close() throws IOException {\n    input.close();\n    input = null; // <-- new!\n}',1
'I agree, Uwe  I\'ll fold that into the patch.',1
Thanks.,0
'29x version of this patch.',1
'Is there a chance that this can also be applied to 3.0.2 / 3.1?',1
'It would be really helpful to get this as soon as possible in the next Lucene version.',1
'OK I\'ll backport.',1
Thanks!,0
'patch with mod to wordlistloader, test, and snowball stoplists for danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, russian, spanish, and swedish',1
'I will commit this in a few days if no one objects.',1
'Again i add the getSnowballWordSet to WordListLoader, but if this is inappropriate we could instead have a SnowballWordListLoader in our snowball package or something, doesn\'t matter to me.',1
'Robert, patch looks good except of one thing.',1
'public static HashSet<String> getSnowballWordSet(Reader reader)\n\n\nit returns a hashset but should really return a Set<String>.',1
'We plan to change all return types to the interface instead of the implementation.',1
'thanks Simon, I agree',1
'Committed revision 899955.',1
'Hi Robert,\nwhen i changed the backwards tests i added a new param to svn exec task.',1
'With this patch it now behaves equal to bw checkouts:\n\nIf you have no svn.exe available, it will ignore the checkout.',1
'If this is so, the test should pass, so I added an exit condition, if the data dir is not available.',1
'If you have a svn.exe available, but the checkout fails, there is an network error or something else.',1
'The data dir now exists but the build should stop in this case.',1
'Sorry some whitespace issues.',0
'Fixed here.',1
'Committed Revision: 900160',1
